---
title: "12_Solution_for_task"
author: "PGaulke"
date: "24 Juli 2019"
output: html_document
---

# (PART) Exercise {-}

# Give it a try

```{r}
# To prepare for the fight
require(tidyverse)
require(ISLR)
require(magrittr)


load("C:/Users/admin/Dropbox/Master/2. Semester/Data Science/MyBook/project_data.Rdata")

summary(train.data)
summary(test.data)
train.data
test.data

# 11 variables for frequency of seven plants
# task: The test.data has the same structure but does not contain the frequencies for each of the 7 plants. Your
# goal is precisely to estimate them for the 140 observations

# remember to remove/replace na's.

```
```{r, eval=FALSE}
plot(mxPH ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "mxPH vs a1")
plot(mnO2 ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "mnO2 vs a1")
plot(Cl ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "Cl vs a1")
plot(NO3 ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "NO3 vs a1")
plot(NH4 ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "NH4 vs a1")
plot(oPO4 ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "oPO4 vs a1")
plot(PO4 ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "PO4 vs a1")
plot(Chla ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "Chla vs a1")

#tendenziell mehr mxPH und mnO2

pairs(train.data)

plot(train.data)
```

## Linear Regression

```{r}
model.slr <- lm(a1 ~ NH4, data = train.data)
summary(model.slr)
model.slr$fitted.values
model.slr$coefficients

```

Creat a real test set

```{r, include = FALSE}
set.seed(30)
num_obs = nrow(train.data)

train.index = sample(num_obs, size = trunc(0.50 * num_obs))
newtrain.data = train.data[train_index, ]
traintest.data = train.data[-train_index, ]

```


```{r}
library(caret)
featurePlot(x = train.data[ , c("mxPH", "mnO2", "Cl","NO3","NH4","oPO4","PO4","Chla")], y = train.data$a1)
```




```{r}
# starting with a simple linear model, with no predictors
fit_0 = lm(a1 ~ 1, data = train.data)
get_complexity(fit_0)

# train RMSE
sqrt(mean((train.data$a1 - predict(fit_0, train.data)) ^ 2))
# test RMSE (not available)
sqrt(mean((test.data$a1 - predict(fit_0, test.data)) ^ 2)) 
```

Create a real test set

Now again same step

```{r}
# starting with a simple linear model, with no predictors
fit_0 = lm(a1 ~ 1, data = newtrain.data)
get_complexity(fit_0)

# train RMSE
sqrt(mean((newtrain.data$a1 - predict(fit_0, newtrain.data)) ^ 2))
# test RMSE (
sqrt(mean((traintest.data$a1 - predict(fit_0, traintest.data)) ^ 2)) 
```

```{r}
library(Metrics)
# train RMSE
rmse(actual = newtrain.data$a1, predicted = predict(fit_0, newtrain.data))
# test RMSE
rmse(actual = traintest.data$a1, predicted = predict(fit_0, traintest.data))
```

RMSE formula

```{r}
get_rmse = function(model, data, response) {
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}
```

```{r}
get_rmse(model = fit_0, data = newtrain.data, response = "a1") # train RMSE
get_rmse(model = fit_0, data = traintest.data, response = "a1") # test RMSE
```

Increase the fit.

We have to remove NA`s first

```{r}
# delete the N/A
is.na(train.data)
na.omit(train.data)


fit_1 = lm(a1 ~ ., data = newtrain.data)
get_complexity(fit_1)


get_rmse(model = fit_1, data = newtrain.data, response = "a1") # train RMSE
get_rmse(model = fit_1, data = traintest.data, response = "a1") # test RMSE
```

```{r}
fit_2 = lm(a1 ~ mxPH * mnO2 * Cl * NO3 * NH4 * oPO4 * PO4 * Chla, data = newtrain.data)
get_complexity(fit_2)

get_rmse(model = fit_2, data = newtrain.data, response = "a1") # train RMSE
get_rmse(model = fit_2, data = traintest.data, response = "a1") # test RMSE
```



```{r, eval=FALSE, error=FALSE}
# newtrain.data$fitted <- model.slr$fitted.values
# newtrain.data

# select{absolutelynewtrain.data, -1)
# plot(newtrain.data$NH4, newtrain.data$a1)
# now add a line

#lines(newtrain.data$NH4, newtrain.data$fitteds, col="blue")
```




## Interaction
```{r}
model.int <- lm(a2 ~   mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=train.data)
summary(model.int)

predint <- predict(model.int)

plot(Credit$Income, Credit$Balance)
lines(Credit$Income, y.hat4, col="red")

s.data <- Credit
s.data$Student <- "Yes"

n.data <- Credit
n.data$Student <- "No"

y.hat5 <- predict(model.it2, newdata = s.data)

y.hat6 <- predict(model.it2, newdata = n.data)

plot(Credit$Income, Credit$Balance)
lines(Credit$Income, y.hat5, col="red")
lines(Credit$Income, y.hat6, col="black")
Credit
```

```{r}
# a function for caluclating the RMSE from two vectors

c.rmse <- function(observed, predicted){
  (observed - predicted)^2%>%
  mean %>%
  sqrt %>%
  round(3)
}

c.rmse2 <- function(observed, predicted) {
round(sqrt(mean((observed -predicted)^2)),3)
}

```

```{r}
require(ISLR)
require(magrittr)
#to load the required packages

set.seed(150)
#in order to create random numbers, but to save this "seed" and not create new random numbers when you run the chunk again (as you would do if you put rnorm(41) instead of set.seed


#in order to have our training data seperated, we need to half it

n <- nrow(train.data) 
# just to have an abbreviation

train <- sample(1:n, ceiling(n/2))
#1: to number of rows, ceiling is used to prevent that in case nrow(auto) is odd, you have a number such as 74,3 (also could use round)

degrees<- 1:3
#the different degrees we want to put in

v.rmse <- numeric ()
#to create a new vector where all values are putted in from the rmse

for (i in degrees){
#basically just creating an abbreviation for putting in several polynomals into the fit1
  
  
fit1 <- glm(a1 ~ poly(Cl,i), data = train.data, subset = train)
  v.rmse[i] <-
# fit in into a linear model, in order to create a ine that fits the model    

v.rmse[i] <- c.rmse(train.data$a1[-train], predict(fit1, newdata=train.data[-train,]))  
    
# how it was before, against what it is now with v.rmse:c.rmse(Auto$mpg[-train], predict(fit1, newdata=Auto[-train,]))
#here we create the function to later calculate the rmse

}
  #now we want to create a plot to see all the test error values for the different polys (the number after horsepower)
  

plot(degrees, v.rmse, type ="b", col = "red")   


#type b just shows the type of the line ( can also be l for line or p for points instead of b for both)


```

As a result we probably take degree 2, because it is quite  good from its v.rmse and it is not complex (the lower the degree, the better is it to understand)

And now for not just one split, but multiple splits:

```{r}

require(ISLR)
require(magrittr)
#to load the required packages

set.seed(120)

degrees <- 1:10

n.splits <- 10

m.rmse <- matrix(NA, length(degrees), n.splits)
#here NA is the data(numbers), length = number of rows, n.splits = number columns


for(s in 1:n.splits){
  train <- sample(1:n, ceiling(n/2))
for(i in degrees) {
  fit1<- glm(mpg ~ poly (horsepower, i), data = Auto, subset = train)
m.rmse[i,s] <- c.rmse(Auto$mpg[-train], predict(fit1, newdata = Auto[-train,]))

}
}
  
plot(degrees, m.rmse[,1], type ="l", col = "red", ylim=c(min(m.rmse), max(m.rmse)))
for (s in 1:n.splits){
  lines(degrees, m.rmse[,s], col =s)
}

```
Now we want to find out which line to take
With K-fold cross validation


```{r}

#we subset the data into 5 parts instead of 2 (50 percent train data, 50 percent test data)
#one part will be validation data, the rest will be train data
#see slide 10
#before we had just the validation approach (2 fold without crossing)

require (boot)
require (magrittr)
require (ISLR)
degrees <- 1:10
loocv <- numeric()

for(i in degrees){
  fit1 <- glm(mpg ~ poly(horsepower,i), data= Auto)
  
loocv[i] <- cv.glm(Auto, fit1)$delta[1] %>% sqrt

}

plot(degrees, loocv, type ="b", col = "red")

```

##KFold

```{r}

require (boot)
require (magrittr)
require (ISLR)
degrees <- 1:10
n.trys <- 12

m.k10 <- matrix(NA, length(degrees), n.trys)

for (s in 1:n.trys){

for(i in degrees){
  fit1 <- glm(mpg ~ poly(horsepower,i), data= Auto)
  
m.k10[i,s] <- cv.glm(Auto, fit1, K=10)$delta[1] %>% sqrt

}
}


plot(degrees, m.k10[,1], type ="l", col = "red", ylim=c(min(m.k10),max(m.k10)))

for(s in 2:n.trys){
  lines(degrees, m.k10[,s], col = s)
}

```






















Polynomials

Train.data
MLR
estimation model lm(a1), pflanzen
train.data

summary

coefficient

estiamte, standard error, t value

categorical regressor

linear regression

season size speed --> 


Polynomials
train.data

lm (a1) s s s, analyze
mlr

mxph

Das hat der doch nie selbst gemacht