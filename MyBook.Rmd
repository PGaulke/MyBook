--- 
title: "MyBook"
author: "Corinna Trierweiler and Philipp Gaulke"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: report
description: "This is a guide for data science with R"
---

# Preface{-}

This book has been produced for and based on the Data Science class of Hochschule Fresenius in Cologne, Germany. In context of the task, the book includes basic R skills, statistical methods for data science and a solution for the exercise given at the end of the semester.


```{r, include=FALSE}

#noch ausblenden

require(magrittr)
require(tidyverse)
require(ISLR)


```

<!--chapter:end:index.Rmd-->

---
title: "01_Introduction"
author: "PGaulke"
date: "14 Juli 2019"
output: html_document
---

# Introduction {-}

This book is created in order to provide programming beginners a clear and understandable overview of how to use R for statistical investigations. This includes the explanation for the set up and an introduction to the basic skills for R, as well as an overview of major statistical methods for data science.

- What is R? -

R is a programming language and free software environment for statistical computing created by the R Foundation for Statistical Computing. It is a common tool to create statistical software that can be used to analyze and interprete data sets. Apart from the ground infrastructure and function, R can be individualized easy and quickly by downloading additional tools and packages which are free avalaibale. These packages may include further function for calculation, data sets or even own programming features.

- How do we approach it? -

Learning programming is broadly declared as a herculean task. Firstly, this is simply not true especially when you consider that most of us learn a second real language in the age of 10, which is by far more difficult. Secondly, learning success is like everywhere else depending on how you approach it. Opening in the first step some R file and apply random statistical methods on a 7 terrabyte file will most probably not lead to a result that makes any sense, especially when you are not too familiar with statistics. However, in the following we will make one step after the other, so that any of you will be able to follow and understand the next step. This will include at first the software set-up, which is quite easy but highly important for further steps. After that, we will introduce you the structure of R and the basic skills. Having you then on a "I now somehow know how to import and calculate things"-level we will go over to the statistical part.
Before you now go on and start with your R career, answer yourself some questions:

- Are you able to read, write and calculate?

- Have you ever worked on a computer in your life (surfing, writing a text or dowload something?)

- Are you actually interested in how to understand and analyze data?

If there is any "No" here, then think about it once again. As already said, this is not a herculean task but of course it will need some effort and time to get into R. If it is "yes, yes, and yes" then great! Let's get started, you will probably be able to write "Basic R skills" into your CV, before you even think about it.

<!--chapter:end:01_Introduction.Rmd-->

---
title: "Chapter_01_R_10"
author: "PGaulke"
date: "13 Juli 2019"
output: html_document
---
# (PART) Explaining R {-}

# Setup

You will need the following software:

The R software itself, RStudio (so to say, the environment where you will work in) and a Latex distribution for creating output files such as pdf files with R graphics.

R Software - https://cran.uni-muenster.de/

FreeVersion of RStudio -  https://www.rstudio.com/products/rstudio/download/#download

Latex distribution - for example: https://www.latex-project.org/get/ (depending on your software)


## Start Your Project

In order to share your work, GitHub is a tool of major importance. In the following, we will explain to you how to set this up. It is totally up to you whether to install it now or later. Please just consider, that you should install it before you start your  project. Otherwise you will face a quite complicated process and limited possibilities to fully enjoy all collaborational features that GitHub provides.

The software you need for this is Git Distribution.

Git distribution - https://git-scm.com/downloads

In parallel of installing the Git distribution, go to https://github.com and create an account.

In a first step, you should activate Git in RStudio. Therefore choose: 'Tools' > 'Global Options' > 'Git/SVN' and click on the button to enable the version control interface. Additionally, you should generate a SHH RSA key which will be needed in a later stage when setting up your repository on GitHub.

Now you can create your project. Click on 'File' > 'New Project' > 'Existing directory' and choose where you want to place your project on your computer. Be aware, that you really know where you place it as you will need the directory in a later stage.

Now it is time to prepare for the marriage of your GitHub account and your project. Go to GitHub and create a new repository and name it exactly the same way as you named your R project. The naming has to be identical.
Next, got to your settings in GitHub and choose 'SHH and GPG Keys' and click on 'New SHH key'. Go back to RStudio and copy the SHH key that has been created in the first step, then paste it into your GitHub account.

Now everything is set up to create the connection. In order to do so, go in RStudio to 'Tools' > 'Project Options' > 'Git/SVN'. Select 'Git' in the Version control system field. After that, go again to 'Tools' > 'Terminal' > 'New Terminal'. Now next to the console a terminal should appear.

As you may already read when you finished your the creation of your repository, here you should type in the following commands:


```{r, eval=FALSE}
git init


git remote add origin https://github.com/YOURNAME/YOURREPOSITORY.git
git push -u origin master


 
```

Obvisously you should adopt the origin link with your own names. 

Now restart RStudio and enjoy that you just completed to connect your project with GitHub. In the upper right corner of RStudio you should now see a Git button (right next to environment/history/connections). To put your files on GitHub, you can now easily commit and then push all files you want to share.

## Work Collaboratively

If you are interested to work simoultanesly with another person on one project, you can create a team in GitHub. However, this is not done by a few clicks.

At first, you have to decide who the owner of the project should be. The role will not have too much influence later on, but it defines the set up for each participant.

The project owner needs to create an organization on Github. Within this organization a new repository should be created, which again should have exactly the same name as the project in R Studio. Herefore, you can follow the steps described above.
If you have done that, you should create a team in the organization and add the further participants to the team. Be aware that you should assign the repository to these members and provide the members with respective rights. 

As the team member who should have received an email at this point. After you confirmed the participation, go onto the repository and click on the button 'Clone or download'. Look for the https adress, copy it and go now into a simple RStudio session (not a project). Open a new Terminal and type in cd with the path where you want to save the project.
Now typ 'git clone' and paste the the copied link from GitHub.

```{r, eval=FALSE}
cd Dropbox/Master/2Sem
git clone https://github.com/ORGANIZATION/repo.git 
```

Now you should finally be able to push and pull the all files and start your collaborative R project.

<!--chapter:end:02_Chapter_01_R_Setup.Rmd-->

---
title: "03_Chapter_02_RMarkdown"
author: "PGaulke"
date: "14 Juli 2019"
output: html_document
---

# Creating Files in R

For working in R we use R Markdown documents. Click on 'File' > 'New File' and then create a new R Markdown file.
R Markdown files include simple formatting syntax for authoring HTML, PDF and Microsoft Word documents. If you look for any specific information about R Markdown which is not included in this book, check this link http://rmarkdown.rstudio.com.

Basically, there are two ways to add something to an R Markdown file.

  - Written text, in which you can include some inline-code by starting with a backstick plus r and ending with a backstick e.g. `r 2*3`. However this kind of code integration will be used less often in this book.
  - Chunks, seperate grey fields that are used to integrate code. You can create a chunk by entering three backsticks plus {r} and end it with again three backticks
  
Below a short example for a chunk

```{r}

2*3

```

In order to process the operation that you entered, you have to click on the green arrow in the right corner. This is what we call to run a chunk.
While working through this book, you will create many chunks with different operations. This includes not only mathematical operations, but also the creation of graphs. There are various ways how to write code in a chunk, we will provide you in the following chapters with more insights, so that you will be able chose the most efficient and fastest ways for each purpose.

In the following, both ways of adding something to a R Markdown file will be illustrated in detail.


## Designing written text

General advises of how to edit written text can you find on the following website: https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf

This cheatsheet contains all inline-formatting options that you can use in base R.
It also shows you how to understand the use of block level elements. By using a hashtag, you can create headlines. The logic is that:

One # - main-headline
Two # - sub-headline
Three # - sub-headline of degree 2

It is important to know that you can only use on main-headline per R markdown file. In order to include more main-headlines you should another R markdown file in your project. For example, in case that you create a book it is recommendable to create one R markdown file per chapter. That is not only more attractive because of the usability of main-headline, but it also provides a better overview of your work.

Of course, formatting is not limited to that. The actual output can be edited in various ways by using YAML.
You find your YAML-header in each of your R markdown files. However, if you want to adopt your files generally rather than specific for each R markdown file you can use the output.yml file. As soon as you create a project, you will find a file in your project that is called output.yml. Settings that you include here will work as a standard for your project, so that you do not need to adopt each R markdown file.

Considering that you probably want to produce a book, respectively create output in form of a PDF, HTML, or other kind of file, it will be necessary to include some information in the header.

For example, if you want to build a PDF file, you have to include:

```
bookdown::pdf_book:
  includes:
    in_header: preamble.tex
```

However, for further information on how to produce output, please check Chapter 3 - (@ref/Creation of Output)


HIER NOCH EINE INFORMATION ZU DEN GESTALTUNGSMÖGLICHKEITEN BEI YAML


## Coding in chunks

Actually, this is the main topic of this book. The explanation of coding in R chunks will not be limited to this chapter, but at this point it is helpful to get an overview of how chunks work and what you can do with them.

The sample chunk in the beginning of this chapter, already revealed the general functionality. Nevertheless, there some general notes to make.

### Chunk options

At first, different options can be set for a chunk, for example wheter to evaluate the code chunk, to stop processing when an error occurs, or to dispay the source code in your output file and a lot more. This can be done by editing the content of the brackets.

If you add eval=TRUE/FALSE, then the whole chunk will be evaluated respectively not evaluated.


```{r, eval=TRUE}
42+17
82-2

```
```{r, eval=FALSE}
42+17
82-2

```

If you add error=TRUE/FALSE, then the run-process will be stopped, respecitively not stopped, if an error occures.

```{r, error=TRUE, eval=FALSE}
100*apples
9*3
```
```{r, error=FALSE, eval=FALSE}
100*apples
9*3
```

If you add echo=TRUE/FALSE, then the source code will be displayed respecitively not displayed in the output file.

```{r, echo=TRUE}
10+27
8+6

```

```{r, echo=FALSE}
10+27
8+6

```

For a detailed explanation for most of all options, please see here https://yihui.name/knitr/options/.

Looking into what you can add into the chunk, you have to be aware that any small mistake will mess up the whole chunk. Especially, for larger and more complex chunks this is a challenge. Therefore, it might be a good idea to make comments for later understanding. Comments can be made by using a hashtag when starting a new line in a chunk. Here an example:

```{r}
10*750000/17
#Just a random calculation

```

### Chunk Functions

As a basic, we can use the chunk as a *calculator*.

We can build up easier calculations as well as more difficult calculations, as far as our keyboard allows us.

```{r}

8*4+12

```

```{r}

37*4235+(19*245)/422+3-10

```

```{r}

sin(40*9)+log(120)

```

Moreover, R provides built-in functions that you can easily use to exercise special operations. In the following example, a sequence will be created by using the function - seq -

```{r}

seq(1,5)

seq(1,10,length.out = 3)


```

More built-in functions can be found under the following link: https://www.statmethods.net/management/functions.html


Another function that is included in chunks is to *name operations*. Naming advantages can be beneficial if you want to use the operational multiple times in your chunk.


```{r}

a <-905/12*5

sin(a)+sin(a^2)+sin(a^3)

```

<!--chapter:end:03_Chapter_02_RMarkdown.Rmd-->

---
title: "04_Chapter_03_Output"
author: "PGaulke"
date: "14 Juli 2019"
output: html_document
---

# Creation of output

By selecting Knit you can create a file out of your .rmd's. R Studio supports various formats which you can set in the header of any .rmd file.

```{r, eval=FALSE}
output: html_document
```

Otherwise you can also select a format in the dropdown menu oft the Knit-button.

Of course, there are various ways of formatting your output. Herefore, you have to use the fields below your output format in the header.
To know which options you can choose for every output format, just check the respective help page. To open the help page, you have to type in ?rmarkdown::(here output format) into the console. 


Another option you have, is to build a book. 
In case you want to compile all your rmd.file to one book, you can call the render function in bookdown. In order to this, you have to download the bookdown package. This is easily done, by clicking on *Tools*, then *Install packages* and search for bookdown. There is more than only one way to download packages, also chunks provide this option by searching for the packages like this:

```{r, eval=FALSE}

install.packages("bookdown")

```

To now prepare for building a book, please go into the .yml file of your project and set the options up accordingly. For example to create a .pdf book, type in this:

```{r, eval=FALSE}

bookdown::pdf_book:

```

This should then be further set up, for options you can again use the console:

```{r, eval=FALSE}
?bookdown::pdf:book
```

<!--chapter:end:04_Chapter_03_Output.Rmd-->

---
title: "05_Chapter_04_Basic_R_Skills"
author: "PGaulke"
date: "16 Juli 2019"
output: html_document
---
# Basic R Skills

Now we dive a little bit deeper into R and go trough the basics of how to handle data.
For this, it is necessary to get an understanding of the most important data structures that do exist, what kind of data they may include and in what kind of format they are. Furthermore, we will introduce you some major rules which should be considered while handling data in R as well as how to import data and what packages might be useful in order to handle data effective and efficiently. Before closing this chapter, also a short overview on how to visualize data is given.

After gathering all the informations and knowledge, it will be possible for you to work with statistics in R, which will be the topic of the second part of this book.



## Data stuctures

In general there are four types of data structures: atomic vectors, lists, matrix and arrays, and data frames.


The most common type of data structure are *atomic vectors*. Vectors can be described by three attributes:

1. the type `typeof()`

Vectors can be either numeric, logical, or character.

```{r}
numeric_vector <- c(1,2,3,4,5) # numeric vector
logical_vector <- c(TRUE,TRUE,TRUE,FALSE) # logical vector
character_vector <- c("first", "apple", "child", "word") # character vector
```

Further, you can also create integer and mixed vectors

```{r}
integer_vector <- c(10L, 4L, 7L) # integer vector
mixed_vector <- c(2, "mixed") # mixed vector
```

When you create a mixed vector and you do not determine which type of vector you create, than R decides on itself. The logic is: logical < numeric < character.


Numeric vectors can be also created by only using 'x:y' if you want to include all numbers from x to y.
```{r}
another_numeric <- 1:5
```


2. the length `length()`

The length basically describes the size of the vector.
If you want to check the length of a vector, you have to use `length()`.

```{r}

whatisthelength <- c(1,4,2,1,16,124,54,6,7)

length(whatisthelength)

```


3. the attributes `attributes()`

Attributes define the nature of a specific vector and are relevant for what kind of function can be applied.
The three major attributes are:

  - the names, `names()`,
  - the dimensions, `dim()`,
  - the class, `class`.

However, you can also check attributes in the summary of the vector. Therefore, you have to use `summary()`, which is (as most all other attributes) also applicable on other data structures.


```{r}
atry <- c(1:15)
summary(atry)

anothertry <- c(T,F,T,F)  
summary(anothertry)
```


Another type of data structure are *lists*. A list can include a number of objects, but also another list. Therefore, it is useful in order to gather data into one structure.

```{r}
alist <- list(numbers=c(1:10),
      fruits=c("Banana", "Peach"),
      values=c(T,T,F,T,F))
alist
```


If you want to illustrate a *matrix* than you can also use R for this. Therefore, you have to consider that all columns have the same mode and the same length.
In general the formel to use is:

```{r}

amatrix <- matrix(c(1:12), nrow=4, ncol=3) 
amatrix

```


Another essential data structure are *data frames*. Somehow the data frame is similar to the matrix, but you can use different modes. Apart from some build-in data sets that are provided in a data frame layout, you can create a data frame by yourself by using the function data.frame.

```{r}

adf <- data.frame(numbers=1:4,
                  fruits= c("banana", "peaches", "orange", "strawberry"),
                  value= c(T,F,F,T))
adf

```
By the way, the length of data frame is determined by the number of columns you include. For our example, you can check the length like this:

```{r}

adf <- data.frame(numbers=1:4,
                  fruits= c("banana", "peaches", "orange", "strawberry"),
                  value= c(T,F,F,T))
length(adf)

```

## Principles in R Chunks

There are some major calculation principles that you have to consider while working with R. For instance, these principles can be quite helpful but being not aware of there existence might lead to errors that are difficult to detect.

At first, we have to consider that *element by element evaluation* is active. In case that you want to somehow create a calculation with two or more vectors, this principle is of major importance.

In case of two numeric vectors of the same length, the calculation will be applied on each element in the same position.

```{r}

store1revenue <- c(10000,12000,18000,9000,11000)
store2revenue <- c(25000,29000,21000,23000,24000)

length(store1revenue) == length(store2revenue)

revenuesum <- store1revenue+store2revenue

revenuesum

```

Every element of the first vector is added to the element that is in the same position in the second vector.

Now, if you violate the premise that the vectors used have the same length, the second principle will be activated. *Recycling* happens and the objects included int he shortest vector will be repetivtively used for the calculation.


```{r}
performanceofa <- c(34,39,51,45,28,37)
performanceofb <- c(30,29,45,42)
performancesum <- performanceofa+performanceofb
performancesum


```

If recycing happens, you will receive an error message which is apparently not an error message that is stopping any process, but making you aware of the recycling.


Another important thing, which is less a principle but more a shortcut, is the *deletion of NA's*.
Sometimes you want to take a vector for a calculation that will take all values of that vector into account. This might lead to difficulties, as missing values are often replaced with `NA` in data sets.
However, with using 'na.rm = T, R will ignore the NA during the calculation.


```{r}
horsepower <- c(400,320,190,200,310,290,420,NA,230,220)
mean(horsepower, na.rm = T)

```


## Subsetting

Subsetting means to create a data set out of the existing data structure. So to say, you copy particular items out of a data collection.

There are three main operators which can be used to subset:

- `[]`
- `[[]]`
- `$`


The first one, `[]`can be applied on all discussed data strucures - vectors, lists, matrices, and data frames.

In case of a vector, you can easily use it these ways:

```{r} 
vec <- c(-7,4,12,6,-2,1,3,-3)

# subsetting only one element by naming the position of the element
vec[3]

# subsetting several elements in a row
vec[c(2:6)]

# subsetting all elements but not the named ones
vec[c(-2,-4)]

# subsetting elements by logical selection
vec[c(T,F,T,F)] # recycling eventually activated

```

If you have a list, than you have to be even more careful about where the data is placed.

```{r}
alist <- list(numbers=c(1:10),
      fruits=c("Banana", "Peach"),
      values=c(T,T,F,T,F))

# subsetting a specific data set in the list
alist[2]

```

Applying `[]` on a matrix requires again a different logic. To understand all dimension, you can use `str()`, which shows you the exact length of the matrix columns and rows.

```{r}

amatrix <- matrix(c(1:12), nrow=4, ncol=3) 

# subsetting one particular row
amatrix[2,]

# subsetting one particular column
amatrix[,2]

# subsetting one specific value
amatrix[2,2]

```

For data frames the use of `[]` is limited, as you can only subset the class.

```{r}

adf <- data.frame(numbers=1:4,
                  fruits= c("banana", "peaches", "orange", "strawberry"),
                  value= c(T,F,F,T))

# subsetting the whole class

adf[1]

```

The second operator, `[[]]`, is mostly used for lists. It is quite similar to `[]`, but is important to differentiate within values.

```{r}
alist <- list(numbers=c(1:10),
      fruits=c("Banana", "Peach"),
      values=c(T,T,F,T,F))

# subsetting a specific data set in the list
alist[[2]]

# subsetting a specific element within a data set
alist[[2]][1]
```


The thrid operator, `$`, is especially used for data frames. You can subset a whole variable, even if you only partially match the variable name.

```{r}

adf <- data.frame(numbers=1:4,
                  fruits= c("banana", "peaches", "orange", "strawberry"),
                  value= c(T,F,F,T))

# subsetting a whole variable

adf$numbers

# even with partial matched naming

adf$num
```


Of course, you can combine the subset operators to create the desired data set. However, if you want to precisely dissect numeric data, then you can use conditions.


```{r}

adf <- data.frame(numbers=1:4,
                  fruits= c("banana", "peaches", "orange", "strawberry"),
                  value= c(T,F,F,T))

# subset a specific number

adf[adf$numbers==2,]

# subset a number that is higher/lower than

adf[adf$numbers<=3,]

```

By the way, you can also assign/replace new numbers by using conditions.

```{r}

adf <- data.frame(numbers=1:4,
                  fruits= c("banana", "peaches", "orange", "strawberry"),
                  value= c(T,F,F,T))

adf[adf$numbers==2,] <- 10
adf

```

## Conditions

With the last part of the previous chapter (REF HERE! subsetting chapter), we implicitly introduced conditions in R. However, writing conditions is not to difficult in the first place and can be used in various ways

```{r}

points <- c(12,4,15,7,10)
sum(points)

if (sum(points) >30) {
  print("Passed")
} else  {
  print ("Failed")
}
# else statement for display something else in case the if condition is false

```

Of course, more conditions can be added.

```{r}

points <- c(12,4,15,7,10)
sum(points)

if (sum(points) >50) {
  print("Grade A")
} else if (sum(points)>40) {
  print("Grade B")
} else if (sum(points)>30) {
  print("Grade C")
} else if (sum(points)>20) {
  print("Grade D")
} else if (sum(points)>10) {
  print("Grade E")  
} else if (sum(points) >=0){
  print("Grade F")
}
  
  
```


## How to Write Functions

To really calculate and use statistical methods, you should be able to write all functions in R chunks. This might lead to difficulties, because not all function are built-in or included in a package. In order to be able to write functions on your own, you will need to understand the following logic.#


```{r}

# designing a simple function

firstfunction <- function(a){
  a^2
}

firstfunction(4)

# firstfunction is a random name for one function
```

To receive several returns on more than one expression in the function, you should create a list.

```{r }
onemorefunction<- function(a){
  list(ff=a^2, sf=a^3, tf=a^4) 
}

onemorefunction(2:5)

# In case of further calculations with the returns, you should assign it to an object

furthercalc <- onemorefunction(2:5)

furthercalc$ff
```

Appropriately to what you need to do, you can include more variables in your function

```{r}
superfunction <- function(x, y){
  y*x^2
}
superfunction(4, 2)
```

In case that you do not want to only trust on reproducing the order, than you can also call the variable to return correctly.

```{r}
afunc <- function(c, d, g){
  g/c*d^2
}
afunc(g=4, c=2, d=10)
```

In order to now combin knowledge from the previous subchapter with this one, we can create conditions dependent on functions.

```{r}
roots <- function(a, b, c){
  
  if (b^4- 3*a*c <0) {
    print("No solution! (negativ root can't be squared)")
  } else {}
  
  (-b + sqrt (b^2- 4*a*c)) / 2*a
}
roots(a=2, b=3, c=1)
roots(a=4, b=1, c=2)
```

<!--chapter:end:05_Chapter_04_Data_Analysis.Rmd-->

---
title: "06_Chapter_05_Packages in R"
author: "PGaulke"
date: "22 Juli 2019"
output: html_document
---

# Data Sets, Visualisation, and Packages in R

You already learned that R provides some built-in functions (such as `seq()`) that make your work more comfortable. However R provides also built-in data sets, that you can use for example calculation or data analysis.

One example for this is the data set mtcars.

```{r dataframe}
mtcars

data(mtcars)
class(mtcars)
mtcars
head(mtcars)
str(mtcars)

names(mtcars) 
length(mtcars)
nrow(mtcars)
```
You can find all built-in data sets here: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html

However, apart from built-in functions and built-in data sets, there is even more to explore. In the following, we will explain how to create your individual and best R environment.

## Import Data

The actual idea of this book is that we want to enable you to analyze data in R. It will be barely possible to do so without being able to import the data you want to analyze in R. Therfore, we want to put data from other files into a data frame in order to work with it in R. With Base R, this is possibly for at least some types of files, however, for others there are some special packages to use which we wil thematize in the chapter //HERE THE REF.


Generally, there are different funtion in how to read a file. The most common one is `read.table`. With this function you can read rectangular data and convert it into a data frame. For all the arguments you should check the help section `?read.table`.
Most importantly you have the arguement `file` which requires a path to find a data. If you have the file in the same folder, then the name of the file is enough. Also of high importance is the `sep` arguement which indicates the character that seperates the values between different columns.


```{r, eval=FALSE}
randomdatafile <- read.table(file="filename.txt",
                             sep=",")

``` 

For other files, R follows the logic of `read.xxx`. The xxx specifies the data format (e.g. read.csv -> .csv files)

## Data Visualisation

In the following, we will describe how you can visualize data in R. This will be limited to the base R functions, in the chapter \@ref(ggplot2) you will find a way to plot data more effectively.

In order to start with this topic, at first we will look at the simplest way of plotting.

The *scatter plot* is a simple line plot in which you plot one variable against an index on the x axis Both vectors need to have exactly the same length, and of course, they need to be numeric.

The main function to use here is `plot`.

```{r simple-plots-1, out.width="25%", fig.align="center", fig.height=10, fig.cap="Creating a simple line plot.", fig.show="hold"}
revenue <- c(59000, 58000, 62000, 62000, 65000, 66000)
month <- c(01, 02, 03, 04, 05, 06)

plot(month,revenue, type ="l")
```


This looks pretty plain, so we can add some individual arguments. Show 
We now add further customization with new functions and arguments.  

- `col` adding a color (for details:(http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf)
- `lty` setting the line type (for details: http://www.sthda.com/english/wiki/line-types-in-r-lty)
- `lines` adding the plot of a vector to a previously opened plot.
- `axis` changing the axis given in its first argument with 1, 2, 3, 4 (bottom, left, top, and right)
- `at` stating for what values of the axis the labels should correspond.
- `las` stating if lables are showed horizontal or vertical
- `xlab` and `ylab` are the x and y axes labels, respectively.
- `xlim` and `ylim` set a numerical limit for the x and y axes labels, respectively, notice that a vector of length 2 is necessary for each.

The following arguements need to included in the chunk, but seperately from the actual function.
- `legend`setting a legend for the plot
- `title` setting a title for the plot

```{r simple-plots-2, out.width="25%", fig.align="center", fig.height=10, fig.cap="Creating a simple line plot.", fig.show="hold"}
revenue <- c(59000, 58000, 62000, 62000, 65000, 66000)
month <- c(01, 02, 03, 04, 05, 06)

plot(month,revenue, type ="l", col="blue",
     axes=TRUE,
     xlab = "Month",
     ylab = "Revenue"
     )
title (main="Halfyearly Plot")
```

Alternatively, you can also create barplots, histograms, boxplots, pies and other plots. For example, for a barplot you should take the function `barplot` and consider the following arguments.


- `col` for setting the colors
- `horiz` for setting the direction of the plot
- `border` setting the design of the borders of the plots
- `beside` forces side-by-side bars instead of stacking bars

```{r bar-plots, out.width="25%", fig.align="center", fig.height=7, fig.cap="Simple bar plots.", fig.show="hold"}

barplot(revenue,
        main="Monthly Revenue",
        names.arg=c("Jan","Feb","Mar","Apr","May","June"),
        border="black",
        col = "blue")


```


## Data Packages

By installing new packages (again: Tools > Install packages) you can download additional tools for R, that gives you access to more operations, functions, and coding options. Before we introduce some major R packages that will make data science a bit easier and faster, please consider this short notice.

In case that you use anything out of an additional R package that you downloaded, you always have to include the following process when reopening the respective project.

```{r, eval=FALSE,error=TRUE}

library(package)

```
This step is necessary to reload the package and use its functions. You do not necessarily need to reinstall the whole package, but loading it from your library will definitely be required.


###MagrittR

Now you will meet a complete new operator for the first time that comes with the package `magrittr`. This operator is called pipe `%>%`. It provides a different way of writing operations into chunks, by which you type in your operation from left to right, instead from the outside to the inside.
From a mathematical point of view, this means`x %>% f` is equivalent to `f(x)`, `x %>% f(y)` is equivalent to `f(x, y)`, and `x %>% f %>% g %>% h`is equivalent to `h(g(f(x)))`


```{r}
require(magrittr)

somenumbers <- c(200,300,700,50,400)
sum(somenumbers)

somenumbers %>%
	sum()

sqrt(sum(somenumbers))

somenumbers %>%
	sum() %>%
	sqrt()


```

The transformation process of data frames can be processed in one operation with piping.

df_after_f <-f(df)
df_after_g <-g(df_after_f)
df_after_h <-g(df_after_g)

with piping it is

df %>%
  f %>%
  g %>%
  h
  
Furthermore, you can also use placeholders for an element that you placed before the pipe.


```{r}

#single placeholder
round(1.66666666,2)

2 %>%
	round(1.66666666, .)

#multiple placeholders
mtcars %>%
  subset(hp > 100) %>%
  aggregate(. ~ mpg,.,mean)
```


## Tidyverse

Tidyverse is a large package that basically includes different packages such as `tibble`, `tidyr`,`readr`, `dplyr` and `ggplot2`. Considering all the functions and possibilties that tidyverse provides, it can be seen a subdialect of R. For a detailed overview of what tidyvere is, and what's included, see here: https://www.tidyverse.org/. Especially, the cheat sheets for ReadR and TidyR are recommendable: https://rawgit.com/rstudio/cheatsheets/master/data-import.pdf.

As a first step, please load `tidyverse`.

```{r, eval=FALSE, error=TRUE}

install.packages("tidyverse")

```

#### tibble

In a first step, we will go through the function and benefit of `tibble`. Tibble is generally a description for a data frame in tidyverse. All tibbles are data frames, but not vice versa.
Using tibble instead of regular data frames provides us benefits in terms of pace, output, informations, and simplicity.

A data set is easily created as a tibble, therefore you have to options:

```{r}

library(tidyverse)

# creating a new data set as a tibble from scratch
new_tib <- tibble(
  a = 1:5, 
  b = 5, 
  c = 20:16,
  d = 3:7
)
new_tib

# converting an existing data set into a tibble


tibmtcars <- as_tibble(mtcars)
tibmtcars

```

#### tidyr

In order to continue with `tidyr`, we are now more about how to organize a data set.
The principle of tidy data is that, that every column is a variable, every row an obersvation and every type of observation belongs in a different table. Tidyr is mainly based upon the following functions:

- `gather`
- `spread`
- `seperate`
- `unite`

To `gather`is the function that let you create key-value pairs out of multiple pairs. A large horizontal data set can therefore be converted in a vertically larger data set. This can be beneficial in order to get a clear overview on the data set.



```{r}
pricing <- tibble(type= c("B2C","B2B"),
                  productA= c(20,15),
                  productB= c(75,70),
                  productC= c(30,20),
                  productD= c(60,55),
                  productE= c(15,10)
                    )
pricing

```
Taking this example, we see that we actuall have the following three variables: type of business, product, and price. However, we have 6 columns, which obviously does not correspond to a tidy data set, in which every variable is a columnn.
Therefore, we should now tidy the data set up by considering the following logic:
- `key`, which are the messy columns (here the products)
- `value`, which are the messy values in the messy cells (here the prices)


```{r}
#The empty call is:
#gather(df, key, value, messy_col1, ..., messy_coln)

require(tidyr)

tidy_pricing <- gather(pricing,
                        key= "products",
                        value= "price",
                        productA:productE
                        ) 
tidy_pricing


```


In contrast, the function `spread` works the opposite way. Therefore, it creates a horzontally larger data set by increasing the amounts of columns according to the given variables.

```{r}
sales<- tibble(
  business= c(rep(c("B2B","B2C","Mixed"),2),"Philantrophy"),
  products= c(rep(c("product", "revenue"),3), "donation"),
  details= c("productA", 300, "productC", 240, "productB", 120, 50)
        )

sales

tidy_sales<- spread(sales, key=products, value=details)
tidy_sales

```

The function `seperate` does what its name implies, it seperates columns. The seperation can be done by different ways, you can let recycling do its work, or base it on numbers and characters.

```{r}
require(tidyverse)

# The empty call is
# separate(df, messy_var, into=c(tidy_var1, tidy_var2))

#Example for using recycling

mixedup <- tibble(info=c("Shanghai,China", "Oslo,Norway"))
mixedup

tidy_mixedup <- separate(mixedup,
                          info,
                          into= c("city", "country")
                          )

tidy_mixedup
                  
# Example for using characters

tidy_mixedup2 <- separate(mixedup,
                          info,
                          into=c("city","country"),
                          sep="a")
tidy_mixedup2

# Example for using numbers of characters

tidy_mixedup3 <- separate(mixedup,
                          info,
                          into=c("city","country"),
                          sep=5)
tidy_mixedup3


```  

Finally, the function `unite` can be simply used for the opposite. By this function you can put two columns together.

```{r}
# The empty call is:
# unite(df, tidy_var, messy_var1, messy_var2, sep="")


backtotheorigin_mixedup <- unite(tidy_mixedup, info, "city", "country", sep=",")
backtotheorigin_mixedup
```

#### readr

The package readr provides you a fast and comfortable way of using data from other data formats.
The follwing file formats are supported by readr

- read_csv(): comma separated (CSV) files
- read_tsv(): tab separated files
- read_delim(): general delimited files
- read_fwf(): fixed width files
- read_table(): tabular files where columns are separated by white-space.
- read_log(): web log files

Readr tries automatically to convert the data from the file into a tibble data set in a way, that column specification is as appropriate as possible. These are basically the main advantages, beside that it is much faster than base R imports.

For us, the most important files are .CSV files as most data sets are create in Excel-files. However, it is pretty easy to just drop the file in your project folder and then use this formula:

```{r, eval=FALSE, error=FALSE}


idea_of_a_name <- read_csv(readr_example("filename.csv"), col_types = 
  cols(
    firstcolumnname = col_double(),
    secondcolumnname = col_integer(),
    thirdcolumnname = col_character(),
    etc = col_integer(),
      )
)


```

#### deplyr

`Dplyr`is a toolset for data manipulation. The packages includes five essential function, which are the following:

- select() picks variables based on their names
- mutate() adds new variables that are functions of existing variables 
both of the two above applied on columns 

- filter() picks cases based on their values
- arrange() changes the ordering of the rows
both of the two above applied on rows

- summarise() creates a summary out of multiple data sets

Before we start with the above mentioned functions, first things first, under the following link you will find a cheat sheet: https://github.com/rstudio/cheatsheets/blob/master/data-transformation.pdf.


As you already met the piping operator `%>%`  in the chapter about magrittr(HIER REF), we will apply it within this chapter. Especially, when manipulating a data set, piping provides an easier and more efficient approach than base R. Furthermore, you can combine different functions of manipulation in one step.

```{r}
starwars
```


First we will start with the operator `select`. 


```{r}
# The empty call is (base R)
# select(df, var1, ..., varn)
# or with piping...
# df %>%
#   select(var1,..., varn)

#here a practical example with mtcars

mtcars[,c("mpg","hp")]


mtcars %>%
  select(mpg,hp)



```
Furthermore, you can select in the following ways:

```{r}
#by columns

mtcars %>%
  select(1:3)

mtcars %>%
  select(mpg:hp)

mtcars %>%
  select(-5)

mtcars %>%
  select(-hp)


```

The selection can be designed very individually by using helper arguments that describe for example a word that should be included in the variable. All helper arguments can be found in the help section:`?select`


The `mutate` function provides the possibilty to create new columns based on existing ones.

```{r}
# The empty call is (base R)
# mutate(df, new_variable = expression)
# or with piping...
# df %>%
#   mutate(new_variable = expression)

#Practical example:

mutate(mtcars, kmpg = mpg*1.60934)


mtcars %>%
  mutate(kmpg= mpg*1.60934)


```

The `filter` function is somehow similar to the `select` function but for rows. The procedure is more or less the same.

```{r}
# The empty call is (base R)
# filter(df, condition)
# or with piping...
# df %>%
#   filter(condition)

mtcars[mtcars$mpg >=20,]

mtcars %>%
  filter(mpg >= 20)

```

You find all operators for conditions in the help section: ?Comparison


The function `arrange` enables you to create a new order by considering the value of variable.

```{r}
# The empty call is (base R)
# arrange(df, var1)
# or with piping...
# df %>%
#   arrange(var1)

arrange(mtcars, desc(hp))

mtcars %>%
  arrange(desc(hp))



```


The fiveth function `summarise` provides the possibility to create a new data frame by deriving summarzing calculations from an existing data set.

The expr means a function on a vector, repsectively a variable.

```{r}
# The empty call is (base R)
# summarise(df, name = expr)
# or with piping...
# df %>%
#   summarise(name = expr)

summarise(mtcars, averagepower = mean(hp))

mtcars %>%
  summarise(averagepower = mean(hp))
```  
Again, helper function can be found in the help section `?summarise`


For some more specialised manipulation tasks you can use the function `group_by` which allows you to choose a specific group on which to apply your manipulation operation.

```{r}
mtcars %>%
  group_by(hp >200) %>%
  summarise(
    mean(mpg)
  )
# interesting result by the way

```
#### ggplot2 {#ggplot}

With the package ggplot2 you have more possiblities to visualize your data. The range in a plot will adopt automatically to new data, it will be drwan as an object instead of an image, the legend can be created automatically and the framework for plotting is unified.


Creating a scatter plot for example, works like this:

```{r, fig.show='hold', results='hide', tidy=FALSE}
library(ggplot2)
newggplot<- ggplot(mtcars, aes(x=cyl, y=hp)) + 
         geom_smooth(method=lm , se=FALSE, aes(col=cyl)) +
         geom_smooth(method=lm , se=FALSE, linetype=1, col="grey", aes(group=1)) + 
  labs(subtitle="Car Power", 
       y="Horsepower", 
       x="Cylinders", 
       title="Scatterplot", 
       caption = "Source: mtcars")

plot(newggplot)


```

The way how to write with ggplot2 might look confusing in the first moment, but there is a consistent logic behind that.


```{r}
# First you choose the data and set the mapping

Superplot <- ggplot(data=mtcars, mapping= aes(x=hp, y= mpg))
Superplot
# The you tell the general form

SP2 <- Superplot + 
      geom_line()  
SP2
# Then you have the possibility to add a variable

SP3 <- ggplot(mtcars, aes(x=hp, y= mpg, col="blue")) +
      geom_point()
SP3
# Or even attributes

SP3b <- ggplot(mtcars, aes(x=hp, y= mpg)) +
      geom_point(col="blue")
SP3b
```

By `aes` we map our data, which includes to tell x/y axis, colour, fill, size, labels, line widt, line type.

By `geom` we set the shape of our object (_point,_line,_histogram,_bar,_boxplot) and set attributes.

<!--chapter:end:06_Chapter_05_Packages_in_R.Rmd-->

---
title: "08_Ersatzfile"
author: "PGaulke"
date: "24 Juli 2019"
output: html_document
---
# (PART) Statistics for Data Science {-}
# Predictive and appropriate model fitting 

http://127.0.0.1:30892/rmd_output/1/creating-files-in-r.html

```{r, include=FALSE}
#ausblenden
library(readr)
advertising = read_csv("data/advertising.csv")
```




People want to make predictions because nothing is clearly true in this world. The answers of the predictions are based on data, and not on intuition. Therefore, people make use of predictive modeling in order to forecast future actions through using data and calculations of propability. Every predictive model has some predictive variables which can influence future actions. 

Today AI is a big topic. However, AI will not tell us WHY something happened, it will not answer questions of "What if, when this..." it only presents a complicated set of correlations but not the causations. 

Moreover, in preditive modeling, there is no lunch theorem. There is not only one technique. There are many techniques, some of them are better, some not. Therefore, it is the goal to find out which works best. 

The model can be a simple linear equation or a complex tree-based model. 


## Building models and predictions 

In the process of predictive modeling, first data is collected for the predictive variables before the actutal model is build. 

Example 

The wage is correlated with the age. A variable to predict is needed. 

\[Y=f(X) + \varepsilon \]

y: response variable 
f(x): set of independent (1, 2, 3), can be inifitive functions, but then you can't present it anymore
e: shock 

This example will be based on simulated data. Knowing the truth will be very helpful. But you can't know the truth unless you simulate it. 


In this model, we need to find f, the predictions. Therefore, you take X in order to see what you can predict. 

In many occasions, the independent variables are known but the response is not. Therefore, \(f()\) can be used to _predict_ these values. These predictions are noted by
\[\hat{Y}=\hat{f}(X)\]
where \(\hat{f}\) is the estimated function for \(f()\).



## Regression 

Regression is a type of supervised learning. In supervised learning, addresses issues whre thre are both an input and an output. These issues in regression deal with a numeric output.

For describing the names of variables and methods, different terns are used in AI, statistical or machine learning. 

Input e.g.: predictors, input/feature vector. These inputs can be either numberic or categorical. 

Output e.g.: response, output/outcome, target. These outputs have to be numeric. 

The goal of regression is to make predictions on undetected data. This can be done through controlling the complexity of the model to protect against under- and overfitting.  
Manipulating the model complexity will accomplish this because there is a bias-variance tradeoff. The bias-variance tradeoff increases the flexibility. It is more shaky and closer to the data but it also increases the variance. The sum is always U-Shaped. 

Furthermore, it will be known that the model generalizes because it is evaluating metrics on test data. Only the (train) models on the training data will fit. The analysis begings with a test-train split. In the regression tasks, the metric will be the RMSE. 


The next step after investigating the structure of the data, is to visualize the date. Due to the fact that in regression is only numeric variables, a scatter plot can be used. 

```{r}
plot(sales ~ TV, data = advertising, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "sales vs television advertising")
```
The function pairs() is helpful in order to visualize a number of scatter plots quickly. 

```{r}
pairs(advertising)
```



## Linear regression 

Linear regression is a simple approach to supervised learning. It assumes taht the dependence of Y on X1, X2, ... Xp is linear. 
The linear regression model is very fast. In the following example, the relationship between different advertising methods and sales is visualized. The relationship is not causal, but the correlations can be detected. Every blue points presents an observation. There are several questions which could be asked: 

- Is there a relationship between sales and the advertising budget?
- How strong is the relationship between sales and the advertising budget 
- Which method contributes to sales?
- How precise is the prediction of the future sales?
- Is the relationship linear?
- Is there synergy among the advertising media?


```{r}
library(caret)
featurePlot(x = advertising[ , c("TV", "radio", "newspaper")], y = advertising$sales)
```
In the graph a clear increase in sales can be seen as radio or TV are increased. The relationship between sales and newspaper is less clear. How all of the predictors work together is also unclear, as there is some obvious correlation between radio and TV. 

Simple linear regression using a single predictor X. 

- The assumed model is

Y = ß0 + ß1X+ e, 

ß0 and ß1: two unknown constants that represent the intercept and slope, also known as coefficients or parameters 
e: error term 

- Given some estimates ^ß0 and ^ß1 for the model coefficients, for predicting future sales 

^y= ^ß0 + 1ß1x, 

^y: indicates a prediction of Y on the basis of X=x. 
The hat symbol denotes an estimated value. 



###Assessing Model Accuracy

There are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that is most interesting is the root-mean-square error.

\[MSE=\frac{1}{n}\sum_i^n \big(y_i-\hat{f}(x_i)\big)^2\]


While for the sake of comparing models, the choice between RMSE and MSE is arbitrary, there is a preference for RMSE, as it has the same units as the response variable.


### Model Complexity

Besides the fact how well a model makes precitions, it is also interesting to know the complexity/flexibility of a model. In this chapter, so make it simple, only linear models are considered. In fact, the model gets more complex when more predictors are added to the model. In order to assigning a numerical value to the complexity of the linear modl, the number of predictors $p$ wil be used. 

```{r, include=FALSE} 
get_complexity = function(model) {
  length(coef(model)) - 1
}
```


### Test-Train Split

For the case of determining how well the model predicts, issues with fitting a model to all available data then using RMSE occur. This can be seen as cheating. The RSS and hence the RMSE can never increae when a linear model becomes more complex. Th RSS and the RMSE dan only decrease or in special cases could stay the same. Hence, the believe could arise that a largest model as possible should be used in order to predict well. But this is not the case because it is very difficult to fit to a peculiar data set As soon as a new data is seen, a large model could predict unfortunate. This issue is called **overfitting**. 

It is very useful to split the given data set into two halds, whereby one half is the **training** data, which is used to fit (train) the model. The other half is the **test** data which is used to assess how well the model can predict. It is important that the test data will never be used to train the model. 


In this example, the function `sample()` will be used in order to get the random sample of the rows of the original data set. The next step is to use those rows as well as the remaining row numbers to split the data correspondingly. Moreover, the function `set.seet()` will be applied in order to replicate the same random split everytime the analysis will be performed. 
```{r}
set.seed(9)
num_obs = nrow(advertising)

train_index = sample(num_obs, size = trunc(0.50 * num_obs))
train_data = advertising[train_index, ]
test_data = advertising[-train_index, ]
```


In this example it is important to concentrate on the **train RMSE** and the **test RMSE**. These are two measures which assess how well the model can predict. 


$$
\text{RMSE}_{\text{Train}} = \text{RMSE}(\hat{f}, \text{Train Data}) = \sqrt{\frac{1}{n_{\text{Tr}}}\displaystyle\sum_{i \in \text{Train}}^{}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
$$
In the measure of the train RMSE, $n_{Tr}$ demonstrates the numbers of observations given in the train data set. When the complexity of the linear model increases, the train RMSE will decrease, or in a special case stay the same. Therefore, when comparing the models, the train RMSE is not useful. However, it can be a helful step to prove if the RMSE is going down. 


$$
\text{RMSE}_{\text{Test}} = \text{RMSE}(\hat{f}, \text{Test Data}) = \sqrt{\frac{1}{n_{\text{Te}}}\displaystyle\sum_{i \in \text{Test}}^{}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
$$
In the measure of the test RMSE, $n_{Tr}$ demonstrates the number of observations in the given test data set. In the training data set, the test RMSE is used to fit the model, but assess on the unused test data. This is a procedure for how wll the fitted model is predicting usually, not just how well it fits the data sed to train the modl, as it is the case for the train RMSE. 


```{r}
# starting with a simple linear model, with no predictors
fit_0 = lm(sales ~ 1, data = train_data)
get_complexity(fit_0)

# train RMSE
sqrt(mean((train_data$sales - predict(fit_0, train_data)) ^ 2))
# test RMSE
sqrt(mean((test_data$sales - predict(fit_0, test_data)) ^ 2)) 
```
Interpretation: the operations use the train and the test RMSE. 
```{r}
library(Metrics)
# train RMSE
rmse(actual = train_data$sales, predicted = predict(fit_0, train_data))
# test RMSE
rmse(actual = test_data$sales, predicted = predict(fit_0, test_data))
```
Interpretation: the function can be enhanced with inputs which are obtaining.
It is helpful to use the train and test RMSE for the fitteed model, given a train or test dataset, and the proper response variable.
```{r}
get_rmse = function(model, data, response) {
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}
```
Interpretation: when obtaining this function, the code is better to read and it bcoms more clear which task is being reached. 
```{r}
get_rmse(model = fit_0, data = train_data, response = "sales") # train RMSE
get_rmse(model = fit_0, data = test_data, response = "sales") # test RMSE
```


### Adding Flexibilty to Linear Models

The consecutive model which are fitted will increase flexibility when obtaining interactions and polynomial terms. In the following example, a training error will be decreasing when the model increases in flexibility. It is expected that the test error will decrease a number of times, and will may be increase, as effect of the overfitting. 

```{r}
fit_1 = lm(sales ~ ., data = train_data)
get_complexity(fit_1)

get_rmse(model = fit_1, data = train_data, response = "sales") # train RMSE
get_rmse(model = fit_1, data = test_data, response = "sales") # test RMSE
```

```{r}
fit_2 = lm(sales ~ radio * newspaper * TV, data = train_data)
get_complexity(fit_2)

get_rmse(model = fit_2, data = train_data, response = "sales") # train RMSE
get_rmse(model = fit_2, data = test_data, response = "sales") # test RMSE
```

```{r}
fit_3 = lm(sales ~ radio * newspaper * TV + I(TV ^ 2), data = train_data)
get_complexity(fit_3)

get_rmse(model = fit_3, data = train_data, response = "sales") # train RMSE
get_rmse(model = fit_3, data = test_data, response = "sales") # test RMSE
```
```{r}
fit_4 = lm(sales ~ radio * newspaper * TV + 
           I(TV ^ 2) + I(radio ^ 2) + I(newspaper ^ 2), data = train_data)
get_complexity(fit_4)

get_rmse(model = fit_4, data = train_data, response = "sales") # train RMSE
get_rmse(model = fit_4, data = test_data, response = "sales") # test RMSE
```
```{r}
fit_5 = lm(sales ~ radio * newspaper * TV +
           I(TV ^ 2) * I(radio ^ 2) * I(newspaper ^ 2), data = train_data)
get_complexity(fit_5)

get_rmse(model = fit_5, data = train_data, response = "sales") # train RMSE
get_rmse(model = fit_5, data = test_data, response = "sales") # test RMSE
```

### Choosing a Model 

In order to get a better picture of the relationship between the train RMSE, test RMSE, and model complexity, results are summarized and are cluttered. 

```{r, eval = FALSE}
fit_1 = lm(sales ~ ., data = train_data)
fit_2 = lm(sales ~ radio * newspaper * TV, data = train_data)
fit_3 = lm(sales ~ radio * newspaper * TV + I(TV ^ 2), data = train_data)
fit_4 = lm(sales ~ radio * newspaper * TV + 
           I(TV ^ 2) + I(radio ^ 2) + I(newspaper ^ 2), data = train_data)
fit_5 = lm(sales ~ radio * newspaper * TV +
           I(TV ^ 2) * I(radio ^ 2) * I(newspaper ^ 2), data = train_data)
```
Interpretation: Recalling the models that have been fitted it helpful. 

```{r}
model_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)
```
Interpretation: A list of models is created

```{r}
train_rmse = sapply(model_list, get_rmse, data = train_data, response = "sales")
test_rmse = sapply(model_list, get_rmse, data = test_data, response = "sales")
model_complexity = sapply(model_list, get_complexity)
```
```{r, echo = FALSE, eval = FALSE}
# the following is the same as the apply command above

test_rmse = c(get_rmse(fit_1, test_data, "sales"),
              get_rmse(fit_2, test_data, "sales"),
              get_rmse(fit_3, test_data, "sales"),
              get_rmse(fit_4, test_data, "sales"),
              get_rmse(fit_5, test_data, "sales"))
```
Interpretation: The train RMSE, test RMSE and the model complexity are used for each. 

```{r}
plot(model_complexity, train_rmse, type = "b", 
     ylim = c(min(c(train_rmse, test_rmse)) - 0.02, 
              max(c(train_rmse, test_rmse)) + 0.02), 
     col = "dodgerblue", 
     xlab = "Model Size",
     ylab = "RMSE")
lines(model_complexity, test_rmse, type = "b", col = "darkorange")
```
Interpretation: The results are plotted. The blue line represents the train RMSE and the orange line represents the test RMSE. 


| Model   | Train RMSE        | Test RMSE        | Predictors              |
|---------|-------------------|------------------|-------------------------|
| `fit_1` | 1.6376991         |	1.7375736        | 3                       |
| `fit_2` | 0.7797226         | 1.1103716        | 7                       |
| `fit_3` | 0.4960149	        | 0.7320758	       | 8                       |
| `fit_4` | 0.488771	        | 0.7466312	       | 10                      |
| `fit_5` | 0.4705201	        | 0.8425384	       | 14                      |

Results: 
Overfitting models: A high train RMSE and a high test RMSE can be seen in `fit_1` and `fit_2`

Overfitting models: A low train RMSE and a high test RMSE can be seen in `fit_4`and `fit_5`

##Hypothesis testing 

Standard errors can also be used to perform hypothesis tests on the coefficints. The most common hypothesis task involves testing the null hypothesis of 

H0: There is no relationship between X and Y versus the alternative hypothesis 

HA: There is some relationship between X and Y 

Mathematically, this correspond to testing 

H0 : $\beta_1$ = 0

vs 

HA: $\beta_0$ = 0

since if $\beta_1=0$ then the model reduces to $Y=\beta_0$ + em and X is not associated with Y.

The function  summary() returns a large amount of useful information about a model fit using lm(). Much of it will be helpful for hypothesis testing including individual tests about each predictor, as well as the significance of the regression test.


```{r, eval=FALSE, error=TRUE}
#funktioniert nicht
summary(mod_1)
```

##Confidence interval 


```{r, eval=FALSE, error=TRUE}
#funktioniert nicht
head(predict(mod_1), n = 10)
```
Here it is important to understand that the function predict () is dependent on the input to the function. The first argument is supplying a model object of class lm. Because of this, predict() then runs the function predict.lm(). 

For further information ?predict.lm() can be used. 


```{r}
new_obs = data.frame(TV = 150, radio = 40, newspaper = 1)
```


ERROR, again with X1 ??

```{r, eval=FALSE, error=TRUE}
#funktioniert nicht
predict(mod_1, newdata = new_obs)

predict(mod_1, newdata = new_obs, interval = "confidence")
```

## Multiple Linear Regression 

The model is: 

$$Y=\beta_0+\beta_1x_1 +\beta_2x_2+\ldots+\beta_px_p+e$$

The interpretation is that ßj is the average effect on Y of a one unit increase in Xj, holding all other predictors fixed. In the advertisting example, the model becomes: 

$$sales=\beta_0+\beta_1xTV+\beta_2xradio+\beta_3xnewspaper+e$$

Interpreting regression coefficients 

The ideal scenario is when the predictors are uncorrelated - a balanced design: 
- each coefficient can be estimated and tested separately. 
- interpretations such as "a unit change in Xj is associated with a ßj change in Y, while all the others variables stay fixed", are possible. 
Correlations amongst predictors cause problems 
- the variance of all coefficients tends to increase, sometimes dramatically
- interpretations become hazardous - when Xj changes, everything else changes. 
Claims of causality should be avoided for observational data. 

The woes of (interpreting) regression coefficients. 
"Data Analysis and Regression" Mosteller and Tukey 1977
- a regression coefficient ßj estimated the expected change in Y per unit change in Xj, will all other predictors held fixed. But predictors ususally change together! 


The lm() Function 

In the following example, an additive linear model with sales as the response and each remaining vairbale as a predictor. 


```{r, eval=FALSE, error=TRUE}
#funktioniert nicht
mod_1 = lm(sales ~ ., data = advertising)
mod_1 = lm(sales ~ TV + radio + newspaper, data = advertising)
```

<!--chapter:end:07_Chapter_06_Introduction_to_predictive_statistical_methods.Rmd-->

---
title: "Bias-Variance Tradeoff"
output: pdf_document
---
# Bias-Variance Tradeoff 

In respect to the general regression setup, where a random pair $(X, Y) \in \mathbb{R}^p \times \mathbb{R}$ is given. Here, the goal is to make a prediction of $Y$ with the funcntion of $X$, e.g. $f(X)$. 
In order to assert what it implys to make a prediction, it is useful that $f(X)$ is near to $Y$. To explain meaning of being near to, the squared error loss of estimating of $Y$ through using $f(X)$, will be definded. 

Definition of the squared error loss: 
$$
L(Y, f(X)) \triangleq (Y - f(X)) ^ 2
$$
The next step is to exlain the goal of regrssion, which is to minimize the squared error loss, on average. This can be describes as the risk if estimating $Y$ through using $f(X)$. 

$$
R(Y, f(X)) \triangleq \mathbb{E}[L(Y, f(X))] = \mathbb{E}_{X, Y}[(Y - f(X)) ^ 2]
$$

The risk is first rewrited after conditioning on $X$, before proving to minimize the risk. 

$$
\mathbb{E}_{X, Y} \left[ (Y - f(X)) ^ 2 \right] = \mathbb{E}_{X} \mathbb{E}_{Y \mid X} \left[ ( Y - f(X) ) ^ 2 \mid X = x \right]
$$

The right-hand side is easier to minimize, because it simply amounts to minimizing the inner expectation to $Y \mid X$, particularly minimizing the risk pointwise, for each $x$. 

The regression function, where the risk is minimzied by the conditional mean of $Y$ given, $X$ is written as following: 

$$
f(x) = \mathbb{E}(Y \mid X = x)
$$
An important notice is that the choice of squared error loss is slidely arbitrary. Rather, the absolute error loss can be supposed. 
$$
L(Y, f(X)) \triangleq | Y - f(X) | 
$$
The risk can then be minimzed by the conditional median. 
$$
f(x) = \text{median}(Y \mid X = x)
$$
In spite of this facility, the goal is still the squared error loss. This is because there are historical reasons, as wll as the eas of opimization and the protection against large deviations. 

The next step is, to find $\hat{f}$ that is a good estimat of the regression function $f$, given the data $\mathcal{D} = (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}$. This amounts to minimizing is called **reducible error**. 

## Reducible and Irreducible Error

Expecting that when preserving some $\hat{f}$, the question is how well does it estimate $f$? For this, the **expected prediction error** of predicting $Y$ using $\hat{f}(X)$ is definded. A good $\hat{f}$ will have a low expected prediction error. 

$$
\text{EPE}\left(Y, \hat{f}(X)\right) \triangleq \mathbb{E}_{X, Y, \mathcal{D}} \left[  \left( Y - \hat{f}(X) \right)^2 \right]
$$

This expectation is over $X$, $Y$, and also $\mathcal{D}$. The estimate $\hat{f}$ is actually random depending on the sampled data $\mathcal{D}$. Therefore, it could be actually written $\hat{f}(X, \mathcal{D})$ in order to make this dependence explicit, but the notation will become cumbrous enough as it is.

Hence, $X$ is required. This results in the expected prediction error of predicting $Y$ using $\hat{f}(X)$ when $X = x$. 

$$
\text{EPE}\left(Y, \hat{f}(x)\right) = 
\mathbb{E}_{Y \mid X, \mathcal{D}} \left[  \left(Y - \hat{f}(X) \right)^2 \mid X = x \right] = 
\underbrace{\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right]}_\textrm{reducible error} + 
\underbrace{\mathbb{V}_{Y \mid X} \left[ Y \mid X = x \right]}_\textrm{irreducible error}
$$

Here are some important things to notice: 

- The expected prediction error is for a random $Y$ given a fixed $x$ and a random $\hat{f}$. As such, the expectation is over $Y \mid X$ and $\mathcal{D}$. The estimated function $\hat{f}$ is random depending on the sampled data, $\mathcal{D}$, which is used to perform the estimation.
- The expected prediction error of predicting $Y$ using $\hat{f}(X)$ when $X = x$ has been decomposed into two errors:
    - The **reducible error**, which is the expected squared error loss of estimation $f(x)$ using $\hat{f}(x)$ at a fixed point $x$. The only thing that is random here is $\mathcal{D}$, the data used to obtain $\hat{f}$. (Both $f$ and $x$ are fixed.) This is often called reducible error the **mean squared error** of estimating $f(x)$ using $\hat{f}$ at a fixed point $x$. $$
\text{MSE}\left(f(x), \hat{f}(x)\right) \triangleq 
\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right]$$
    - The **irreducible error**. This is simply the variance of $Y$ given that $X = x$, essentially noise that is not important to learn. This is also called the **Bayes error**.

As the name suggests, the reducible error is the error that is to have some control over. But how can this error be controlled? 

## Bias-Variance Decomposition

Right after the expected predition error is decomposed into the reducible and inreducible error, the reducible error can even further be decomposed. 

Bearing the definiton of the variance of an estimator into the mind: 
$$
\text{bias}(\hat{\theta}) \triangleq \mathbb{E}\left[\hat{\theta}\right] - \theta
$$
the reducible error, which is the mean squared error can be further decomposen into bias squared and variance. 
$$
\mathbb{V}(\hat{\theta}) = \text{var}(\hat{\theta}) \triangleq \mathbb{E}\left [ ( \hat{\theta} -\mathbb{E}\left[\hat{\theta}\right] )^2 \right]
$$
Even if this is actually a common fact in estimation theory, it is mentiond at this place because the estimation of some regression function $f$ using $\hat{f}$ at some point $x$. 

$$
\text{MSE}\left(f(x), \hat{f}(x)\right) = \text{bias}^2 \left(\hat{f}(x) \right) + \text{var} \left(\hat{f}(x) \right)
$$
It can be stated that is a perfect world, it would be possible to finde some $\hat{f}$ which is unbiased, thta is bias $\text{bias}\left(\hat{f}(x) \right) = 0$ which has also a small variance. However, in the real world, this is not feasible. 

Hence, it appears that there is a **bias-variance tradeoff**. This bias-variance tradeoff is that the variance is decreasing, when the bias is increasing in the estimation. At once, increasing bias in the estimation leads to decrasing the variance. Intricate models tend to be unbiased, however, these models are  highly variable. On the other side, simple models are often very biased, but have a small variance. 

In terms of regression, it can be stated that models are biased when: 

- Parametric: The type of the model does not incorporate all the necessary varibales, of the type of the relationship is too simple. E.g. the linear relationship is assumed, but the real relationship is quadratic. 

- Non-parametric: the model present too much smoothing. 

In terms of regression, it can be stated that models are variable when: 

- Parametric: The type of the model incorporates many variables, or the type of the relationshio is too complex. E.g. the cubic relationship is assumed, but the real relationship is linear. 

- Non-parametric: the model does not present enough smoothing. The model is very shaking. 

In order to choose a model which is expected to balance the tradeoff betweeen the bias and the variance, and hence can minimize the reducible error, a model has to be choosen which provides the appropriate cimplexity for the data. 

Bearing into mind, that when fitting models, on the one hand, the train RMSE turns out the get larger as the model gets more complex. On the other hand, the test RMSE gets smaller until a certain point of model complexity, and then begins to increase. 

This is because the expected test RMSE is cruitally the expected prediction error, which is known as tp decompose into (squared) bias, variance and the irreducible Bayes error. This can be seen in the following thre plots, which are examples of the bias-variance tradeoff.

```{r, fig.height = 4, fig.width = 12, echo = FALSE}
x = seq(0.01, 0.99, length.out = 1000)

par(mfrow = c(1, 3))
par(mgp = c(1.5, 1.5, 0))
```

```{r}
b = 0.05 / x
v = 5 * x ^ 2 + 0.5
bayes = 4
epe = b + v + bayes

plot(x, b, type = "l", ylim = c(0, 10), col = "dodgerblue", lwd = 2, lty = 3,
     xlab = "Model Complexity", ylab = "Error", axes = FALSE,
     main = "More Dominant Variance")
axis(1, labels = FALSE)
axis(2, labels = FALSE)
grid()
box()
lines(x, v, col = "darkorange", lwd = 2, lty = 4)
lines(x, epe, col = "black", lwd = 2)
abline(h = bayes, lty = 2, lwd = 2, col = "darkgrey")
abline(v = x[which.min(epe)], col = "grey", lty = 3, lwd = 2)

```
Interpretation: The variance influenced the expected prediction error more than the bias. 

```{r}
b = 0.05 / x
v = 5 * x ^ 4 + 0.5
bayes = 4
epe = b + v + bayes

plot(x, b, type = "l", ylim = c(0, 10), col = "dodgerblue", lwd = 2, lty = 3,
     xlab = "Model Complexity", ylab = "Error", axes = FALSE,
     main = "Decomposition of Prediction Error")
axis(1, labels = FALSE)
axis(2, labels = FALSE)
grid()
box()
lines(x, v, col = "darkorange", lwd = 2, lty = 4)
lines(x, epe, col = "black", lwd = 2)
abline(h = bayes, lty = 2, lwd = 2, col = "darkgrey")
abline(v = x[which.min(epe)], col = "grey", lty = 3, lwd = 2)

```
Interpretation: The influence is neutral.

```{r}
b = 6 - 6 * x ^ (1 / 4)
v = 5 * x ^ 6 + 0.5
bayes = 4
epe = b + v + bayes

plot(x, b, type = "l", ylim = c(0, 10), col = "dodgerblue", lwd = 2, lty = 3,
     xlab = "Model Complexity", ylab = "Error", axes = FALSE,
     main = "More Dominant Bias")
axis(1, labels = FALSE)
axis(2, labels = FALSE)
grid()
box()
lines(x, v, col = "darkorange", lwd = 2, lty = 4)
lines(x, epe, col = "black", lwd = 2)
abline(h = bayes, lty = 2, lwd = 2, col = "darkgrey")
abline(v = x[which.min(epe)], col = "grey", lty = 3, lwd = 2)
legend("topright", c("Squared Bias", "Variance", "Bayes", "EPE"), lty = c(3, 4, 2, 1),
       col = c("dodgerblue", "darkorange", "darkgrey", "black"), lwd = 2)
```
Interpreatation: The variance influenced the bias more than the expected prediction error. 


In all three examples, the difference between the Bayer error, which is the horizontal dashed grey line, and the expected prediction, which is representet by the solid black curve, is exactly the mean squared error, which is the sum of the squared bias (blue curve) and the vairance (orange curve). The vertical line represents the complexity that minimized the prediction error. 

It is suposed that the irreducible error can be written as: 
$$
\mathbb{V}[Y \mid X = x] = \sigma ^ 2
$$
Hence, it full decomposition of the expected prediction error of predicting $Y$ using $\hat{f}$ when $X = x$ can be written as: 

$$
\text{EPE}\left(Y, \hat{f}(x)\right) =  
\underbrace{\text{bias}^2\left(\hat{f}(x)\right) + \text{var}\left(\hat{f}(x)\right)}_\textrm{reducible error} + \sigma^2.
$$
In summary it can be said that when the model complexity increeases, the bias decreases, while the variance increases. Therefore, understanding the tradeoff between bias and variance, the model complexity can be manipulated in order to find a model which predicts well on unseen observations. 

```{r, fig.height = 6, fig.width = 10, echo = FALSE}
x = seq(0, 100, by = 0.001)
f = function(x) {
  ((x - 50) / 50) ^ 2 + 2
}
g = function(x) {
  1 - ((x - 50) / 50)
}

par(mgp = c(1.5, 1.5, 0)) 
plot(x, g(x), ylim = c(0, 3), type = "l", lwd = 2,
     ylab = "Error", xlab = "",
     main = "Error versus Model Complexity", col = "darkorange", 
     axes = FALSE)
grid()
axis(1, labels = FALSE)
axis(2, labels = FALSE)
box()
ylabels = list(bquote("Low" %<-% "Complexity" %->% "High"), 
               bquote("High" %<-% "Bias" %->% "Low"),
               bquote("Low" %<-% "Variance" %->% "High"))
mtext(do.call(expression, ylabels), side = 1, line = 2:4)
curve(f, lty = 6, col = "dodgerblue", lwd = 3, add = TRUE)
legend("bottomleft", c("(Expected) Test", "Train"), lty = c(6, 1), lwd = 3,
       col = c("dodgerblue", "darkorange"))
```


## Simulation 

The decompositions, as well as the bias-variance tradeoff, can be illustrated through simulation. 
Assuming that a train model should learn the true regression function $f(x) = x^2$.

```{r}
f = function(x) {
  x ^ 2
}
```

In particular, an observation $Y$ should be predicted, given $X = x$ by using $\hat{f}(x)$ where

$$
\mathbb{E}[Y \mid X = x] = f(x) = x^2
$$
and

$$
\mathbb{V}[Y \mid X = x] = \sigma ^ 2.
$$

Alternatively, this can be written as

$$
Y = f(X) + \epsilon
$$

where $\mathbb{E}[\epsilon] = 0$ and $\mathbb{V}[\epsilon] = \sigma ^ 2$. In this formulation, $f(X)$ is called the **signal** and $\epsilon$ the **noise**.

In order to extradite a specific simulation example, the data genaerating process need to be fully specfied: 

```{r}
get_sim_data = function(f, sample_size = 100) {
  x = runif(n = sample_size, min = 0, max = 1)
  y = rnorm(n = sample_size, mean = f(x), sd = 0.3)
  data.frame(x, y)
}
```

Note: If it is prefered to think if this simulation using the $Y = f(X) + \epsilon$ formulation, the following code represents the same data generating process.

```{r, eval = FALSE}
get_sim_data = function(f, sample_size = 100) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = 0.75)
  y = f(x) + eps
  data.frame(x, y)
}
```


In order to completely specify the data generating process, more model assumptions has to be made than simply $\mathbb{E}[Y \mid X = x] = x^2$ and $\mathbb{V}[Y \mid X = x] = \sigma ^ 2$. In particular,

- The $x_i$ in $\mathcal{D}$ are sampled from a uniform distribution over $[0, 1]$.
- The $x_i$ and $\epsilon$ are independent.
- The $y_i$ in $\mathcal{D}$ are sampled from the conditional normal distribution.

$$
Y \mid X \sim N(f(x), \sigma^2)
$$

```{r, echo = FALSE}
# TODO: colors like this...
# \color{blue}{\texttt{predict(fit0, x)}}
# trick is getting it to render in both html and pdf
```

For obtaining this setup, the datasets $\mathcal{D}$ will be generated with a sample size  $n = 100$ and fit four models. 

$$
\begin{aligned}
\texttt{predict(fit0, x)} &= \hat{f}_0(x) = \hat{\beta}_0\\
\texttt{predict(fit1, x)} &= \hat{f}_1(x) = \hat{\beta}_0 + \hat{\beta}_1 x \\
\texttt{predict(fit2, x)} &= \hat{f}_2(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 \\
\texttt{predict(fit9, x)} &= \hat{f}_9(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \ldots + \hat{\beta}_9 x^9
\end{aligned}
$$
For making use of the data and the four models, a simulated dataset is generated, and fit the four models. 

```{r}
set.seed(1)
sim_data = get_sim_data(f)
```

```{r}
fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)
```



```{r, fig.height = 6, fig.width = 9, echo = FALSE}
set.seed(42)
plot(y ~ x, data = sim_data, col = "grey", pch = 20,
     main = "Four Polynomial Models fit to a Simulated Dataset")
grid()
grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, f(grid), col = "black", lwd = 3)
lines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = "dodgerblue",  lwd = 2, lty = 2)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = "firebrick",   lwd = 2, lty = 3)
lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = "springgreen", lwd = 2, lty = 4)
lines(grid, predict(fit_9, newdata = data.frame(x = grid)), col = "darkorange",  lwd = 2, lty = 5)

legend("topleft", 
       c("y ~ 1", "y ~ poly(x, 1)", "y ~ poly(x, 2)",  "y ~ poly(x, 9)", "truth"), 
       col = c("dodgerblue", "firebrick", "springgreen", "darkorange", "black"), lty = c(2, 3, 4, 5, 1), lwd = 2)
```
Interpretation: When plotting the four trained models, it can be seen that the zero predictor models does very bad. The first degree mdeol is reasonabale, but it can be seen that second degree model fits much better. The ninth model seem rather wild. 

When staying to the three plots which are created when using three further simulated datasets. The zero predictor and nith degree ploynomial were fit to each. 

```{r, fig.height = 4, fig.width = 12, echo = FALSE}
par(mfrow = c(1, 3))

# if you're reading this code
# it's BAD! don't use it. (or clean it up)
# also, note to self: clean up this code!!!

set.seed(430)
sim_data_1 = get_sim_data(f)
sim_data_2 = get_sim_data(f)
sim_data_3 = get_sim_data(f)
fit_0_1 = lm(y ~ 1, data = sim_data_1)
fit_0_2 = lm(y ~ 1, data = sim_data_2)
fit_0_3 = lm(y ~ 1, data = sim_data_3)
fit_9_1 = lm(y ~ poly(x, degree = 9), data = sim_data_1)
fit_9_2 = lm(y ~ poly(x, degree = 9), data = sim_data_2)
fit_9_3 = lm(y ~ poly(x, degree = 9), data = sim_data_3)

```

```{r}
plot(y ~ x, data = sim_data_1, col = "grey", pch = 20, main = "Simulated Dataset 1")
grid()
grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, predict(fit_0_1, newdata = data.frame(x = grid)), col = "dodgerblue", lwd = 2, lty = 2)
lines(grid, predict(fit_9_1, newdata = data.frame(x = grid)), col = "darkorange", lwd = 2, lty = 5)
legend("topleft", c("y ~ 1", "y ~ poly(x, 9)"), col = c("dodgerblue", "darkorange"), lty = c(2, 5), lwd = 2)

```
```{r}
plot(y ~ x, data = sim_data_2, col = "grey", pch = 20, main = "Simulated Dataset 2")
grid()
grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, predict(fit_0_2, newdata = data.frame(x = grid)), col = "dodgerblue", lwd = 2, lty = 2)
lines(grid, predict(fit_9_2, newdata = data.frame(x = grid)), col = "darkorange", lwd = 2, lty = 5)
legend("topleft", c("y ~ 1", "y ~ poly(x, 9)"), col = c("dodgerblue", "darkorange"), lty = c(2, 5), lwd = 2)
```
```{r}
plot(y ~ x, data = sim_data_3, col = "grey", pch = 20, main = "Simulated Dataset 3")
grid()
grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, predict(fit_0_3, newdata = data.frame(x = grid)), col = "dodgerblue", lwd = 2, lty = 2)
lines(grid, predict(fit_9_3, newdata = data.frame(x = grid)), col = "darkorange", lwd = 2, lty = 5)
legend("topleft", c("y ~ 1", "y ~ poly(x, 9)"), col = c("dodgerblue", "darkorange"), lty = c(2, 5), lwd = 2)
```

Interpretation: The plots make straighten out the difference between the bias and variance of these two models. The zero predictor model is clearly wrong, that is, biased, but nearly the same for each of the datasets, since it has very low variance.

While the ninth degree model does not appear to be correct for any of these three simulations, it can be seen that on average it is, and thus is performing unbiased estimation. These plots do however clearly illustrate that the ninth degree polynomial is extremely variable. Each dataset results in a very different fitted model. Correct on average is not the only goal that after, since in practice, only a single dataset is used. This is why also the models like to exhibit low variance.

In this case, it can be seen that when $k$ = 100, it is a biased model with very low variance. When $k$ = 5, it is again a highly variable model. 

These two sets of plots reinforce the intuition about the bias-variance tradeoff. Complex models (ninth degree polynomial and $k$ = 5) are highly variable, and often unbiased. Simple models (zero predictor linear model and $k = 100$) are very biased, but have extremely low variance.



<!--chapter:end:08_Chapter_07_Bias_Variance_Tradeoff.Rmd-->

---
title: "09_Ersatz"
author: "PGaulke"
date: "24 Juli 2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(amsmath)
```


# Classification 

Classification is also a form of supervised learning. Here, the response variable is categorical, as opposed to numeric for regression. The goal is to find a rule, algorithm, or a function which takes as input a feature vector, and outputs a category which is the true category as often as possible. (David Dalpiaz)

That is, the classifier $\hat{C}(x)$ returns the predicted category $\hat{y}(X)$.

$$\hat{y}(x) = \hat{C}(x)$$

- Qualitative variables take values in an unordered set C, such as email {spam, ham}. 
- Given a feature vector X and a qualitative response Y taking values in the set C, the classification task is to build a function C(X) that takes as input the feature vector X and predicts value; i.e. C(X)E C. 
- Often we are more interested in estimating the probabilities that X belongs to each category in C. 

For example, it is more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification fraudulent or not. 


In order to build the first classifier, the Default dataset from the ISLR package is used. 

```{r}
library(ISLR)
library(tibble)
as_tibble(Default)
```
The goal is to decently classify individuals as defaulters based on student status, credit card balance, and income. 
Note: The response default is the factor, as is the predictor student. 

```{r}
is.factor(Default$default)
is.factor(Default$student)
```
As done previous chaper regression, the data is splitted into test and train. In this example, 50 % each are used. 

```{r}
set.seed(42)
default_idx   = sample(nrow(Default), 5000)
default_trn = Default[default_idx, ]
default_tst = Default[-default_idx, ]
```


## Classification Visualization 

Simple classification rules can be used for simple visualizations. In order to create effective visualizations, the function featurePlot () from the package caret () is used. 

```{r, message = FALSE, warning = FALSE}
library(caret)
```
Based on a numerica predictor, a density plot can often suggest a simple split. Essentially this plot graphs a density estimate

$$\hat{f}_{X_i}(x_i \mid Y = k)$$

for each numeric predictor $x_i$ and each category $k$ of the response $y$.

```{r, fig.height = 5, fig.width = 10}
featurePlot(x = default_trn[, c("balance", "income")], 
            y = default_trn$default,
            plot = "density", 
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(2, 1), 
            auto.key = list(columns = 2))
```

Some notes about the arguments to this function according to David Dalpiaz:

- `x` is a data frame containing only **numeric predictors**. It would be nonsensical to estimate a density for a categorical predictor.
- `y` is the response variable. It needs to be a factor variable. If coded as `0` and `1`, you will need to coerce to factor for plotting.
- `plot` specifies the type of plot, here `density`.
- `scales` defines the scale of the axes for each plot. By default, the axis of each plot would be the same, which often is not useful, so the arguments here, a different axis for each plot, will almost always be used.
- `adjust` specifies the amount of smoothing used for the density estimate.
- `pch` specifies the **p**lot **ch**aracter used for the bottom of the plot.
- `layout` places the individual plots into rows and columns. For some odd reason, it is given as (col, row).
- `auto.key` defines the key at the top of the plot. The number of columns should be the number of categories.

It can be seems that the income variable by itself is not peculiarly effective. However, there seems to be a big difference in default status at a `balance` of about 1400. This information will be used shortly.

```{r, fig.height = 5, fig.width = 10, message = FALSE, warning = FALSE}
featurePlot(x = default_trn[, c("balance", "income")], 
            y = default_trn$student,
            plot = "density", 
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(2, 1), 
            auto.key = list(columns = 2))
```

A similar plot is created, except with `student` as the response. It can be seen that students often carry a slightly larger balance, and have far lower income. This will be useful to know when making more complicated classifiers.

```{r, fig.height = 6, fig.width = 6, message = FALSE, warning = FALSE}
featurePlot(x = default_trn[, c("student", "balance", "income")], 
            y = default_trn$default, 
            plot = "pairs",
            auto.key = list(columns = 2))
```

`plot = "pairs"` can be used to consider multiple variables at the same time. This plot reinforces using `balance` to create a classifier, and again shows that `income` seems not that useful.

```{r, fig.height = 6, fig.width = 6, message = FALSE, warning = FALSE}
library(ellipse)
featurePlot(x = default_trn[, c("balance", "income")], 
            y = default_trn$default, 
            plot = "ellipse",
            auto.key = list(columns = 2))
```

Similar to `pairs` is a plot of type `ellipse`, which requires the `ellipse` package. Here we only use numeric predictors, as essentially we are assuming multivariate normality. The ellipses mark points of equal density. This will be useful later when discussing LDA and QDA.

Example: Credit Card Default 

```{r}
show.index <- sample(1:nrow(Default), 1000)

plot(Default$balance[show.index], 
Default$income[show.index], col =
Default$default[show.index])

boxplot(Default$balance ~ Default$default)
```

## Can we use Linear Regression? 

Supposing for the Default classification task that it is coded 


$${Y} = 
\begin{cases} 
      0 & if \ no \\
      1 & if \ yes
\end{cases}$$


Can a simple linear regresssion of Y on X can be performed and classify as Yes if $\hat{Y} > 0.5$? 

- In this case of a binary outcome, linear regression does a good job as a classifier, and is equivalent to linear discriminat analysis which is discussed in a later. 
- Since in the population 
$$\mathbb{E}[Y \mid X = x] = P(Y = 1 \mid X = x).$$
it might be thinking that regression is perfect for this task. 
- However, linaer regression might produce probabilities less than zero or bigger than one. Logistic regression is more appropriate. 

## Linear versus Logistic Regression 

```{r}
default_trn_lm = default_trn
default_tst_lm = default_tst
```
```{r}
default_trn_lm$default = as.numeric(default_trn_lm$default) - 1
default_tst_lm$default = as.numeric(default_tst_lm$default) - 1
```

```{r}
model_lm = lm(default ~ balance, data = default_trn_lm)
```

```{r, fig.height=5, fig.width=7}
plot(default ~ balance, data = default_trn_lm, 
     col = "darkorange", pch = "|", ylim = c(-0.2, 1),
     main = "Using Linear Regression for Classification")
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
abline(model_lm, lwd = 3, col = "dodgerblue")
```
Linear regression does not estimate $P(Y = 1 \mid X = x)$.
The graph of linear regression shows that the predicted probabilities are below 0.5., indicating that every observation would be classified as `"No" This could be possible, but it is not what is expected. 

```{r}
all(predict(model_lm) < 0.5)
```
A further issue is that the predicted probabilty is less than 0. 
```{r}
any(predict(model_lm) < 0)
```
# Logistic regression 

$$p(x) = P(Y = 1 \mid {X = x})$$

```{r}
model_glm = glm(default ~ balance, data = default_trn, family = "binomial")
```
```{r}
coef(model_glm)
```

```{r}
head(predict(model_glm))
```
```{r}
head(predict(model_glm, type = "link"))
```
```{r}
head(predict(model_glm, type = "response"))
```

```{r}
calc_class_err = function(actual, predicted) {
  mean(actual != predicted)
}
```

```{r}
#calc_class_err(actual = default_trn$default, predicted = model_glm_pred)
```
Logistic regression is used to better estimate the propability.

The model is 

$$\log\left(\frac{p(x)}{1 - p(x)}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p.$$
```{r, fig.height=5, fig.width=7}
plot(default ~ balance, data = default_trn_lm, 
     col = "darkorange", pch = "|", ylim = c(-0.2, 1),
     main = "Using Logistic Regression for Classification")
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
curve(predict(model_glm, data.frame(balance = x), type = "response"), 
      add = TRUE, lwd = 3, col = "dodgerblue")
abline(v = -coef(model_glm)[1] / coef(model_glm)[2], lwd = 2)
```
In logistic regression it suited well to the task. 

This plot contains a wealth of information.

- The orange `|` characters are the data, $(x_i, y_i)$.
- The blue "curve" is the predicted probabilities given by the fitted logistic regression. That is,
$$\hat{p}(x) = \hat{P}(Y = 1 \mid { X = x})$$
- The solid vertical black line represents the **[decision boundary](https://en.wikipedia.org/wiki/Decision_boundary)**, the `balance` that obtains a predicted probability of 0.5. In this case `balance` = `r -coef(model_glm)[1] / coef(model_glm)[2]`.

<!--chapter:end:09_Chapter_08_Classifaction.Rmd-->

---
title: "10_Ersatz"
author: "PGaulke"
date: "24 Juli 2019"
output: html_document
---
# Cross-validation and the Bootstrap

Cross-validation and the bootstrap are two methods of resampling. These two methods refit a model of interest to samples created from the training set, for the reason to obtain additional information about the fitted model. The methods provide estimates of test-set prediction error, and the standard deviation and bias of the parameter estimates. 

## Training Error versus Test error

Here it is useful to recall the distinction between the test error and the training error. 
- Test error: average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method. 
- Training error: can be easily calculated by applying the statistical learning method to the observations used in its training. 
- Error rate: the training error rate can dramatically underestimate the test error rate. 


## Validation-Set Approach 

In the validation-set approach, the available set of samples is divided into two parts: A training set and a validation or hold-out set. 
The model is fit on the training set, and the fitted model is used to predict the reponse for the observations in the validation set. 
The resulting validation-set error provides an estimate of the test error. This is typically assessed using MSE in the case of a quantitative reponse and misclassification rate in the case of a qualitative (discrete) reponse. 


Example 1. (with explanations)

In the automobile data example, linear vs. higher-order polynomial terms in a linear regression are compared. The 392 observations are splited into two sets, a training set containing 196 of the data points, and a validation set containing the remaining 196 observations. 

```{r}
# a function for calculating the RMSE from two vectors

c.rmse <- function(observed, predicted){
  (observed - predicted)^2%>%
  mean %>%
  sqrt %>%
  round(3)
}

c.rmse2 <- function(observed, predicted) {
round(sqrt(mean((observed -predicted)^2)),3)
}

```

```{r}
require(ISLR)
require(magrittr)
#to load the required packages

set.seed(43245)
#in order to create random numbers, but to save this "seed" and not create new random numbers chunks are runned again (as done if put rnorm(41) instead of set.seed

#in order to have our training data seperated, we need to half it

n <- nrow(Auto) 
# just to have an abbreviation

train <- sample(1:n, ceiling(n/2))
#1: to number of rows, ceiling is used to prevent that in case nrow(auto) is odd, you have a number such as 74,3 (also could use round)

degrees<- 1:10
#the different degrees wanted to put in

v.rmse <- numeric ()
#to create a new vector where all values are putted in from the rmse

for (i in degrees){
#basically just creating an abbreviation for putting in several polynomals into the fit1
  
    
fit1 <- glm(mpg ~ poly(horsepower,i), data = Auto, subset = train)
  v.rmse[i] <-
# fit in into a linear model, in order to create a line that fits the model    
v.rmse[i] <- c.rmse(Auto$mpg[-train], predict(fit1, newdata=Auto[-train,]))  
    
# how it was before, against what it is now with v.rmse:c.rmse(Auto$mpg[-train], predict(fit1, newdata=Auto[-train,]))
#here function is created in order to calculate later the rmse

}
# the plot is created to see all the test error values for the different polys (the number after horsepower)
  

plot(degrees, v.rmse, type ="b", col = "red")   


#type b just shows the type of the line ( can also be l for line or p for points instead of b for both)


```
As a result degree 2 is probably taken, because it is quite good from its v.rmse and it is not complex (the lower the degree, the better is it to understand)

In the next step, is is done not just for one split, but multiple splits:

```{r}

require(ISLR)
require(magrittr)
#to load the required packages

set.seed(120)


degrees <- 1:10

n.splits <- 10

m.rmse <- matrix(NA, length(degrees), n.splits)
#here NA is the data(numbers), length = number of rows, n.splits = number columns

library(ISLR)

for(s in 1:n.splits){
  train <- sample(1:n, ceiling(n/2))
for(i in degrees) {
  fit1<- glm(mpg ~ poly (horsepower, i), data = Auto, subset = train)
m.rmse[i,s] <- c.rmse(Auto$mpg[-train], predict(fit1, newdata = Auto[-train,]))

}
}
  
plot(degrees, m.rmse[,1], type ="l", col = "red", ylim=c(min(m.rmse), max(m.rmse)))
for (s in 1:n.splits){
  lines(degrees, m.rmse[,s], col =s)
}

```


Example 2. 

- Consider fitting polynomial models of degree k = 1:10 data from this data generating process
- Consider k, the polynomial degree, as a turning parameter how well validation set approach works. 

```{r}
num_sims = 100
num_degrees = 10
val_rmse = matrix(0, ncol = num_degrees, nrow = num_sims)
```


```{r, include=FALSE}
gen_sim_data = function(sample_size) {
  x = runif(n = sample_size, min = -1, max = 1)
  y = rnorm(n = sample_size, mean = x ^ 3, sd = 0.25)
  data.frame(x, y)
}
```

```{r, include=FALSE}
calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
```
```{r, include=FALSE, eval=FALSE, error=TRUE}
#funktioniert nicht
fit = lm(y ~ poly(x, 10), data = sim_trn)

calc_rmse(actual = sim_trn$y, predicted = predict(fit, sim_trn))
calc_rmse(actual = sim_val$y, predicted = predict(fit, sim_val))
```

```{r, include=FALSE, eval=FALSE, error=TRUE}
#funktioniert nicht
fit = lm(y ~ poly(x, 10), data = sim_trn)

calc_rmse(actual = sim_trn$y, predicted = predict(fit, sim_trn))
calc_rmse(actual = sim_val$y, predicted = predict(fit, sim_val))
```

The simulations are: 

```{r} 
set.seed(42)
for (i in 1:num_sims) {
  # simulate data
  sim_data = gen_sim_data(sample_size = 200)
  # set aside validation set
  sim_idx = sample(1:nrow(sim_data), 160)
  sim_trn = sim_data[sim_idx, ]
  sim_val = sim_data[-sim_idx, ]
  # fit models and store RMSEs
  for (j in 1:num_degrees) {
    #fit model
    fit = glm(y ~ poly(x, degree = j), data = sim_trn)
    # calculate error
    val_rmse[i, j] = calc_rmse(actual = sim_val$y, predicted = predict(fit, sim_val))
  }
}
```
```{r echo = FALSE, fig.height = 5, fig.width = 10}
par(mfrow = c(1, 2))
matplot(t(val_rmse)[, 1:10], pch = 20, type = "b", ylim = c(0.17, 0.35), xlab = "Polynomial Degree", ylab = "RMSE", main = "RMSE vs Degree")
barcol = c("grey", "grey", "dodgerblue", "grey", "grey", "grey", "grey", "grey", "grey", "grey")
barplot(table(factor(apply(val_rmse, 1, which.min), levels = 1:10)),
        ylab = "Times Chosen", xlab = "Polynomial Degree", col = barcol, main = "Model Chosen vs Degree")
```

## Drawbacks of validation set approach 

The validation estimate of the test error can be highly variable, depending on precisely which observations are included in the training set and which observations can be included in the validation set. 
In the validation approach, only a subset of the observations - those that are included in the training set rather than in the validation set - are used to fit the model. 
This suggestes that the validation set error may tend to overestimate the test error for the model fit on the entire data set. 

## K-fold Cross validation 

This is a widely used approach for estimating the test error. The estimtates can be used to select the optimal model and to give an idea of the test error and the final chosen model. The idea is to randomly divide the data into K equal-sized parts. The k part is left out, fit the model to the other predictions for the left-out kth part. This appears through in turn for ach part k = 1, 2,...K, and then the results are combined. 


1          2     3     4     5 
Validation Train Train Train Train

## The Bootstrap 

The bootstrap is another resampling method. It is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method. E.g. it is usedful for providing an estimate of the standard error of a coefficient, or a confidence interval for that coefficient. The bootstrap could be used to replace the cross-validation method, however it aligns significantly more computation. 

<!--chapter:end:10_Chapter_09_Cross-Validation_and_the_Bootstrap.Rmd-->

---
title: "11_Ersatz"
author: "PGaulke"
date: "24 Juli 2019"
output: html_document
---
# Tree-based methods 

In this chapter, tree-based methods for regression and classification are discussed. These include stratifying or segmenting the predictor space into a number of single regions. Since the set of splitting ruls used to segment the predictor space can be summarized in a tree, these type pf approaches are known as decision tree methods. 

## Pro and Cons of Trees

One the one hand, tree-based methods are simple and useful for interpretation. On the other hand, they are typically not competitive with the best supervised learning approaches in terms of prediction accuracy. Further methods are bagging, random forest, and boosting, which grow multiple trees which are then combined to yield a single consensus prediction. Combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss interpretation. 

## The Basics of Decision Trees 

Decision trees can be used to regression and classification problems. In this chaper, the regression problems are considered first and second the classification problems 

## Example

Baseball salaray data: how to stratify it?


```{r}
require(ggplot2)

data("Hitters")

Hitters  %>%
  ggplot(aes(x=Years, y=Hits, col=Salary)) +
  geom_point()
```
The salary level is demonstrated in the shaded from low (dark blue) to high (light blue)

## Decision tree for these data 

```{r}
library(rpart)

b.tree <- rpart(Salary ~ Years + Hits, data = Hitters)

min.of.cp <- b.tree$cptable[which.min(b.tree$cptable[,"xerror"]),"CP"]

pruned.b.tree <- prune(b.tree, cp = min.of.cp)
plot(pruned.b.tree)
text(pruned.b.tree, pretty = 0)

```
Details of the previous figure (Decision tree)
For the hitters data, a regression tree for predicting the log salary of a baseball player, based on the number of years that he has played in the major leagues and the number of hits that he made in the previous year. At a given internal node, the label (of the form Xj < tk) indicating that the left-hand branch emanating from that split, and the right-hand branch corresponds to Xj >- tk=. For example, the left-hand branc corresponse to years < 4.5, and the right-hand branch corresponds to years >= 4.5 
The tree has two internal nodes and three terminal nodes, or leaved. The number in each leaf is the mean of the response for the observations that fall there. 

## Terminology for Trees 

- In keeping with the tree analogy, the region R1, R2, R3 are known as terminal nodes. 
- Decision treers are typically drawn upside down, in the sense that the leaves are at the bottom of the tree.
- The points along the tree where the predictor space is split are referred to as internal nodes. 
- In the hitters tree, the two internal nodes are indicated by the text Years < 4.5 and Hits < 117.5. 

## Interpretation of Results 

- Years is the less important factor in determining Salary, and players with less experience earn lower salaries than more experienced players. 
- Given that a player is less experienced, the number of Hits that he made in the previous year seems to play little role in his Salary. 
- But among players who have been in the major leagues for five or more years, the number of Hits made in the previous year does affect Salary, and players who made more Hits last year tend to have higher salaries. 
- Surely an over-simplification, but compared to a regression model, it is easy to display, interpret and explain. 

## Pruning a tree 

A small tree with fewer sploits (that is, fewer regions R1,...Rj) might lead to lower variance and better interpretations at the cost of a little bias. A possible alternative is to grow a tree only so long as the decreas in the RSS due to each split exceeds some (high) threshold. This will in smaller trees, but is too short-sighted: a seemingly worthless split early on in the tree might be followed by a very good split - that is, a split that leads to a large reduction in RSS later on. 
A better startegy is to grow a very large tree T0, and then prune is back in order to obtain a subtree. Cost complexity pruning - also known as weakest link pruneing - is used to do this. 


## Choosing the best subtree

A trade-off betwen the subtree's complexity and its fit to the training data is controlled by the tuning parameter alpha. The optimal alpha is selecting by using the cross-validation. After that, there is a return to the full data set and obtaining the subtree corresponding to alpha.  

## Summary: tree algorithm 

1. Using recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations. 
2. Applying cost complexity pruning to the large tree in order to obtain a sequence of besr subtrees, as a function of alpha. 
3. Using K-fold cross-validation to choose alpha. For each k = 1, ..., K: 
3.1 Repeating step 1 and 2 on the K-1/Kth fraction of the training data, excluding the kth fold.
3.2 Evaluating the mean squared prediction error on the data in the left-out kth fold, as a function of alpha.
Averaging the results, and picking alpha tp minimize the average error. 
4. Returning the subtree from Step 2 that correspond to the chosen value of alpha. 


## Classification Trees

The classification trees are similar to the regression trees. The difference is that the classification trees are used to predict that every observation belongs to the most commonly occuring class of training obervations in the region to which it belongs. 

## Details of classification Trees

As already used in the regression setting, recursive binary splitting are used to grow a classification tree. In the classification setting, RSS cannnot be used as a criterion for making the binary splits. A natural alternative to the RSS is the classification error rate. This is simply the fraction of the training observation in that region that do not belong to the most common class. 

E = 1 - max(^pmk)/k 

Note: ^pmk represents the proportion of training observations in the mth region that are from the kth class. However, classification errror is not sufficiently sensitive for tree-growing, and in practive two other measures are preferable (Gini Index and Deviance)

## Advantages and Disadvantages of Trees

There are four advanatages and one disadvantage of trees. 

The first advantage is that trees are perfect to explain people. 
The second advantage is that decision trees can be seen as more closely mirror human decision-making than do the regression and classification approaches. 
The third advantage is that trees can be displayed graphically and can be easily interpretated, even by a non-expert. 
The forth advanatge is that tree can easily handle qualitative predictors without the need to create dummy variabls. 
One disadvantage is that trees have not the same level of predictive accurarcy in general, as some of the other regression and classification approaches. 



## Bagging 

Bagging is one way to fix the over-fitting of trees. It is a general-purpose procedure for the reduction of variance of statistical learning method. Bagging is a useful and frequently method used in the context to decision trees. Bagging is a special form of random forest where `mtry` which is equal to p, the number of predictors. 

Example 

The goal is now to fit a bagged model, by using the package `randomForest`. 


```{r, message = FALSE, warning = FALSE, eval=FALSE, error=TRUE}
#funktioniert nicht
library(randomForest)

boston_bag = randomForest(medv ~ ., data = boston_trn, mtry = 13, 
                          importance = TRUE, ntrees = 500)
boston_bag
```

```{r, eval=FALSE, error=TRUE}
#funktioniert nicht
boston_bag_tst_pred = predict(boston_bag, newdata = boston_tst)
plot(boston_bag_tst_pred,boston_tst$medv,
     xlab = "Predicted", ylab = "Actual",
     main = "Predicted vs Actual: Bagged Model, Test Data",
     col = "dodgerblue", pch = 20)
grid()
abline(0, 1, col = "darkorange", lwd = 2)
```

```{r, eval=FALSE, error=TRUE}
#funktioniert nicht
(bag_tst_rmse = calc_rmse(boston_bag_tst_pred, boston_tst$medv))
```
Interpratation: Two interesting results can be seen. 

- The first interesting result is that the predicted vs actual plot has no longer a small number of predicted valued. 
- The second interesting result is that the test error has dropped immemsely. 
Note: the Mean of squared residuals, which is the outbut by the `randomForest`is the Oit of Bag estimate of the error. 

```{r, eval=FALSE, error=TRUE}
#funktioniert nicht
plot(boston_bag, col = "dodgerblue", lwd = 2, main = "Bagged Trees: Error vs Number of Trees")
grid()
```

## Random Forest 

Random forests provide an improvement over bagged trees by way of small tweak that decorrelates the trees. Hence, this reduces the variance when averaging the trees. Further, as already seen in bagging, here a number of decision trees are build on bootstrapping training samples. However, when decision trees are build, every time a split in a tree is considered, a random selection of m predictors is chosen as split candidates from the full set of p predictors. The split is allowed to use only one of those m predictors. 

Note: Now a random forest is tried. For regression, the suggestion is to use `mtry` equal to $p/3$. 
```{r, eval=FALSE, error=TRUE}
#funktioniert nicht
boston_forest = randomForest(medv ~ ., data = boston_trn, mtry = 4, 
                             importance = TRUE, ntrees = 500)
boston_forest
```

```{r, eval=FALSE, error=TRUE}
#funktioniert nicht
importance(boston_forest, type = 1)
varImpPlot(boston_forest, type = 1)
```

```{r, eval=FALSE, error=TRUE}
#funktioniert nicht
boston_forest_tst_pred = predict(boston_forest, newdata = boston_tst)
plot(boston_forest_tst_pred, boston_tst$medv,
     xlab = "Predicted", ylab = "Actual",
     main = "Predicted vs Actual: Random Forest, Test Data",
     col = "dodgerblue", pch = 20)
grid()
abline(0, 1, col = "darkorange", lwd = 2)
```

```{r, eval=FALSE, error=TRUE}
#funktioniert nicht
(forest_tst_rmse = calc_rmse(boston_forest_tst_pred, boston_tst$medv))
boston_forest_trn_pred = predict(boston_forest, newdata = boston_trn)
forest_trn_rmse = calc_rmse(boston_forest_trn_pred, boston_trn$medv)
forest_oob_rmse = calc_rmse(boston_forest$predicted, boston_trn$medv)
```

Interpretation: Here are three RMSEs noted. The training RMSE, which is optimistic and the OOB RMSE which is a reasonable estimate of the test erro and the test RMSE. Further, the variables importance was calculated. 


```{r, echo = FALSE, eval=FALSE, error=TRUE}
#funktioniert nicht
(forst_errors = data.frame(
  Data = c("Training", "OOB", "Test"),
  Error = c(forest_trn_rmse, forest_oob_rmse, forest_tst_rmse)
  )
)
```


## Boosting 

Similar to bagging, boosting is a general approach which can be applied to many methods in statistical learning for regression or classification. When recalling that bagging involves creating multiple copies of the orginal training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to each copy, and then combining all of the trees in order to create a single predictive model. Every tree is built on a bootstrap data set, independent of the other trees. 
Here, booting runs in a similar way, except that the trees are grown sequentially, meaning that each tree is grown using information from previously grown trees. 

Example 

In this example, it is tried to boost a model, which by default will produce a nice variable importance plot as well as plots of marginal effects of the predictors. The package `gbm` is used. 

```{r}
library(gbm)
```

```{r, fig.height = 6, fig.width = 8, message = FALSE, warning = FALSE, eval=FALSE, error=TRUE}
#funktioniert nicht
booston_boost = gbm(medv ~ ., data = boston_trn, distribution = "gaussian", 
                    n.trees = 5000, interaction.depth = 4, shrinkage = 0.01)
booston_boost
```
```{r, fig.height = 8, fig.width = 8, message = FALSE, warning = FALSE, eval=FALSE, error=TRUE}
#funktioniert nicht
tibble::as_tibble(summary(booston_boost))
```

```{r, fig.height = 5, fig.width = 12, message = FALSE, warning = FALSE, eval=FALSE, error=TRUE}
#funktioniert nicht
par(mfrow = c(1, 3))
plot(booston_boost, i = "rm", col = "dodgerblue", lwd = 2)
plot(booston_boost, i = "lstat", col = "dodgerblue", lwd = 2)
plot(booston_boost, i = "dis", col = "dodgerblue", lwd = 2)
```

```{r, eval=FALSE, error=TRUE}
#funktioniert nicht
boston_boost_tst_pred = predict(booston_boost, newdata = boston_tst, n.trees = 5000)
(boost_tst_rmse = calc_rmse(boston_boost_tst_pred, boston_tst$medv))
```

```{r, eval=FALSE, error=TRUE}
#funktioniert nicht
plot(boston_boost_tst_pred, boston_tst$medv,
     xlab = "Predicted", ylab = "Actual", 
     main = "Predicted vs Actual: Boosted Model, Test Data",
     col = "dodgerblue", pch = 20)
grid()
abline(0, 1, col = "darkorange", lwd = 2)
```
 
 
## Summary 

Decision trees can be used for regression and classification when they are simple and interpretable. However, decision tres are often not competitive with other methods in terms of prediction accuracy. Further, bagging, random forest and boosting are good methods for imporving the prediction accuracy of trees. They work by growing many trees on the training data and then combining the predictions of the resulting ensemble of trees. 
Random forests and boosting are among the state-of-the-art methods for supervised learning. Howeverm their results can be difficult to predict. 

<!--chapter:end:11_Chapter_10_Trees.Rmd-->

---
title: "12_Solution_for_task"
author: "PGaulke"
date: "24 Juli 2019"
output: html_document
---

# (PART) Exercise {-}

# Give it a try

```{r}
# To prepare for the fight
require(tidyverse)
require(ISLR)
require(magrittr)


load("C:/Users/admin/Dropbox/Master/2. Semester/Data Science/MyBook/project_data.Rdata")

summary(train.data)
summary(test.data)
train.data



# 11 variables for frequency of seven plants
# task: The test.data has the same structure but does not contain the frequencies for each of the 7 plants. Your
# goal is precisely to estimate them for the 140 observations

# remember to remove/replace na's.

```
```{r, eval=FALSE}
plot(mxPH ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "mxPH vs a1")
plot(mnO2 ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "mnO2 vs a1")
plot(Cl ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "Cl vs a1")
plot(NO3 ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "NO3 vs a1")
plot(NH4 ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "NH4 vs a1")
plot(oPO4 ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "oPO4 vs a1")
plot(PO4 ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "PO4 vs a1")
plot(Chla ~ a1, data = train.data, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "Chla vs a1")

#tendenziell mehr mxPH und mnO2

pairs(train.data)

```

## Linear Regression

```{r}
model.slr <- lm(a1 ~ NH4, data = train.data)
model.slr

model.slr$fitted.values
```


```{r}
library(caret)
featurePlot(x = train.data[ , c("mxPH", "mnO2", "Cl","NO3","NH4","oPO4","PO4","Chla")], y = train.data$a1)
```




```{r}
# starting with a simple linear model, with no predictors
fit_0 = lm(a1 ~ 1, data = train.data)
get_complexity(fit_0)

# train RMSE
sqrt(mean((train.data$a1 - predict(fit_0, train.data)) ^ 2))
# test RMSE (not available)
sqrt(mean((test.data$a1 - predict(fit_0, test.data)) ^ 2)) 
```

Create a real test set

```{r}
set.seed(30)
num_obs = nrow(train.data)

train.index = sample(num_obs, size = trunc(0.50 * num_obs))
newtrain.data = train.data[train_index, ]
traintest.data = train.data[-train_index, ]

```

Now again same step

```{r}
# starting with a simple linear model, with no predictors
fit_0 = lm(a1 ~ 1, data = newtrain.data)
get_complexity(fit_0)

# train RMSE
sqrt(mean((newtrain.data$a1 - predict(fit_0, newtrain.data)) ^ 2))
# test RMSE (
sqrt(mean((traintest.data$a1 - predict(fit_0, traintest.data)) ^ 2)) 
```

```{r}
library(Metrics)
# train RMSE
rmse(actual = newtrain.data$a1, predicted = predict(fit_0, newtrain.data))
# test RMSE
rmse(actual = traintest.data$a1, predicted = predict(fit_0, traintest.data))
```

RMSE formula

```{r}
get_rmse = function(model, data, response) {
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}
```

```{r}
get_rmse(model = fit_0, data = newtrain.data, response = "a1") # train RMSE
get_rmse(model = fit_0, data = traintest.data, response = "a1") # test RMSE
```

Increase the fit.

We have to remove NA`s first

```{r}
fit_1 = lm(a1 ~ ., data = newtrain.data)
get_complexity(fit_1)

get_rmse(model = fit_1, data = newtrain.data, response = "a1") # train RMSE
get_rmse(model = fit_1, data = traintest.data, response = "a1") # test RMSE
```

```{r}
newtrain.data
traintest.data
```


```{r, eval=FALSE, error=FALSE}

#newtrain.data$fitteds <- model.slr$fitted.values
#newtrain.data

#select{absolutelynewtrain.data, -1)
#plot(newtrain.data$NH4, newtrain.data$a1)
# now add a line

lines(newtrain.data$NH4, newtrain.data$fitteds, col="blue")


```


<!--chapter:end:12_Solution_Task.Rmd-->

