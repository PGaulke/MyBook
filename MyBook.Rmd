--- 
title: "MyBook"
author: "Corinna Trierweiler and Philipp Gaulke"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: report
description: "This is a guide for data science with R"
---

# Preface {-}

"Data is the sword of the 21st century, those who wield it well, the Samurai." Jonathan Rosenberg, adviser to Larry Page and former SVP of products at Google. 
This book has been produced for and based on the Data Science for Business class of Hochschule Fresenius in Cologne, Germany. In context of the task, the book includes basic R skills, statistical methods for data science and a own project is fulfilled. 
The majority of information used in this book are learned in the lessons in class. Especially, lecture notes have been the basis for the book. Besides this, the online sources  tidyverse.com and the book of David Dalpiaz have been used as reference. 

This book can serve as a source for all people who are interested in studying R and statistic models. 


```{r, include=FALSE}

require(magrittr)
require(tidyverse)
require(ISLR)
require(gbm)
require(MASS)
require(caret)
require(FNN)
require(rpart)
require(boot)
require(forcats)
require(lattice)
require(stringr)
require(tibble)
require(ellipse)
require(metrics)
require(GGally)
require(tidyr)
require(ggplot2)
require(dplyr)
require(readr)

```

<!--chapter:end:index.Rmd-->

---
title: "01_Introduction"
author: "CTrierweiler,PGaulke"
date: "14 Juli 2019"
output: html_document
---

# Introduction {-}

This book is created in order to provide programming beginners a clear and understandable overview of how to use R for statistical investigations. This includes the explanation for the set up and an introduction to the basic skills for R, as well as an overview of major statistical methods for data science.

- What is R? -

R is a programming language and free software environment for statistical computing created by the R Foundation for Statistical Computing. It is a common tool for statistics that can be used to analyze and interprete data sets. Apart from the ground infrastructure and functions, R can be individualized easy and quickly by downloading additional tools and packages which are freely avalaibale. These packages may include further functions for calculation, data sets or even own programming features.

- How do we approach it? -

Learning programming is broadly declared as a herculean task. Firstly, this is simply not true, especially when you consider that most of us learn a second real language in the age of 10, which is by far more difficult. Secondly, learning success is like everywhere else depending on how you approach it. Opening in the first step some R file and apply random statistical methods on a 7 terrabyte file will most probably not lead to a success, especially when you are not too familiar with statistics. However, in the following we will make one step after another, so that any of you will be able to follow and understand what to do. This will include at first the software set-up, which is quite easy but highly important for further steps. After that, we will introduce you the structure of R and the basic skills. Having you then on a "I now somehow know how to import and calculate things"-level we will go over to the statistical part.
Before you now go on and start with your R career, answer yourself some questions:

- Are you able to read, write and calculate?

- Have you ever worked on a computer in your life (surfing, writing a text or dowload something?)

- Are you actually interested in how to understand and analyze data?

If there is any "No" here, then think about it once again. As already said, this is not a herculean task but of course it will need some effort and time to get into R. If it is "yes, yes, and yes" then great! Let's get started, you will probably be able to write "Basic R skills" into your CV, before you even think about it.

<!--chapter:end:01_Introduction.Rmd-->

---
title: "Chapter_01_R_10"
author: "CTrierweiler,PGaulke"
date: "13 Juli 2019"
output: html_document
---
# (PART) Explaining R {-}

# Setup

You will need the following software:

The R software itself, RStudio (so to say, the environment where you will work in) and a Latex distribution for creating output files such as .pdf files with R graphics.

R Software - https://cran.uni-muenster.de/

FreeVersion of RStudio -  https://www.rstudio.com/products/rstudio/download/#download

Latex distribution - for example: https://www.latex-project.org/get/ (depending on your computer)


## Start Your Project

In order to share your work, GitHub is a tool of major importance. In the following, we will explain to you how to set this up. It is totally up to you whether to install it now or later. Please just consider, that you should install it before you start your collaborative project. Otherwise you will face a quite complicated process and limited possibilities to fully enjoy all collaborational features that GitHub provides.

The software you need for this is Git Distribution.

Git distribution - https://git-scm.com/downloads

In parallel of installing the Git distribution, go to https://github.com and create an account.

In a first step, you should activate Git in RStudio. Therefore choose: 'Tools' > 'Global Options' > 'Git/SVN' and click on the button to enable the version control interface. Additionally, you should generate a SHH RSA key which will be needed in a later stage when setting up your repository on GitHub.

Now you can create your project. Click on 'File' > 'New Project' > 'Existing directory' and choose where you want to place your project on your computer. Be aware, that you really know where you place it as you will need the directory in a later stage.

Now it is time to prepare for the marriage of your GitHub account and your project. Go to GitHub and create a new repository and name it exactly the same way as you named your R project. The naming has to be identical.
Next, got to your settings in GitHub and choose 'SHH and GPG Keys' and click on 'New SHH key'. Go back to RStudio and copy the SHH key that has been created in the first step, then paste it into your GitHub account.

Now everything is set up to create the connection. In order to do so, go in RStudio to 'Tools' > 'Project Options' > 'Git/SVN'. Select 'Git' in the Version control system field. After that, go again to 'Tools' > 'Terminal' > 'New Terminal'. Now next to the console a terminal should appear.

As you may already read when you finished your the creation of your repository, here you should type in the following commands:


```{r, eval=FALSE}
git init


git remote add origin https://github.com/YOURNAME/YOURREPOSITORY.git
git push -u origin master
```

Obvisously you should adopt the origin link with your own names. 

Now restart RStudio and enjoy that you just completed to connect your project with GitHub. In the upper right corner of RStudio you should now see a Git button (right next to environment/history/connections). To put your files on GitHub, you can now easily commit and then push all files you want to share.

## Work Collaboratively

If you are interested to work simoultanesly with another person on one project, you can create a team in GitHub. However, this is not done by a few clicks.

At first, you have to decide who the owner of the project should be. The role will not have too much influence later on, but it defines the set up for each participant.

The project owner needs to create an organization on Github. Within this organization a new repository should be created, which again should have exactly the same name as the project in R Studio. Herefore, you can follow the steps described above.
If you have done that, you should create a team in the organization and add the further participants to the team. Be aware that you should assign the repository to these members and provide the members with respective rights. 

As the team member who should have received an email at this point. After you confirmed the participation, go onto the repository and click on the button 'Clone or download'. Look for the https adress, copy it and go now into a simple RStudio session (not a project). Open a new Terminal and type in cd with the path where you want to save the project.
Now type 'git clone' and paste the the copied link from GitHub.

```{r, eval=FALSE}
cd Dropbox/Master/2Sem
git clone https://github.com/ORGANIZATION/repo.git 
```

Now you should finally be able to push and pull the all files and start your collaborative R project.

<!--chapter:end:02_Chapter_01_R_Setup.Rmd-->

---
title: "03_Chapter_02_RMarkdown"
author: "CTrierweiler,PGaulke"
date: "14 Juli 2019"
output: html_document
---

# Creating Files in R

For working in R we use R Markdown documents. Click on 'File' > 'New File' and then create a new R Markdown file.
R Markdown files include simple formatting syntax for authoring HTML, PDF and Microsoft Word documents. If you look for any specific information about R Markdown which is not included in this book, check this link http://rmarkdown.rstudio.com.

Basically, there are two ways to add something to an R Markdown file.

  - Written text, in which you can include some inline-code by starting with a backstick plus r and ending with a backstick e.g. `r 2*3`. However this kind of code integration will be used less often in this book.
  - Chunks, seperate grey fields that are used to integrate code. You can create a chunk by entering three backsticks plus {r} and end it with again three backticks
  
Below a short example for a chunk

```{r}

2*3

```

In order to process the operation that you entered, you have to click on the green arrow in the right corner. This is what we call to run a chunk.
While working through this book, you will create many chunks with different operations. This includes not only mathematical operations, but also the creation of graphs. There are various ways how to write code in a chunk, we will provide you in the following chapters with more insights, so that you will be able chose the most efficient and fastest ways for each purpose.

In the following, both ways of adding something to a R Markdown file will be illustrated in detail.


## Designing written text

General advises of how to edit written text can you find on the following website: https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf

This cheatsheet contains all inline-formatting options that you can use in base R.
It also shows you how to understand the use of block level elements. By using a hashtag, you can create headlines. The logic is that:

One # - main-headline
Two # - sub-headline
Three # - sub-headline of degree 2

It is important to know that you can only use on main-headline per R markdown file. In order to include more main-headlines you should another R markdown file in your project. For example, in case that you create a book it is recommendable to create one R markdown file per chapter. That is not only more attractive because of the usability of main-headline, but it also provides a better overview of your work.

Of course, formatting is not limited to that. The actual output can be edited in various ways by using YAML.
You find your YAML-header in each of your R markdown files. However, if you want to adopt your files generally rather than specific for each R markdown file you can use the output.yml file. As soon as you create a project, you will find a file in your project that is called output.yml. Settings that you include here will work as a standard for your project, so that you do not need to adopt each R markdown file.

Considering that you probably want to produce a book or a report, respectively create output in form of a PDF, HTML, or other kind of file, it will be necessary to include some information in the header.

For example, if you want to build a PDF file, you have to include:

```
bookdown::pdf_book:
  includes:
    in_header: preamble.tex
```

However, for further information on how to produce output, please check \@ref(output).


## Coding in chunks

Actually, this is the main topic of this book. The explanation of coding in R chunks will not be limited to this chapter, but at this point it is helpful to get an overview of how chunks work and what you can do with them.

The sample chunk in the beginning of this chapter, already revealed the general functionality. Nevertheless, there some general notes to make.

### Chunk options

At first, different options can be set for a chunk, for example wheter to evaluate the code chunk, to stop processing when an error occurs, or to dispay the source code in your output file and a lot more. This can be done by editing the content of the brackets.

If you add eval=TRUE/FALSE, then the whole chunk will be evaluated respectively not evaluated.


```{r, eval=TRUE}
42+17
82-2

```
```{r, eval=FALSE}
42+17
82-2

```

If you add error=TRUE/FALSE, then the run-process will be stopped, respecitively not stopped, if an error occures.

```{r, error=TRUE, eval=FALSE}
100*apples
9*3
```
```{r, error=FALSE, eval=FALSE}
100*apples
9*3
```

If you add echo=TRUE/FALSE, then the source code will be displayed respecitively not displayed in the output file.

```{r, echo=TRUE}
10+27
8+6

```

```{r, echo=FALSE}
10+27
8+6

```

For a detailed explanation for most of all options, please see here https://yihui.name/knitr/options/.

Looking into what you can add into the chunk, you have to be aware that any small mistake will mess up the whole chunk. Especially, for larger and more complex chunks this is a challenge. Therefore, it might be a good idea to make comments for later understanding. Comments can be made by using a hashtag when starting a new line in a chunk. Here an example:

```{r}
10*750000/17
#Just a random calculation

```

### Chunk Functions

As a basic, we can use the chunk as a *calculator*.

We can build up easier calculations as well as more difficult calculations, as far as our keyboard allows us.

```{r}

8*4+12

```

```{r}

37*4235+(19*245)/422+3-10

```

```{r}

sin(40*9)+log(120)

```

Moreover, R provides built-in functions that you can easily use to exercise special operations. In the following example, a sequence will be created by using the function - seq -

```{r}

seq(1,5)

seq(1,10,length.out = 3)


```

More built-in functions can be found under the following link: https://www.statmethods.net/management/functions.html


Another function that is included in chunks is to *name operations*. Naming advantages can be beneficial if you want to use the operational multiple times in your chunk.


```{r}

a <-905/12*5

sin(a)+sin(a^2)+sin(a^3)

```

<!--chapter:end:03_Chapter_02_RMarkdown.Rmd-->

---
title: "04_Chapter_03_Output"
author: "CTrierweiler,PGaulke"
date: "14 Juli 2019"
output: html_document
---

# Creation of Output {#output}

By selecting Knit you can create a file out of your .rmd's. R Studio supports various formats which you can set in the header of any .rmd file.

```{r, eval=FALSE}
output: html_document
```

Otherwise you can also select a format in the dropdown menu oft the Knit-button.

Of course, there are various ways of formatting your output. Herefore, you have to use the fields below your output format in the header.
To know which options you can choose for every output format, just check the respective help page. To open the help page, you have to type in ?rmarkdown::(here output format) into the console. 


Another option you have, is to build a book. 
In case you want to compile all your rmd.file to one book, you can call the render function in bookdown. In order to this, you have to download the bookdown package. This is easily done, by clicking on *Tools*, then *Install packages* and search for bookdown. There is more than only one way to download packages, also chunks provide this option by searching for the packages like this:

```{r, eval=FALSE}

install.packages("bookdown")

```

To now prepare for building a book, please go into the .yml file of your project and set the options up accordingly. For example to create a .pdf book, type in this:

```{r, eval=FALSE}

bookdown::pdf_book:

```

This should then be further set up, for options you can again use the console:

```{r, eval=FALSE}
?bookdown::pdf:book
```

<!--chapter:end:04_Chapter_03_Output.Rmd-->

---
title: "05_Chapter_04_Basic_R_Skills"
author: "CTrierweiler,PGaulke"
date: "16 Juli 2019"
output: html_document
---
# Basic R Skills

Now we dive a little bit deeper into R and go trough the basics of how to handle data.
For this, it is necessary to get an understanding of the most important data structures that do exist, what kind of data they may include and in what kind of format they are. Furthermore, we will introduce you some major rules which should be considered while handling data in R as well as how to import data and what packages might be useful in order to handle data effective and efficiently. Before closing this chapter, also a short overview on how to visualize data is given.

After gathering all the informations and knowledge, it will be possible for you to work with statistics in R, which will be the topic of the second part of this book.

## Data stuctures

In general there are four types of data structures: atomic vectors, lists, matrix and arrays, and data frames.


The most common type of data structure are *atomic vectors*. Vectors can be described by three attributes:

1. the type `typeof()`

Vectors can be either numeric, logical, or character.

```{r}
numeric_vector <- c(1,2,3,4,5) # numeric vector
logical_vector <- c(TRUE,TRUE,TRUE,FALSE) # logical vector
character_vector <- c("first", "apple", "child", "word") # character vector
```

Further, you can also create integer and mixed vectors

```{r}
integer_vector <- c(10L, 4L, 7L) # integer vector
mixed_vector <- c(2, "mixed") # mixed vector
```

When you create a mixed vector and you do not determine which type of vector you create, than R decides on itself. The logic is: logical < numeric < character.


Numeric vectors can be also created by only using 'x:y' if you want to include all numbers from x to y.
```{r}
another_numeric <- 1:5
```


2. the length `length()`

The length basically describes the size of the vector.
If you want to check the length of a vector, you have to use `length()`.

```{r}

whatisthelength <- c(1,4,2,1,16,124,54,6,7)

length(whatisthelength)

```


3. the attributes `attributes()`

Attributes define the nature of a specific vector and are relevant for what kind of function can be applied.
The three major attributes are:

  - the names, `names()`,
  - the dimensions, `dim()`,
  - the class, `class`.

However, you can also check attributes in the summary of the vector. Therefore, you have to use `summary()`, which is (as most all other attributes) also applicable on other data structures.


```{r}
atry <- c(1:15)
summary(atry)

anothertry <- c(T,F,T,F)  
summary(anothertry)
```


Another type of data structure are *lists*. A list can include a number of objects, but also another list. Therefore, it is useful in order to gather data into one structure.

```{r}
alist <- list(numbers=c(1:10),
      fruits=c("Banana", "Peach"),
      values=c(T,T,F,T,F))
alist
```


If you want to illustrate a *matrix* than you can also use R for this. Therefore, you have to consider that all columns have the same mode and the same length.
In general the formel to use is:

```{r}

amatrix <- matrix(c(1:12), nrow=4, ncol=3) 
amatrix

```


Another essential data structure are *data frames*. Somehow the data frame is similar to the matrix, but you can use different modes. Apart from some build-in data sets that are provided in a data frame layout, you can create a data frame by yourself by using the function data.frame.

```{r}

adf <- data.frame(numbers=1:4,
                  fruits= c("banana", "peaches", "orange", "strawberry"),
                  value= c(T,F,F,T))
adf

```
By the way, the length of data frame is determined by the number of columns you include. For our example, you can check the length like this:

```{r}

adf <- data.frame(numbers=1:4,
                  fruits= c("banana", "peaches", "orange", "strawberry"),
                  value= c(T,F,F,T))
length(adf)

```

## Principles in R Chunks

There are some major calculation principles that you have to consider while working with R. For instance, these principles can be quite helpful but being not aware of there existence might lead to errors that are difficult to detect.

At first, we have to consider that *element by element evaluation* is active. In case that you want to somehow create a calculation with two or more vectors, this principle is of major importance.

In case of two numeric vectors of the same length, the calculation will be applied on each element in the same position.

```{r}

store1revenue <- c(10000,12000,18000,9000,11000)
store2revenue <- c(25000,29000,21000,23000,24000)

length(store1revenue) == length(store2revenue)

revenuesum <- store1revenue+store2revenue

revenuesum

```

Every element of the first vector is added to the element that is in the same position in the second vector.

Now, if you violate the premise that the vectors used have the same length, the second principle will be activated. *Recycling* happens and the objects included int he shortest vector will be repetivtively used for the calculation.


```{r}
performanceofa <- c(34,39,51,45,28,37)
performanceofb <- c(30,29,45,42)
performancesum <- performanceofa+performanceofb
performancesum


```

If recycing happens, you will receive an error message which is apparently not an error message that is stopping any process, but making you aware of the recycling.


Another important thing, which is less a principle but more a shortcut, is the *deletion of NA's*.
Sometimes you want to take a vector for a calculation that will take all values of that vector into account. This might lead to difficulties, as missing values are often replaced with `NA` in data sets.
However, with using 'na.rm = T, R will ignore the NA during the calculation.


```{r}
horsepower <- c(400,320,190,200,310,290,420,NA,230,220)
mean(horsepower, na.rm = T)

```


## Subsetting {#subsetting}

Subsetting means to create a data set out of the existing data structure. So to say, you copy particular items out of a data collection.

There are three main operators which can be used to subset:

- `[]`
- `[[]]`
- `$`


The first one, `[]`can be applied on all discussed data strucures - vectors, lists, matrices, and data frames.

In case of a vector, you can easily use it these ways:

```{r} 
vec <- c(-7,4,12,6,-2,1,3,-3)

# subsetting only one element by naming the position of the element
vec[3]

# subsetting several elements in a row
vec[c(2:6)]

# subsetting all elements but not the named ones
vec[c(-2,-4)]

# subsetting elements by logical selection
vec[c(T,F,T,F)] # recycling eventually activated

```

If you have a list, than you have to be even more careful about where the data is placed.

```{r}
alist <- list(numbers=c(1:10),
      fruits=c("Banana", "Peach"),
      values=c(T,T,F,T,F))

# subsetting a specific data set in the list
alist[2]

```

Applying `[]` on a matrix requires again a different logic. To understand all dimension, you can use `str()`, which shows you the exact length of the matrix columns and rows.

```{r}

amatrix <- matrix(c(1:12), nrow=4, ncol=3) 

# subsetting one particular row
amatrix[2,]

# subsetting one particular column
amatrix[,2]

# subsetting one specific value
amatrix[2,2]

```

For data frames the use of `[]` is limited, as you can only subset the class.

```{r}

adf <- data.frame(numbers=1:4,
                  fruits= c("banana", "peaches", "orange", "strawberry"),
                  value= c(T,F,F,T))

# subsetting the whole class

adf[1]

```

The second operator, `[[]]`, is mostly used for lists. It is quite similar to `[]`, but is important to differentiate within values.

```{r}
alist <- list(numbers=c(1:10),
      fruits=c("Banana", "Peach"),
      values=c(T,T,F,T,F))

# subsetting a specific data set in the list
alist[[2]]

# subsetting a specific element within a data set
alist[[2]][1]
```


The thrid operator, `$`, is especially used for data frames. You can subset a whole variable, even if you only partially match the variable name.

```{r}

adf <- data.frame(numbers=1:4,
                  fruits= c("banana", "peaches", "orange", "strawberry"),
                  value= c(T,F,F,T))

# subsetting a whole variable

adf$numbers

# even with partial matched naming

adf$num
```


Of course, you can combine the subset operators to create the desired data set. However, if you want to precisely dissect numeric data, then you can use conditions.


```{r}

adf <- data.frame(numbers=1:4,
                  fruits= c("banana", "peaches", "orange", "strawberry"),
                  value= c(T,F,F,T))

# subset a specific number

adf[adf$numbers==2,]

# subset a number that is higher/lower than

adf[adf$numbers<=3,]

```

By the way, you can also assign/replace new numbers by using conditions.

```{r}

adf <- data.frame(numbers=1:4,
                  fruits= c("banana", "peaches", "orange", "strawberry"),
                  value= c(T,F,F,T))

adf[adf$numbers==2,] <- 10
adf

```

## Conditions

With the last part of the previous chapter \@ref(subsetting), we implicitly introduced conditions in R. However, writing conditions is not to difficult in the first place and can be used in various ways

```{r}

points <- c(12,4,15,7,10)
sum(points)

if (sum(points) >30) {
  print("Passed")
} else  {
  print ("Failed")
}
# else statement for display something else in case the if condition is false

```

Of course, more conditions can be added.

```{r}

points <- c(12,4,15,7,10)
sum(points)

if (sum(points) >50) {
  print("Grade A")
} else if (sum(points)>40) {
  print("Grade B")
} else if (sum(points)>30) {
  print("Grade C")
} else if (sum(points)>20) {
  print("Grade D")
} else if (sum(points)>10) {
  print("Grade E")  
} else if (sum(points) >=0){
  print("Grade F")
}
  
  
```


## How to Write Functions

To really calculate and use statistical methods, you should be able to write all functions in R chunks. This might lead to difficulties, because not all function are built-in or included in a package. In order to be able to write functions on your own, you will need to understand the following logic.


```{r}

# designing a simple function

firstfunction <- function(a){
  a^2
}

firstfunction(4)

# firstfunction is a random name for one function
```

To receive several returns on more than one expression in the function, you should create a list.

```{r }
onemorefunction<- function(a){
  list(ff=a^2, sf=a^3, tf=a^4) 
}

onemorefunction(2:5)

# In case of further calculations with the returns, you should assign it to an object

furthercalc <- onemorefunction(2:5)

furthercalc$ff
```

Appropriately to what you need to do, you can include more variables in your function

```{r}
superfunction <- function(x, y){
  y*x^2
}
superfunction(4, 2)
```

In case that you do not want to only trust on reproducing the order, than you can also call the variable to return correctly.

```{r}
afunc <- function(c, d, g){
  g/c*d^2
}
afunc(g=4, c=2, d=10)
```

In order to now combin knowledge from the previous subchapter with this one, we can create conditions dependent on functions.

```{r}
roots <- function(a, b, c){
  
  if (b^4- 3*a*c <0) {
    print("No solution! (negativ root can't be squared)")
  } else {}
  
  (-b + sqrt (b^2- 4*a*c)) / 2*a
}
roots(a=2, b=3, c=1)
roots(a=4, b=1, c=2)
```

<!--chapter:end:05_Chapter_04_Data_Analysis.Rmd-->

---
title: "06_Chapter_05_Packages in R"
author: "CTrierweiler,PGaulke"
date: "22 Juli 2019"
output: html_document
---

# Data Sets, Visualisation, and Packages in R

You already learned that R provides some built-in functions (such as `seq()`) that make your work more comfortable. However R provides also built-in data sets, that you can use for example calculation or data analysis.

One example for this is the data set mtcars.

```{r dataframe}
mtcars

data(mtcars)
class(mtcars)
mtcars
head(mtcars)
str(mtcars)

names(mtcars) 
length(mtcars)
nrow(mtcars)
```
You can find all built-in data sets here: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html

However, apart from built-in functions and built-in data sets, there is even more to explore. In the following, we will explain how to create your individual and best R environment.

## Import Data

The actual idea of this book is that we want to enable you to analyze data in R. It will be barely possible to do so without being able to import the data you want to analyze in R. Therfore, we want to put data from other files into a data frame in order to work with it in R. With Base R, this is possibly for at least some types of files, however, for others there are some special packages to use which we wil thematize in the chapter \@ref(packages).


Generally, there are different funtion in how to read a file. The most common one is `read.table`. With this function you can read rectangular data and convert it into a data frame. For all the arguments you should check the help section `?read.table`.
Most importantly you have the arguement `file` which requires a path to find a data. If you have the file in the same folder, then the name of the file is enough. Also of high importance is the `sep` arguement which indicates the character that seperates the values between different columns.


```{r, eval=FALSE}
randomdatafile <- read.table(file="filename.txt",
                             sep=",")

``` 

For other files, R follows the logic of `read.xxx`. The xxx specifies the data format (e.g. read.csv -> .csv files)

## Data Visualisation

In the following, we will describe how you can visualize data in R. This will be limited to the base R functions, in the chapter \@ref(ggplot) you will find a way to plot data more effectively.

In order to start with this topic, at first we will look at the simplest way of plotting.

The *scatter plot* is a simple line plot in which you plot one variable against an index on the x axis Both vectors need to have exactly the same length, and of course, they need to be numeric.

The main function to use here is `plot`.

```{r simple-plots-1, out.width="25%", fig.align="center", fig.height=10, fig.cap="Creating a simple line plot.", fig.show="hold"}
revenue <- c(59000, 58000, 62000, 62000, 65000, 66000)
month <- c(01, 02, 03, 04, 05, 06)

plot(month,revenue, type ="l")
```


This looks pretty plain, so we can add some individual arguments. Show 
We now add further customization with new functions and arguments.  

- `col` adding a color (for details:(http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf)
- `lty` setting the line type (for details: http://www.sthda.com/english/wiki/line-types-in-r-lty)
- `lines` adding the plot of a vector to a previously opened plot.
- `axis` changing the axis given in its first argument with 1, 2, 3, 4 (bottom, left, top, and right)
- `at` stating for what values of the axis the labels should correspond.
- `las` stating if lables are showed horizontal or vertical
- `xlab` and `ylab` are the x and y axes labels, respectively.
- `xlim` and `ylim` set a numerical limit for the x and y axes labels, respectively, notice that a vector of length 2 is necessary for each.

The following arguements need to included in the chunk, but seperately from the actual function.
- `legend`setting a legend for the plot
- `title` setting a title for the plot

```{r simple-plots-2, out.width="25%", fig.align="center", fig.height=10, fig.cap="Creating a simple line plot.", fig.show="hold"}
revenue <- c(59000, 58000, 62000, 62000, 65000, 66000)
month <- c(01, 02, 03, 04, 05, 06)

plot(month,revenue, type ="l", col="blue",
     axes=TRUE,
     xlab = "Month",
     ylab = "Revenue"
     )
title (main="Halfyearly Plot")
```

Alternatively, you can also create barplots, histograms, boxplots, pies and other plots. For example, for a barplot you should take the function `barplot` and consider the following arguments.

- `col` for setting the colors
- `horiz` for setting the direction of the plot
- `border` setting the design of the borders of the plots
- `beside` forces side-by-side bars instead of stacking bars

```{r bar-plots, out.width="25%", fig.align="center", fig.height=7, fig.cap="Simple bar plots.", fig.show="hold"}

barplot(revenue,
        main="Monthly Revenue",
        names.arg=c("Jan","Feb","Mar","Apr","May","June"),
        border="black",
        col = "blue")


```


## Data Packages {#packages}

By installing new packages (again: Tools > Install packages) you can download additional tools for R, that gives you access to more operations, functions, and coding options. Before we introduce some major R packages that will make data science a bit easier and faster, please consider this short notice.

In case that you use anything out of an additional R package that you downloaded, you always have to include the following process when reopening the respective project.

```{r, eval=FALSE,error=TRUE}

library(package)

```
This step is necessary to reload the package and use its functions. You do not necessarily need to reinstall the whole package, but loading it from your library will definitely be required.


###MagrittR {magrittr}

Now you will meet a complete new operator for the first time that comes with the package `magrittr`. This operator is called pipe `%>%`. It provides a different way of writing operations into chunks, by which you type in your operation from left to right, instead from the outside to the inside.
From a mathematical point of view, this means`x %>% f` is equivalent to `f(x)`, `x %>% f(y)` is equivalent to `f(x, y)`, and `x %>% f %>% g %>% h`is equivalent to `h(g(f(x)))`


```{r}
require(magrittr)

somenumbers <- c(200,300,700,50,400)
sum(somenumbers)

somenumbers %>%
	sum()

sqrt(sum(somenumbers))

somenumbers %>%
	sum() %>%
	sqrt()


```

The transformation process of data frames can be processed in one operation with piping.

df_after_f <-f(df)
df_after_g <-g(df_after_f)
df_after_h <-g(df_after_g)

with piping it is

df %>%
  f %>%
  g %>%
  h
  
Furthermore, you can also use placeholders for an element that you placed before the pipe.


```{r}

#single placeholder
round(1.66666666,2)

2 %>%
	round(1.66666666, .)

#multiple placeholders
mtcars %>%
  subset(hp > 100) %>%
  aggregate(. ~ mpg,.,mean)
```


## Tidyverse {tidyverse}

Tidyverse is a large package that basically includes different packages such as `tibble`, `tidyr`,`readr`, `dplyr` and `ggplot2`. Considering all the functions and possibilties that tidyverse provides, it can be seen a subdialect of R. For a detailed overview of what tidyvere is, and what's included, see here: https://www.tidyverse.org/. Especially, the cheat sheets for ReadR and TidyR are recommendable: https://rawgit.com/rstudio/cheatsheets/master/data-import.pdf.

As a first step, please load `tidyverse`.

```{r, eval=FALSE, error=TRUE}

install.packages("tidyverse")

```

#### tibble

In a first step, we will go through the function and benefit of `tibble`. Tibble is generally a description for a data frame in tidyverse. All tibbles are data frames, but not vice versa.
Using tibble instead of regular data frames provides us benefits in terms of pace, output, informations, and simplicity.

A data set is easily created as a tibble, therefore you have to options:

```{r}

library(tidyverse)

# creating a new data set as a tibble from scratch
new_tib <- tibble(
  a = 1:5, 
  b = 5, 
  c = 20:16,
  d = 3:7
)
new_tib

# converting an existing data set into a tibble


tibmtcars <- as_tibble(mtcars)
tibmtcars

```

#### tidyr

In order to continue with `tidyr`, we are now more about how to organize a data set.
The principle of tidy data is that, that every column is a variable, every row an obersvation and every type of observation belongs in a different table. Tidyr is mainly based upon the following functions:

- `gather`
- `spread`
- `seperate`
- `unite`

To `gather`is the function that let you create key-value pairs out of multiple pairs. A large horizontal data set can therefore be converted in a vertically larger data set. This can be beneficial in order to get a clear overview on the data set.



```{r}
pricing <- tibble(type= c("B2C","B2B"),
                  productA= c(20,15),
                  productB= c(75,70),
                  productC= c(30,20),
                  productD= c(60,55),
                  productE= c(15,10)
                    )
pricing

```
Taking this example, we see that we actuall have the following three variables: type of business, product, and price. However, we have 6 columns, which obviously does not correspond to a tidy data set, in which every variable is a columnn.
Therefore, we should now tidy the data set up by considering the following logic:
- `key`, which are the messy columns (here the products)
- `value`, which are the messy values in the messy cells (here the prices)

```{r}
#The empty call is:
#gather(df, key, value, messy_col1, ..., messy_coln)

require(tidyr)

tidy_pricing <- gather(pricing,
                        key= "products",
                        value= "price",
                        productA:productE
                        ) 
tidy_pricing


```


In contrast, the function `spread` works the opposite way. Therefore, it creates a horzontally larger data set by increasing the amounts of columns according to the given variables.

```{r}
sales<- tibble(
  business= c(rep(c("B2B","B2C","Mixed"),2),"Philantrophy"),
  products= c(rep(c("product", "revenue"),3), "donation"),
  details= c("productA", 300, "productC", 240, "productB", 120, 50)
        )

sales

tidy_sales<- spread(sales, key=products, value=details)
tidy_sales

```

The function `seperate` does what its name implies, it seperates columns. The seperation can be done by different ways, you can let recycling do its work, or base it on numbers and characters.

```{r}
require(tidyverse)

# The empty call is
# separate(df, messy_var, into=c(tidy_var1, tidy_var2))

#Example for using recycling

mixedup <- tibble(info=c("Shanghai,China", "Oslo,Norway"))
mixedup

tidy_mixedup <- separate(mixedup,
                          info,
                          into= c("city", "country")
                          )

tidy_mixedup
                  
# Example for using characters

tidy_mixedup2 <- separate(mixedup,
                          info,
                          into=c("city","country"),
                          sep="a")
tidy_mixedup2

# Example for using numbers of characters

tidy_mixedup3 <- separate(mixedup,
                          info,
                          into=c("city","country"),
                          sep=5)
tidy_mixedup3


```  

Finally, the function `unite` can be simply used for the opposite. By this function you can put two columns together.

```{r}
# The empty call is:
# unite(df, tidy_var, messy_var1, messy_var2, sep="")


backtotheorigin_mixedup <- unite(tidy_mixedup, info, "city", "country", sep=",")
backtotheorigin_mixedup
```

#### readr

The package readr provides you a fast and comfortable way of using data from other data formats.
The follwing file formats are supported by readr

- read_csv(): comma separated (CSV) files
- read_tsv(): tab separated files
- read_delim(): general delimited files
- read_fwf(): fixed width files
- read_table(): tabular files where columns are separated by white-space.
- read_log(): web log files

Readr tries automatically to convert the data from the file into a tibble data set in a way, that column specification is as appropriate as possible. These are basically the main advantages, beside that it is much faster than base R imports.

For us, the most important files are .CSV files as most data sets are create in Excel-files. However, it is pretty easy to just drop the file in your project folder and then use this formula:

```{r, eval=FALSE, error=FALSE}


idea_of_a_name <- read_csv(readr_example("filename.csv"), col_types = 
  cols(
    firstcolumnname = col_double(),
    secondcolumnname = col_integer(),
    thirdcolumnname = col_character(),
    etc = col_integer(),
      )
)


```

#### deplyr

`Dplyr`is a toolset for data manipulation. The packages includes five essential function, which are the following:

- select() picks variables based on their names
- mutate() adds new variables that are functions of existing variables 
both of the two above applied on columns 

- filter() picks cases based on their values
- arrange() changes the ordering of the rows
both of the two above applied on rows

- summarise() creates a summary out of multiple data sets

Before we start with the above mentioned functions, first things first, under the following link you will find a cheat sheet: https://github.com/rstudio/cheatsheets/blob/master/data-transformation.pdf.


As you already met the piping operator `%>%`  in the chapter about magrittr(HIER REF), we will apply it within this chapter. Especially, when manipulating a data set, piping provides an easier and more efficient approach than base R. Furthermore, you can combine different functions of manipulation in one step.

```{r}
starwars
```


First we will start with the operator `select`. 


```{r, eval= FALSE}
# The empty call is (base R)
# select(df, var1, ..., varn)
# or with piping...
# df %>%
#   select(var1,..., varn)

#here a practical example with mtcars

mtcars[,c("mpg","hp")]


mtcars %>%
  select(mpg,hp)



```
Furthermore, you can select in the following ways:

```{r, eval= FALSE}
#by columns

mtcars %>%
  select(1:3)

mtcars %>%
  select(mpg:hp)

mtcars %>%
  select(-5)

mtcars %>%
  select(-hp)


```

The selection can be designed very individually by using helper arguments that describe for example a word that should be included in the variable. All helper arguments can be found in the help section:`?select`


The `mutate` function provides the possibilty to create new columns based on existing ones.

```{r}
# The empty call is (base R)
# mutate(df, new_variable = expression)
# or with piping...
# df %>%
#   mutate(new_variable = expression)

#Practical example:

mutate(mtcars, kmpg = mpg*1.60934)


mtcars %>%
  mutate(kmpg= mpg*1.60934)


```

The `filter` function is somehow similar to the `select` function but for rows. The procedure is more or less the same.

```{r}
# The empty call is (base R)
# filter(df, condition)
# or with piping...
# df %>%
#   filter(condition)

mtcars[mtcars$mpg >=20,]

mtcars %>%
  filter(mpg >= 20)

```

You find all operators for conditions in the help section: ?Comparison


The function `arrange` enables you to create a new order by considering the value of variable.

```{r, eval= FALSE}
# The empty call is (base R)
# arrange(df, var1)
# or with piping...
# df %>%
#   arrange(var1)

arrange(mtcars, desc(hp))

mtcars %>%
  arrange(desc(hp))



```


The fiveth function `summarise` provides the possibility to create a new data frame by deriving summarzing calculations from an existing data set.

The expr means a function on a vector, repsectively a variable.

```{r, eval = FALSE}
# The empty call is (base R)
# summarise(df, name = expr)
# or with piping...
# df %>%
#   summarise(name = expr)

summarise(mtcars, averagepower = mean(hp))

mtcars %>%
  summarise(averagepower = mean(hp))
```  

Again, helper function can be found in the help section `?summarise`


For some more specialised manipulation tasks you can use the function `group_by` which allows you to choose a specific group on which to apply your manipulation operation.

```{r}
mtcars %>%
  group_by(hp >200) %>%
  summarise(
    mean(mpg)
  )
# interesting result by the way

```
#### ggplot2 {#ggplot}

With the package ggplot2 you have more possiblities to visualize your data. The range in a plot will adopt automatically to new data, it will be drwan as an object instead of an image, the legend can be created automatically and the framework for plotting is unified.


Creating a scatter plot for example, works like this:

```{r, fig.show='hold', results='hide', tidy=FALSE}
library(ggplot2)
newggplot<- ggplot(mtcars, aes(x=cyl, y=hp)) + 
         geom_smooth(method=lm , se=FALSE, aes(col=cyl)) +
         geom_smooth(method=lm , se=FALSE, linetype=1, col="grey", aes(group=1)) + 
  labs(subtitle="Car Power", 
       y="Horsepower", 
       x="Cylinders", 
       title="Scatterplot", 
       caption = "Source: mtcars")

plot(newggplot)


```

The way how to write with ggplot2 might look confusing in the first moment, but there is a consistent logic behind that.


```{r}
# First you choose the data and set the mapping

Superplot <- ggplot(data=mtcars, mapping= aes(x=hp, y= mpg))
Superplot
# The you tell the general form

SP2 <- Superplot + 
      geom_line()  
SP2
# Then you have the possibility to add a variable

SP3 <- ggplot(mtcars, aes(x=hp, y= mpg, col="blue")) +
      geom_point()
SP3
# Or even attributes

SP3b <- ggplot(mtcars, aes(x=hp, y= mpg)) +
      geom_point(col="blue")
SP3b
```

By `aes` we map our data, which includes to tell x/y axis, colour, fill, size, labels, line widt, line type.

By `geom` we set the shape of our object (_point,_line,_histogram,_bar,_boxplot) and set attributes.

<!--chapter:end:06_Chapter_05_Packages_in_R.Rmd-->

---
title: "08_Ersatzfile"
author: "CTrierweiler, PGaulke"
date: "24 Juli 2019"
output: html_document
---
# (PART) Statistics for Data Science {-}
# Predictive and appropriate model fitting 

http://127.0.0.1:30892/rmd_output/1/creating-files-in-r.html

```{r, include=FALSE}
library(readr)
advertising = read_csv("data/advertising.csv")
```

People want to make predictions because nothing is clearly true in this world. The answers of the predictions are based on data, and not on intuition. Therefore, people make use of predictive modeling in order to forecast future actions through using data and calculations of propability. Every predictive model has some predictive variables which can influence future actions. 

Today AI is a big topic. However, AI will not tell us WHY something happened, it will not answer questions of "What if, when this..." it only presents a complicated set of correlations but not the causations. 

Moreover, in preditive modeling, there is no lunch theorem. There is not only one technique. There are many techniques, some of them are better, some not. Therefore, it is the goal to find out which works best. 

The model can be a simple linear equation or a complex tree-based model. 

In this part of the book, statistical models are presented. Starting from simple model building over  regression, classification, unsupervised learning methods, practical examples and new methods like trees, random forests, bagging and boosting. 


## Building models and predictions 

In the process of predictive modeling, first data is collected for the predictive variables before the actutal model is build. 

Example 

The wage is correlated with the age. A variable to predict is needed. 

\[Y=f(X) + \varepsilon \]

Y: response variable 
f(X): set of independent (1, 2, 3), can be inifitive functions, but then it can't be presentanymore
$\varepsilon\:$ shock 

This example will be based on simulated data. Knowing the truth will be very helpful. But truth can not be known unless it will be simulated. 

In this model, f needs to be find, the predictions. Therefore, X is taken in order to see what can be predict. 

In many occasions, the independent variables are known but the response is not. Therefore, \(f()\) can be used to predict these values. These predictions are noted by
\[\hat{Y}=\hat{f}(X)\]
where \(\hat{f}\) is the estimated function for \(f()\).

Besides making predictions, inference is a second major reason why estimating \(f()\) is useful. 

The estimated \(\hat{f}()\) is also used to answer questions about the relationship between the independent variables and the response variables, such as:

  - which predictors contributes the response,
  - how much each predictor contributes to the response,
  - what is the form of the relationship.
  
  
The first step will always be that the given data set will be inspected. The goal is to understand the basic structure of the data set. Therefore, the data set will be just printed in order to read the structure easily. 

```{r}
advertising
```
Interpretation: The data set "advertising" includes 200 observations and 4 variables, each of which is numeric. In this data set, Sales will be the response variable. The response variable is the variable which will be used to understand the relationship with the predictor variables, which are in this data set: TV, Radio and Newspaper. 

The next step after identifying the structure of the data set will be that the data set should be brought to visalization. This could be perfectly done with a scatter plot, because of in this example, numeric variables are used. 

However, to understand the data, it is essential that a deeper understanding of regression will be provided. In the next chapter, regression will be explained.  


## Regression 

Regression is a type of supervised learning. In supervised learning, addresses issues whre thre are both an input and an output. These issues in regression deal with a numeric output.


For describing the names of variables and methods, different terns are used in AI, statistical or machine learning. 

Input e.g.: predictors, input/feature vector. These inputs can be either numberic or categorical. 

Output e.g.: response, output/outcome, target. These outputs have to be numeric. 

The goal of regression is to make predictions on undetected data. This can be done through controlling the complexity of the model to protect against under- and overfitting.  
Manipulating the model complexity will accomplish this because there is a bias-variance tradeoff. The bias-variance tradeoff increases the flexibility. It is more shaky and closer to the data but it also increases the variance. The sum is always U-Shaped. 

Furthermore, it will be known that the model generalizes because it is evaluating metrics on test data. Only the (train) models on the training data will fit. The analysis begings with a test-train split. In the regression tasks, the metric will be the RMSE. 


The next step after investigating the structure of the data, is to visualize the date. Due to the fact that in regression is only numeric variables, a scatter plot can be used. 

```{r}
plot(sales ~ TV, data = advertising, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "sales vs television advertising")
```

The function pairs() is helpful in order to visualize a number of scatter plots quickly. 
```{r}
pairs(advertising)
```

In many cases, the most interesting thing to know will be the raltionship between ach predictor and the reponse. Therefore, the function featurePlot() from the package caret is very useful. 

```{r, fig.height = 4, fig.width = 10, message = FALSE, warning = FALSE}
library(caret)
featurePlot(x = advertising[ , c("TV", "radio", "newspaper")], y = advertising$sales)
```
Interpretation: In the graph, a clear increase in sales can be seen as radio or TV are increased. The relationship between sales and newspaper is less clear. How all of the predictors work together is also unclear, as there is some obvious correlation between radio and TV. 

## The lm() Function 

In order to illustrate how the response "sales" will fit every residual variable as predictor. This code includes an additive linear model. The linear model will be later explained in detail in this book. But at this stage, it is only important to notice that with using the attach() function, instead of using the argument data= the model will be specified withing using each variable name directly. 

```{r}
mod_1 = lm(sales ~ ., data = advertising)
# mod_1 = lm(Sales ~ TV + radio + newspaper, data = advertising)
```


## Hypothesis testing 

Standard errors can also be used to perform hypothesis tests on the coefficints. The most common hypothesis task involves testing the null hypothesis of 

H0: There is no relationship between X and Y versus the alternative hypothesis 

HA: There is some relationship between X and Y 

Mathematically, this correspond to testing 

H0 : $\beta_1$ = 0

vs 

HA: $\beta_0$ = 0

since if $\beta_1=0$ then the model reduces to $Y=\beta_0$ + em and X is not associated with Y.

The function  summary() returns a large amount of useful information about a model fit using lm(). Much of it will be helpful for hypothesis testing including individual tests about each predictor, as well as the significance of the regression test. In the following example, an additive linear model with sales as the response and each remaining vairbale as a predictor. 

```{r}
summary(mod_1)
```

```{r}
mod_0 = lm(sales ~ TV + radio, data = advertising)
```

```{r}
mod_1 = lm(sales ~., data = advertising)

summary(mod_1)
```

## Predictions 

In order to make predictions, the function predict() is very heplful. An important fact to be aware of is, that when the function is used on the result of a model fit using, by default it will retun the predictions for each of the data points used to fit the model. 

```{r}
head(predict(mod_1), n = 10)
```
Interpretation: Because the example should be easily to real, the result is limited by n= 10. 

A further essential point to notice is that the effect of the function predict() will always be depending on the input to the function. In this case, the model object of the class lm is supplying as the first argument. Therefore, the function predict() runs the predict.lm() function. Hence, the function ?predict() can serce as a source for more details. 

The next step should be that the new data needs to be specified. Therefore, the new data should be a data frame or a tibble with the same column names a predictors. 

```{r}
new_obs = data.frame(TV = 150, radio = 40, newspaper = 1)
```

After this step, the function predict() can be used to point the estimates, the confidence intervals as well as the prediction intervals. 

It is helpful the use only the first two arguments, because R will easily retun a point estimate which will be the predicted value, $\hat{y}$. 


## Unusual Observations

The software R supply numerous functions for obtaining metric related unusual observations. 

- The function resid() can supply the remaining for each observation
```{r}
head(resid(mod_1), n = 10)
```

- The function hatvalues() gives the leverage of each observation
```{r}
head(hatvalues(mod_1), n = 10)
```

- The function rstudent() gives the studentized remaining for each observation
```{r}
head(rstudent(mod_1), n = 10)
```

- The function cooks.distance() estimates the influence of each observation
```{r}
head(cooks.distance(mod_1), n = 10)
```

## Linear regression 

Linear regression is a simple approach to supervised learning. It assumes that the dependence of Y on X1, X2, ... Xp is linear. 
Various important reasons explain why it is often the first tool in any analyst's toolbox.
On the one hand the model can straightforwardly be extended and produce reasonably good estimates in many applications. On the other hand, despite its simplicity, it allows to clearly illustrate advanced concepts. In particular, it lays the ground for the need of more complicated techniques.

The linear regression model is very fast. In the following example, the relationship between different advertising methods and sales is visualized. The relationship is not causal, but the correlations can be detected. Every blue points presents an observation. There are several questions which could be asked: 

- Is there a relationship between sales and the advertising budget?
- How strong is the relationship between sales and the advertising budget 
- Which method contributes to sales?
- How precise is the prediction of the future sales?
- Is the relationship linear?
- Is there synergy among the advertising media?


Simple linear regression using a single predictor X. 

- The assumed model is

\[Y=\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p + \varepsilon\]

$\beta_0$ and $\beta_1$: two unknown constants that represent the intercept and slope, also known as coefficients or parameters 
$\varepsilon$: error term 


This type assumes linearity in the coefficients, \(\beta\)'s, with \(p\) predictors, \(X\)'s,  for \(n\) observations. The response \(Y\) is also assumed to be influenced by shocks or errors record by \(\varepsilon\). The standard deviation of these errors is assumed to be \(\sigma\). 

Hence, even if the estimated model would correctly fit the data, the predictions would still be off because of this irreducible error.  

An important notice is that the linear function is almost never believed to fit the true data generating process but, instead, to more or less appropriately approximate it.       


This part builds around the example of the simple linear regression of sales on the amount of TV advertising in the advertising data set.  
To fix ideas, the linear model estimated here is

\[\text{sales} = \beta_0 + \beta_1\times \text{TV}  + \varepsilon\]


First step is to load the data and manipulate it to make it usable.

```{r, warning = FALSE, message = FALSE}
require(tidyverse)
require(ISLR)
advertising <- read_csv("data/advertising.csv")
advertising
advertising <- read_csv("data/advertising.csv") %>%
	select(-X1) 
advertising
```


The estimation of the model is carried with the function `lm` from the built-in `stats` package. The result of the estimation is an object to assigned to a name.

```{r}
model.slr <- lm(sales ~ TV, data = advertising)
```

The content of this linear regression object is better described with the function `summary`.

```{r}
summary(model.slr)
names(model.slr)
model.slr$fitted.values
names(summary(model.slr))
summary(model.slr)$r.squared
summary(model.slr)$df
```
There is also an alternative with tidyverse
```{r}
advertising %>%
  mutate(y.hat1 = model.slr$fitted.values,
         y.hat2 <- predict(model.slr),
         y.hat3 <- model.slr$coefficients[1] + model.slr$coefficients[2]*TV)
advertising
advertising$TV
advertising$y.hat1 <- model.slr$fitted.values
advertising$y.hat2 <- predict(model.slr)
advertising$y.hat3 <- model.slr$coefficients[1] + model.slr$coefficients[2]*advertising$TV
advertising
plot(advertising$TV, advertising$sales)
lines(advertising$TV, advertising$y.hat2, col="blue")
lines(advertising$TV, advertising$y.hat1, col="red")
```

Interesting to see are also the errors/residuals of the prediction.
```{r}
advertising$residuals <- advertising$sales - advertising$y.hat2
sum(advertising$residuals)
```

The model can be used to predict for data not in the training data.

```{r}
# predict sales for TV=400, 500, 600...
# brute force way, very tedious in general
sales_tv_400 <- model.slr$coefficients[1] + model.slr$coefficients[2]*400
sales_tv_400
# 'predict' way
# step 1: create a data.frame for the new X data
# step 2: predict with newdata= newdata
my.boss.question <- data.frame(TV=c(400, 500, 600))
my.boss.question
sales_tv_boss <- predict(model.slr, newdata = my.boss.question ) 
sales_tv_boss
```

 
An important notice is that parts of this regression object can be accessed through sub-setting of the object and used, once know under what name they are stored, which can obtain by the next call 

Recall that `?lm` can serve as a good source for more details for the value of the function, i.e., what the function can return. 

```{r}
names(model.slr)
# model.slr %>% names
```
Alternatively, and somehow more surprising, all the numbers given by the `summary` function can also be accessed in the same fashion.

```{r}
model.slr %>% summary %>% names
```

| Quantity | Value |
|:--|:--|
| Residual Standard Error | `r summary(model.slr)$sigma %>% round(2)` |
| \(R^2\) | `r summary(model.slr)$r.squared %>% round(3)` |
| \(F\)-statistic | `r summary(model.slr)$fstatistic[1] %>% round(1)` |

Table: Results for simple linear regression (Advertising)


As for the confidence interval of \(\beta_1\)  i.e., the random interval in which, under repeated sampling, the true parameter would fall \(95\%\) of the time, we type the code below.

```{r}
c.i.beta1 <- c(summary(model.slr)$coefficients[2,1] -
                 2 * summary(model.slr)$coefficients[2,2],
               summary(model.slr)$coefficients[2,1] +
                 2 * summary(model.slr)$coefficients[2,2])
c.i.beta1 %>% round(3)
```

One of the main reasons the simple linear regression is exposed is its graphical appeal. In particular, the ordinary least squares criterion can be visualized with a graph of the residuals with respect to the fit.  
This visualization builds on the regression fit which we obtain first below in two alternative ways.  

1. The fitted line can be obtained with the fitted values of the model given by the `lm` function, i.e., `.$fitted.values`.

```{r}
tibble(advertising$TV, advertising$sales, model.slr$fitted.values)
```

When going deeper into the details, it becomes clear that these fitted values  are simply obtained. This is because of estimated parameters using the \(X\) values which is TV, in this case. 
```{r}
manually.fitted <- model.slr$coefficients[1] + model.slr$coefficients[2] * advertising$TV
all.equal(as.vector(model.slr$fitted.values), manually.fitted)
```


2. The second approach uses the function `predict` from the built-in `stats` package. The function is a bit versatile as its behavior depends on which type of objects it is fed with.  
Applied to a `lm` object, it will, by default, return predictions for each of the \(X\) values used to fit the model. 

```{r}
all.equal(as.vector(model.slr$fitted.values), manually.fitted, predict(model.slr))
```

Through the use of fitted/predicted values, the values can be estimated above about the quality of the fit. Below are a few lines of code to manually calculate these statistics.

```{r}
## R2
TSS <- sum((advertising$sales - mean(advertising$sales))^2)
TSS
RSS <- sum((advertising$sales -  predict(model.slr))^2)
RSS
R2 <- 1 - RSS/TSS
R2 %>% round(3)
## RSE
n <- length(advertising$sales)
p <- length(model.slr$coefficients) - 1
RSE <- sqrt(RSS /(n - p - 1))
RSE %>% round(2)
# notice that this is more or less the sd of the errors
sd(advertising$sales -  predict(model.slr)) %>% round(2)
## F-statistic
F <- (TSS - RSS)/p * (RSS/(n-p-1))^(-1)
F %>% round(1)
```

A further step is to turn the graph of the fit.  
As much as possible,  `ggplot` should be used for our graphs. In this case, first, the predicted/fitted values to the data frame needs to be added. There are various, though similar ways to achieve that first step, including one with `geom_smooth`.


```{r, out.width = "100%", message = FALSE, warning = FALSE, include=TRUE}
advertising <- advertising %>%
	mutate(fit_TV= model.slr$fitted.values)
	# mutate(fit_TV= predict(model.slr))
	# mutate(fit_TV = predict(lm(sales ~ TV), interval = "confidence")[,"fit"])
         
p1 <- advertising %>% ggplot(
	mapping= aes(x=TV, y= sales)
	) +
	geom_point(size=1, shape=21) +
	#geom_smooth(method='lm', se = TRUE) + # another alternative for the fit
	geom_line(aes(y=fit_TV), color ="blue", size =1) +
	geom_segment(aes(x = TV, y = sales, xend = TV, yend = fit_TV, colour = "red")) +
	theme(legend.position = "none")
p1
```



###Assessing Model Accuracy

There are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that is most interesting is the root-mean-square error.

\[MSE=\frac{1}{n}\sum_i^n \big(y_i-\hat{f}(x_i)\big)^2\]


While for the sake of comparing models, the choice between RMSE and MSE is arbitrary, there is a preference for RMSE, as it has the same units as the response variable.


Here it is also important to establish the essential feature of the statistical learning philosophy.  
Any chosen method/technique is given data to learn, the **train data**. However, the crucial attribute of the method should be measure on data not previously seen, the **test data**.  
In other words, the important measure is the _test MSE_. It is computed as
\[\text{Ave}\big(y_0-\hat{f}(x_0)\big)^2 \]
where \((y_0, x_0)\) are the test observations.  


### Model Complexity

Besides the fact how well a model makes precitions, it is also interesting to know the complexity/flexibility of a model. In this chapter, so make it simple, only linear models are considered. In fact, the model gets more complex when more predictors are added to the model. In order to assigning a numerical value to the complexity of the linear modl, the number of predictors $p$ wil be used. 

```{r} 
get_complexity = function(model) {
  length(coef(model)) - 1
}
```

In order to add more complexity, interactions, polynominals and transformations can be used. These will be explained at a later state of this chapter.  


### Test-Train Split

For the case of determining how well the model predicts, issues with fitting a model to all available data then using RMSE occur. This can be seen as cheating. The RSS and hence the RMSE can never increae when a linear model becomes more complex. Th RSS and the RMSE dan only decrease or in special cases could stay the same. Hence, the believe could arise that a largest model as possible should be used in order to predict well. But this is not the case because it is very difficult to fit to a peculiar data set As soon as a new data is seen, a large model could predict unfortunate. This issue is called **overfitting**. 

It is very useful to split the given data set into two halds, whereby one half is the **training** data, which is used to fit (train) the model. The other half is the **test** data which is used to assess how well the model can predict. It is important that the test data will never be used to train the model. 


In this example, the function `sample()` will be used in order to get the random sample of the rows of the original data set. The next step is to use those rows as well as the remaining row numbers to split the data correspondingly. Moreover, the function `set.seet()` will be applied in order to replicate the same random split everytime the analysis will be performed. 
```{r}
set.seed(9)
num_obs = nrow(advertising)

train_index = sample(num_obs, size = trunc(0.50 * num_obs))
train_data = advertising[train_index, ]
test_data = advertising[-train_index, ]
```


In this example it is important to concentrate on the **train RMSE** and the **test RMSE**. These are two measures which assess how well the model can predict. 


$$
\text{RMSE}_{\text{Train}} = \text{RMSE}(\hat{f}, \text{Train Data}) = \sqrt{\frac{1}{n_{\text{Tr}}}\displaystyle\sum_{i \in \text{Train}}^{}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
$$
In the measure of the train RMSE, $n_{Tr}$ demonstrates the numbers of observations given in the train data set. When the complexity of the linear model increases, the train RMSE will decrease, or in a special case stay the same. Therefore, when comparing the models, the train RMSE is not useful. However, it can be a helful step to prove if the RMSE is going down. 


$$
\text{RMSE}_{\text{Test}} = \text{RMSE}(\hat{f}, \text{Test Data}) = \sqrt{\frac{1}{n_{\text{Te}}}\displaystyle\sum_{i \in \text{Test}}^{}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
$$
In the measure of the test RMSE, $n_{Tr}$ demonstrates the number of observations in the given test data set. In the training data set, the test RMSE is used to fit the model, but assess on the unused test data. This is a procedure for how wll the fitted model is predicting usually, not just how well it fits the data sed to train the modl, as it is the case for the train RMSE. 


```{r}
# starting with a simple linear model, with no predictors
fit_0 = lm(sales ~ 1, data = train_data)
get_complexity(fit_0)

# train RMSE
sqrt(mean((train_data$sales - predict(fit_0, train_data)) ^ 2))
# test RMSE
sqrt(mean((test_data$sales - predict(fit_0, test_data)) ^ 2)) 
```
Interpretation: the operations use the train and the test RMSE. 


```{r}
library(Metrics)
# train RMSE
rmse(actual = train_data$sales, predicted = predict(fit_0, train_data))
# test RMSE
rmse(actual = test_data$sales, predicted = predict(fit_0, test_data))
```
Interpretation: the function can be enhanced with inputs which are obtaining.
It is helpful to use the train and test RMSE for the fitteed model, given a train or test dataset, and the proper response variable.


```{r}
get_rmse = function(model, data, response) {
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}
```
Interpretation: when obtaining this function, the code is better to read and it bcoms more clear which task is being reached. 


```{r}
get_rmse(model = fit_0, data = train_data, response = "sales") # train RMSE
get_rmse(model = fit_0, data = test_data, response = "sales") # test RMSE
```


### Adding Flexibilty to Linear Models

The consecutive model which are fitted will increase flexibility when obtaining interactions and polynomial terms. In the following example, a training error will be decreasing when the model increases in flexibility. It is expected that the test error will decrease a number of times, and will may be increase, as effect of the overfitting. 

```{r}
fit_1 = lm(sales ~ ., data = train_data)
get_complexity(fit_1)

get_rmse(model = fit_1, data = train_data, response = "sales") # train RMSE
get_rmse(model = fit_1, data = test_data, response = "sales") # test RMSE
```

```{r}
fit_2 = lm(sales ~ radio * newspaper * TV, data = train_data)
get_complexity(fit_2)

get_rmse(model = fit_2, data = train_data, response = "sales") # train RMSE
get_rmse(model = fit_2, data = test_data, response = "sales") # test RMSE
```

```{r}
fit_3 = lm(sales ~ radio * newspaper * TV + I(TV ^ 2), data = train_data)
get_complexity(fit_3)

get_rmse(model = fit_3, data = train_data, response = "sales") # train RMSE
get_rmse(model = fit_3, data = test_data, response = "sales") # test RMSE
```
```{r}
fit_4 = lm(sales ~ radio * newspaper * TV + 
           I(TV ^ 2) + I(radio ^ 2) + I(newspaper ^ 2), data = train_data)
get_complexity(fit_4)

get_rmse(model = fit_4, data = train_data, response = "sales") # train RMSE
get_rmse(model = fit_4, data = test_data, response = "sales") # test RMSE
```
```{r}
fit_5 = lm(sales ~ radio * newspaper * TV +
           I(TV ^ 2) * I(radio ^ 2) * I(newspaper ^ 2), data = train_data)
get_complexity(fit_5)

get_rmse(model = fit_5, data = train_data, response = "sales") # train RMSE
get_rmse(model = fit_5, data = test_data, response = "sales") # test RMSE
```

### Choosing a Model 

In order to get a better picture of the relationship between the train RMSE, test RMSE, and model complexity, results are summarized and are cluttered. 

```{r}
fit_1 = lm(sales ~ ., data = train_data)
fit_2 = lm(sales ~ radio * newspaper * TV, data = train_data)
fit_3 = lm(sales ~ radio * newspaper * TV + I(TV ^ 2), data = train_data)
fit_4 = lm(sales ~ radio * newspaper * TV + 
           I(TV ^ 2) + I(radio ^ 2) + I(newspaper ^ 2), data = train_data)
fit_5 = lm(sales ~ radio * newspaper * TV +
           I(TV ^ 2) * I(radio ^ 2) * I(newspaper ^ 2), data = train_data)
```
Interpretation: Recalling the models that have been fitted it helpful. 

```{r}
model_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)
```
Interpretation: A list of models is created

```{r}
train_rmse = sapply(model_list, get_rmse, data = train_data, response = "sales")
test_rmse = sapply(model_list, get_rmse, data = test_data, response = "sales")
model_complexity = sapply(model_list, get_complexity)
```
```{r, echo = FALSE}
# the following is the same as the apply command above

test_rmse = c(get_rmse(fit_1, test_data, "sales"),
              get_rmse(fit_2, test_data, "sales"),
              get_rmse(fit_3, test_data, "sales"),
              get_rmse(fit_4, test_data, "sales"),
              get_rmse(fit_5, test_data, "sales"))
```
Interpretation: The train RMSE, test RMSE and the model complexity are used for each. 

```{r}
plot(model_complexity, train_rmse, type = "b", 
     ylim = c(min(c(train_rmse, test_rmse)) - 0.02, 
              max(c(train_rmse, test_rmse)) + 0.02), 
     col = "dodgerblue", 
     xlab = "Model Size",
     ylab = "RMSE")
lines(model_complexity, test_rmse, type = "b", col = "darkorange")
```
Interpretation: The results are plotted. The blue line represents the train RMSE and the orange line represents the test RMSE. 


| Model   | Train RMSE        | Test RMSE        | Predictors              |
|---------|-------------------|------------------|-------------------------|
| `fit_1` | 1.6376991         |	1.7375736        | 3                       |
| `fit_2` | 0.7797226         | 1.1103716        | 7                       |
| `fit_3` | 0.4960149	        | 0.7320758	       | 8                       |
| `fit_4` | 0.488771	        | 0.7466312	       | 10                      |
| `fit_5` | 0.4705201	        | 0.8425384	       | 14                      |

Results: 
Overfitting models: A high train RMSE and a high test RMSE can be seen in `fit_1` and `fit_2`

Overfitting models: A low train RMSE and a high test RMSE can be seen in `fit_4`and `fit_5`


## Multiple Linear Regression 


The procedure of the multiple linear regression is similar to the linear regression. However, some differences exists:

- The command for the `lm` function;
- No graphical representation;
- The often tedious interpretation of the coefficients.

The model is: 

$$Y=\beta_0+\beta_1x_1 +\beta_2x_2+\ldots+\beta_px_p+e$$


The interpretation is that ßj is the average effect on Y of a one unit increase in Xj, holding all other predictors fixed. In the advertisting example, the model becomes: 


We proceed by estimating 

\[\text{sales} = \beta_0 + \beta_1\times \text{TV} + \beta_2\times \text{radio} + \beta_3\times \text{newspaper}  + \varepsilon\]


```{r}
model.mlr <- lm(sales ~ TV + radio + newspaper, data = advertising)
```

Importantly, the `+` sign does not mean that the regression is on the sum of the variables. Instead, the expression should be read "regression of sales on TV _plus on_ radio _plus on_ newspaper".

```{r}
summary(model.mlr)
```

For the interpretation of the coefficients, the correlations between the predictors is often useful.

```{r}
require(magrittr)
advertising %>% {cor(.[,c("TV", "radio", "newspaper")])} %>% round(4)
```
$$sales=\beta_0+\beta_1xTV+\beta_2xradio+\beta_3xnewspaper+e$$

Interpreting regression coefficients 

The ideal scenario is when the predictors are uncorrelated - a balanced design: 
- each coefficient can be estimated and tested separately. 
- interpretations such as "a unit change in Xj is associated with a ßj change in Y, while all the others variables stay fixed", are possible. 
Correlations amongst predictors cause problems 
- the variance of all coefficients tends to increase, sometimes dramatically
- interpretations become hazardous - when Xj changes, everything else changes. 
Claims of causality should be avoided for observational data. 

The woes of (interpreting) regression coefficients. 
"Data Analysis and Regression" Mosteller and Tukey 1977
- a regression coefficient ßj estimated the expected change in Y per unit change in Xj, will all other predictors held fixed. But predictors ususally change together! 

## Categorical regressors

The predictors of the model need not be numeric variables. They can also be factors.

```{r}
require(ISLR)
data("Credit")
str(Credit)
```

In order to understand the model better, scatter plots help to visualize each pair of variables. 

```{r, out.width = "100%", message = FALSE, warning = FALSE, include=TRUE}
require(GGally)
require(tidyverse)
require(ggplot2)
require(ggpairs)
ggpairs(Credit[,c("Balance", "Age", "Cards", "Education",
                  "Income", "Limit", "Rating")])
```

In this example it can be seen that the qualitatitive / categorical predictors, also called factor variables play a major role.   
Besides that, in the `Credit` data set, variables such `gender` or `student` are factors.  

In the following, it is visualized how these variables can be used in a linear regression.

```{r}
model.cr1 <- lm(Balance ~ Gender, data = Credit)
summary(model.cr1)
```

The expression can be evoked, that it is a smoothly process. However, a indeept analysis needs to be undrtaken regarding on fact.  
`R` has automatically created a dummy variable. This may or may not be the intended choice.  
Hence, this needs to be examined. The following call is helpful in order to do so. 

```{r}
contrasts(Credit$Gender)
```
Interpretation: the variable `GenderFemale` (read in the upper part of the table) shown in the summary of the model takes the value \(0\) if the individual is Male and \(1\) if the individual is Female.   
With multiple factors in the variable, the reading of the table must be well understood.

```{r}
contrasts(Credit$Ethnicity)
```

These values are used in the next model.

```{r}
model.cr2 <- lm(Balance ~ Ethnicity, data = Credit)
summary(model.cr2)
```

Notice that these dummy values are created in alphabetical order. Hence, the first will always server as reference.  
This behavior can be changed thanks to the `relevel` function.

```{r}
Credit$Ethnicity <- relevel(Credit$Ethnicity, ref = "Caucasian")
contrasts(Credit$Ethnicity)
```


## Interactions terms

Notice that the \(\beta\)'s represent the average effect of a one unit change in the predictor on the response.  
The assumption of a constant effect on the response, i.e., constant \(\beta_i\), is often difficult to sustain. For instance, in case of synergies of the advertising media, the effect of one particular media depends on how much of the other media are already been run.  
Interactions terms constitute a variation of the linear regression whose aim is precisely to allow for non-constant effects of variables on the response.  
The interaction between variables are built with the `:` symbol. For instance, the result in @isln, Chap. 3, slide 37 is obtained through the following call.

```{r}
model.it1 <- lm(sales ~ TV + radio + TV:radio, data = advertising)
summary(model.it1) 
## alternatively, use the cross *
lm(sales ~ TV*radio, data = advertising)
```

Interactions can be done between quantitative and categorical variables. This case is actually the very easy to interpret and even visualize, despite the multiple variables.


```{r}
model.it2 <- lm(Balance ~  Income + Student, data = Credit)
summary(model.it2) 
model.it3 <- lm(Balance ~  Income + Student + Income:Student, data = Credit)
summary(model.it3) 
y.hat4 <- predict(model.it2)
plot(Credit$Income, Credit$Balance)
lines(Credit$Income, y.hat4, col="red")
s.data <- Credit
s.data$Student <- "Yes"
n.data <- Credit
n.data$Student <- "No"
y.hat5 <- predict(model.it2, newdata = s.data)
y.hat6 <- predict(model.it2, newdata = n.data)
plot(Credit$Income, Credit$Balance)
lines(Credit$Income, y.hat5, col="red")
lines(Credit$Income, y.hat6, col="black")
Credit
```

These different models can be plotted.

```{r, out.width = "100%", message = FALSE, warning = FALSE, include=TRUE, fig.cap= "Fits of models without (left) and with (right) interactions terms of Income and Student for Students (red) and not Students (black)."}
require(gridExtra)
s.data <- Credit
s.data$Student <- "Yes"
ns.data <- Credit
ns.data$Student <- "No"
cols <- c("Student"="red", "Yes" ="red", "Not student"="black", "No"="black")
p1 <- Credit %>%
	mutate(fit.student = predict(model.it2, newdata = s.data),
		fit.not.student = predict(model.it2, newdata = ns.data)) %>%
	ggplot(aes(x=Income, y=Balance)) +
  geom_point(aes(x=Income, y=Balance, color=Student)) +
  geom_line(aes(y=fit.student, color="Student")) +
  geom_line(aes(y=fit.not.student, color="Not student")) +
  scale_colour_manual(values=cols) +
  theme(legend.position = "none")
p2 <- Credit %>%
	mutate(fit.student = predict(model.it3, newdata = s.data),
	       fit.not.student = predict(model.it3, newdata = ns.data)) %>%
  ggplot(aes(x=Income, y=Balance)) +
  geom_point(aes(x=Income, y=Balance, color=Student)) +
  geom_line(aes(y=fit.student, color="Student")) +
  geom_line(aes(y=fit.not.student, color="Not student")) +
  scale_colour_manual(values=cols) +
  theme(legend.position = c(50, 1000),
          legend.direction = "horizontal")
grid.arrange(p1, p2, ncol=2)
```





## Polynomials of degree n

Another very useful extension of the linear model is to include powers of variables in order to capture non-linear effects. This seems to be a contradiction in terms, but a possible answer could be that the model is still linear in the coefficients.  

To fix ideas, here is an example of fitting a quadratic model.

\[\text{mpg} = \beta_0 + \beta_{1}\times \text{horsepower} + \beta_{2}\times \text{horsepower}^2 + \varepsilon\]

This model can be estimated in the `Auto` data set of the `ISLR` package.

```{r}
require(ISLR)
data("Auto")
model.pd1 <- lm(mpg ~ horsepower, data = Auto)
summary(model.pd1)
model.pd2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)
summary(model.pd2) 
```
Notice the use of the `I` function which, in a formula, inhibits the interpretation of operators such as `+` and `^` as formula operators but, instead, makes them be used as arithmetical operators.  

For higher degrees of polynomial, it can become cumbersome to write all the degrees. That is where the function `poly` is handy.

```{r}
require(ISLR)
data("Auto")
model.pd5 <- lm(mpg ~ poly(horsepower, 5), data = Auto)
model.pd9 <- lm(mpg ~ poly(horsepower, 9), data = Auto)
```

Again, the advantage of the linear regression with a single predictor is the visualization of its fits, as illustrated below.

```{r, out.width = "100%", message = FALSE, warning = FALSE, include=TRUE, fig.cap= "Fits of mpg for various degrees of the polynomial of horsepower."}
Auto <- Auto %>%
	mutate(fit1 = predict(model.pd1),
	fit2 = predict(model.pd2),
	fit5 = predict(model.pd5),
	fit9 = predict(model.pd9))
cols <- c("Deg.1", "Deg.2", "Deg.5", "Deg.9")
Auto %>% 
	ggplot(aes(x=horsepower, y=mpg)) +
	geom_point() +
	geom_line(aes(y=fit1, color="Deg.1"), size =2) +
	geom_line(aes(y=fit2, color="Deg.2"), size =2) +
	geom_line(aes(y=fit5, color="Deg.5"), size =2) +
  geom_line(aes(y=fit9, color="Deg.9"), size =2) +
	theme(legend.title = element_blank(), 
		legend.position = "bottom", 
		legend.direction = "horizontal")
```




```{r}
fita1 <- lm(sales ~ TV + radio + newspaper, data=advertising )
summary(fita1)
fita2 <- lm(sales ~ poly(TV,5) + radio + newspaper , data=advertising )
summary(fita2)
fita3 <- lm(sales ~ poly(TV,5) + poly(radio,3) + poly(newspaper,6) , data=advertising )
summary(fita3)
fita4 <- lm(sales ~ TV + radio + newspaper + TV:radio, data=advertising )
summary(fita4)
fita5 <- lm(sales ~ TV*radio*newspaper, data=advertising )
summary(fita5)
fita6 <- lm(sales ~ poly(TV,2)*poly(radio,2)*poly(newspaper,2), data=advertising )
summary(fita6)
length(fita6$coefficients)
```

## Transformations 

This is a helpful tool when more complex models are created. This models are permit for non-linearity, using transformations. An inportant fact to take into account is that when using the transformations to the response variable, this will be affecting the units of said variable. Hence, in order to do the comparision to the non-transformed models, a un-transformation is helpful. 


```{r}
# define first mod_7
mod_7 = lm(sales ~ . ^ 2 + poly(TV, degree = 3), data = advertising)
# mod_7 = lm(Sales ~ . ^ 2 + I(TV ^ 2) + I(TV ^ 3), data = Advertising)
coef(mod_7)
```
```{r}
mod_8 = lm(log(sales) ~ ., data = advertising)
sqrt(mean(resid(mod_8) ^ 2)) # incorrect RMSE for Model 8
sqrt(mean(resid(mod_7) ^ 2)) # RMSE for Model 7
sqrt(mean(exp(resid(mod_8)) ^ 2)) # correct RMSE for Model 8
```


<!--chapter:end:07_Chapter_06_Introduction_to_Predictive_Statistical_Methods.Rmd-->

---
title: "Bias-Variance Tradeoff"
output: pdf_document
---
# Bias-Variance Tradeoff 

In respect to the general regression setup, where a random pair $(X, Y) \in \mathbb{R}^p \times \mathbb{R}$ is given. Here, the goal is to make a prediction of $Y$ with the funcntion of $X$, e.g. $f(X)$. 
In order to assert what it implys to make a prediction, it is useful that $f(X)$ is near to $Y$. To explain meaning of being near to, the squared error loss of estimating of $Y$ through using $f(X)$, will be definded. 

Definition of the squared error loss: 
$$
L(Y, f(X)) \triangleq (Y - f(X)) ^ 2
$$
The next step is to exlain the goal of regrssion, which is to minimize the squared error loss, on average. This can be describes as the risk if estimating $Y$ through using $f(X)$. 

$$
R(Y, f(X)) \triangleq \mathbb{E}[L(Y, f(X))] = \mathbb{E}_{X, Y}[(Y - f(X)) ^ 2]
$$

The risk is first rewrited after conditioning on $X$, before proving to minimize the risk. 

$$
\mathbb{E}_{X, Y} \left[ (Y - f(X)) ^ 2 \right] = \mathbb{E}_{X} \mathbb{E}_{Y \mid X} \left[ ( Y - f(X) ) ^ 2 \mid X = x \right]
$$

The right-hand side is easier to minimize, because it simply amounts to minimizing the inner expectation to $Y \mid X$, particularly minimizing the risk pointwise, for each $x$. 

The regression function, where the risk is minimzied by the conditional mean of $Y$ given, $X$ is written as following: 

$$
f(x) = \mathbb{E}(Y \mid X = x)
$$
An important notice is that the choice of squared error loss is slidely arbitrary. Rather, the absolute error loss can be supposed. 
$$
L(Y, f(X)) \triangleq | Y - f(X) | 
$$
The risk can then be minimzed by the conditional median. 
$$
f(x) = \text{median}(Y \mid X = x)
$$
In spite of this facility, the goal is still the squared error loss. This is because there are historical reasons, as wll as the eas of opimization and the protection against large deviations. 

The next step is, to find $\hat{f}$ that is a good estimat of the regression function $f$, given the data $\mathcal{D} = (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}$. This amounts to minimizing is called **reducible error**. 

## Reducible and Irreducible Error

Expecting that when preserving some $\hat{f}$, the question is how well does it estimate $f$? For this, the **expected prediction error** of predicting $Y$ using $\hat{f}(X)$ is definded. A good $\hat{f}$ will have a low expected prediction error. 

$$
\text{EPE}\left(Y, \hat{f}(X)\right) \triangleq \mathbb{E}_{X, Y, \mathcal{D}} \left[  \left( Y - \hat{f}(X) \right)^2 \right]
$$

This expectation is over $X$, $Y$, and also $\mathcal{D}$. The estimate $\hat{f}$ is actually random depending on the sampled data $\mathcal{D}$. Therefore, it could be actually written $\hat{f}(X, \mathcal{D})$ in order to make this dependence explicit, but the notation will become cumbrous enough as it is.

Hence, $X$ is required. This results in the expected prediction error of predicting $Y$ using $\hat{f}(X)$ when $X = x$. 

$$
\text{EPE}\left(Y, \hat{f}(x)\right) = 
\mathbb{E}_{Y \mid X, \mathcal{D}} \left[  \left(Y - \hat{f}(X) \right)^2 \mid X = x \right] = 
\underbrace{\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right]}_\textrm{reducible error} + 
\underbrace{\mathbb{V}_{Y \mid X} \left[ Y \mid X = x \right]}_\textrm{irreducible error}
$$

Here are some important things to notice: 

- The expected prediction error is for a random $Y$ given a fixed $x$ and a random $\hat{f}$. As such, the expectation is over $Y \mid X$ and $\mathcal{D}$. The estimated function $\hat{f}$ is random depending on the sampled data, $\mathcal{D}$, which is used to perform the estimation.
- The expected prediction error of predicting $Y$ using $\hat{f}(X)$ when $X = x$ has been decomposed into two errors:
    - The **reducible error**, which is the expected squared error loss of estimation $f(x)$ using $\hat{f}(x)$ at a fixed point $x$. The only thing that is random here is $\mathcal{D}$, the data used to obtain $\hat{f}$. (Both $f$ and $x$ are fixed.) This is often called reducible error the **mean squared error** of estimating $f(x)$ using $\hat{f}$ at a fixed point $x$. $$
\text{MSE}\left(f(x), \hat{f}(x)\right) \triangleq 
\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right]$$
    - The **irreducible error**. This is simply the variance of $Y$ given that $X = x$, essentially noise that is not important to learn. This is also called the **Bayes error**.

As the name suggests, the reducible error is the error that is to have some control over. But how can this error be controlled? 

## Bias-Variance Decomposition

Right after the expected predition error is decomposed into the reducible and inreducible error, the reducible error can even further be decomposed. 

Bearing the definiton of the variance of an estimator into the mind: 
$$
\text{bias}(\hat{\theta}) \triangleq \mathbb{E}\left[\hat{\theta}\right] - \theta
$$
the reducible error, which is the mean squared error can be further decomposen into bias squared and variance. 
$$
\mathbb{V}(\hat{\theta}) = \text{var}(\hat{\theta}) \triangleq \mathbb{E}\left [ ( \hat{\theta} -\mathbb{E}\left[\hat{\theta}\right] )^2 \right]
$$
Even if this is actually a common fact in estimation theory, it is mentiond at this place because the estimation of some regression function $f$ using $\hat{f}$ at some point $x$. 

$$
\text{MSE}\left(f(x), \hat{f}(x)\right) = \text{bias}^2 \left(\hat{f}(x) \right) + \text{var} \left(\hat{f}(x) \right)
$$
It can be stated that is a perfect world, it would be possible to finde some $\hat{f}$ which is unbiased, thta is bias $\text{bias}\left(\hat{f}(x) \right) = 0$ which has also a small variance. However, in the real world, this is not feasible. 

Hence, it appears that there is a **bias-variance tradeoff**. This bias-variance tradeoff is that the variance is decreasing, when the bias is increasing in the estimation. At once, increasing bias in the estimation leads to decrasing the variance. Intricate models tend to be unbiased, however, these models are  highly variable. On the other side, simple models are often very biased, but have a small variance. 

In terms of regression, it can be stated that models are biased when: 

- Parametric: The type of the model does not incorporate all the necessary varibales, of the type of the relationship is too simple. E.g. the linear relationship is assumed, but the real relationship is quadratic. 

- Non-parametric: the model present too much smoothing. 

In terms of regression, it can be stated that models are variable when: 

- Parametric: The type of the model incorporates many variables, or the type of the relationshio is too complex. E.g. the cubic relationship is assumed, but the real relationship is linear. 

- Non-parametric: the model does not present enough smoothing. The model is very shaking. 

In order to choose a model which is expected to balance the tradeoff betweeen the bias and the variance, and hence can minimize the reducible error, a model has to be choosen which provides the appropriate cimplexity for the data. 

Bearing into mind, that when fitting models, on the one hand, the train RMSE turns out the get larger as the model gets more complex. On the other hand, the test RMSE gets smaller until a certain point of model complexity, and then begins to increase. 

This is because the expected test RMSE is cruitally the expected prediction error, which is known as tp decompose into (squared) bias, variance and the irreducible Bayes error. This can be seen in the following thre plots, which are examples of the bias-variance tradeoff.

```{r, fig.height = 4, fig.width = 12, echo = FALSE}
x = seq(0.01, 0.99, length.out = 1000)

par(mfrow = c(1, 3))
par(mgp = c(1.5, 1.5, 0))
```

```{r}
b = 0.05 / x
v = 5 * x ^ 2 + 0.5
bayes = 4
epe = b + v + bayes

plot(x, b, type = "l", ylim = c(0, 10), col = "dodgerblue", lwd = 2, lty = 3,
     xlab = "Model Complexity", ylab = "Error", axes = FALSE,
     main = "More Dominant Variance")
axis(1, labels = FALSE)
axis(2, labels = FALSE)
grid()
box()
lines(x, v, col = "darkorange", lwd = 2, lty = 4)
lines(x, epe, col = "black", lwd = 2)
abline(h = bayes, lty = 2, lwd = 2, col = "darkgrey")
abline(v = x[which.min(epe)], col = "grey", lty = 3, lwd = 2)

```
Interpretation: The variance influenced the expected prediction error more than the bias. 

```{r}
b = 0.05 / x
v = 5 * x ^ 4 + 0.5
bayes = 4
epe = b + v + bayes

plot(x, b, type = "l", ylim = c(0, 10), col = "dodgerblue", lwd = 2, lty = 3,
     xlab = "Model Complexity", ylab = "Error", axes = FALSE,
     main = "Decomposition of Prediction Error")
axis(1, labels = FALSE)
axis(2, labels = FALSE)
grid()
box()
lines(x, v, col = "darkorange", lwd = 2, lty = 4)
lines(x, epe, col = "black", lwd = 2)
abline(h = bayes, lty = 2, lwd = 2, col = "darkgrey")
abline(v = x[which.min(epe)], col = "grey", lty = 3, lwd = 2)

```
Interpretation: The influence is neutral.

```{r}
b = 6 - 6 * x ^ (1 / 4)
v = 5 * x ^ 6 + 0.5
bayes = 4
epe = b + v + bayes

plot(x, b, type = "l", ylim = c(0, 10), col = "dodgerblue", lwd = 2, lty = 3,
     xlab = "Model Complexity", ylab = "Error", axes = FALSE,
     main = "More Dominant Bias")
axis(1, labels = FALSE)
axis(2, labels = FALSE)
grid()
box()
lines(x, v, col = "darkorange", lwd = 2, lty = 4)
lines(x, epe, col = "black", lwd = 2)
abline(h = bayes, lty = 2, lwd = 2, col = "darkgrey")
abline(v = x[which.min(epe)], col = "grey", lty = 3, lwd = 2)
legend("topright", c("Squared Bias", "Variance", "Bayes", "EPE"), lty = c(3, 4, 2, 1),
       col = c("dodgerblue", "darkorange", "darkgrey", "black"), lwd = 2)
```
Interpreatation: The variance influenced the bias more than the expected prediction error. 


In all three examples, the difference between the Bayer error, which is the horizontal dashed grey line, and the expected prediction, which is representet by the solid black curve, is exactly the mean squared error, which is the sum of the squared bias (blue curve) and the vairance (orange curve). The vertical line represents the complexity that minimized the prediction error. 

It is suposed that the irreducible error can be written as: 
$$
\mathbb{V}[Y \mid X = x] = \sigma ^ 2
$$
Hence, it full decomposition of the expected prediction error of predicting $Y$ using $\hat{f}$ when $X = x$ can be written as: 

$$
\text{EPE}\left(Y, \hat{f}(x)\right) =  
\underbrace{\text{bias}^2\left(\hat{f}(x)\right) + \text{var}\left(\hat{f}(x)\right)}_\textrm{reducible error} + \sigma^2.
$$
In summary it can be said that when the model complexity increeases, the bias decreases, while the variance increases. Therefore, understanding the tradeoff between bias and variance, the model complexity can be manipulated in order to find a model which predicts well on unseen observations. 

```{r, fig.height = 6, fig.width = 10, echo = FALSE}
x = seq(0, 100, by = 0.001)
f = function(x) {
  ((x - 50) / 50) ^ 2 + 2
}
g = function(x) {
  1 - ((x - 50) / 50)
}

par(mgp = c(1.5, 1.5, 0)) 
plot(x, g(x), ylim = c(0, 3), type = "l", lwd = 2,
     ylab = "Error", xlab = "",
     main = "Error versus Model Complexity", col = "darkorange", 
     axes = FALSE)
grid()
axis(1, labels = FALSE)
axis(2, labels = FALSE)
box()
ylabels = list(bquote("Low" %<-% "Complexity" %->% "High"), 
               bquote("High" %<-% "Bias" %->% "Low"),
               bquote("Low" %<-% "Variance" %->% "High"))
mtext(do.call(expression, ylabels), side = 1, line = 2:4)
curve(f, lty = 6, col = "dodgerblue", lwd = 3, add = TRUE)
legend("bottomleft", c("(Expected) Test", "Train"), lty = c(6, 1), lwd = 3,
       col = c("dodgerblue", "darkorange"))
```


## Simulation 

The decompositions, as well as the bias-variance tradeoff, can be illustrated through simulation. 
Assuming that a train model should learn the true regression function $f(x) = x^2$.

```{r}
f = function(x) {
  x ^ 2
}
```

In particular, an observation $Y$ should be predicted, given $X = x$ by using $\hat{f}(x)$ where

$$
\mathbb{E}[Y \mid X = x] = f(x) = x^2
$$
and

$$
\mathbb{V}[Y \mid X = x] = \sigma ^ 2.
$$

Alternatively, this can be written as

$$
Y = f(X) + \epsilon
$$

where $\mathbb{E}[\epsilon] = 0$ and $\mathbb{V}[\epsilon] = \sigma ^ 2$. In this formulation, $f(X)$ is called the **signal** and $\epsilon$ the **noise**.

In order to extradite a specific simulation example, the data genaerating process need to be fully specfied: 

```{r}
get_sim_data = function(f, sample_size = 100) {
  x = runif(n = sample_size, min = 0, max = 1)
  y = rnorm(n = sample_size, mean = f(x), sd = 0.3)
  data.frame(x, y)
}
```

Note: If it is prefered to think if this simulation using the $Y = f(X) + \epsilon$ formulation, the following code represents the same data generating process.

```{r}
get_sim_data = function(f, sample_size = 100) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = 0.75)
  y = f(x) + eps
  data.frame(x, y)
}
```


In order to completely specify the data generating process, more model assumptions has to be made than simply $\mathbb{E}[Y \mid X = x] = x^2$ and $\mathbb{V}[Y \mid X = x] = \sigma ^ 2$. In particular,

- The $x_i$ in $\mathcal{D}$ are sampled from a uniform distribution over $[0, 1]$.
- The $x_i$ and $\epsilon$ are independent.
- The $y_i$ in $\mathcal{D}$ are sampled from the conditional normal distribution.

$$
Y \mid X \sim N(f(x), \sigma^2)
$$

```{r, echo = FALSE}
# TODO: colors like this...
# \color{blue}{\texttt{predict(fit0, x)}}
# trick is getting it to render in both html and pdf
```

For obtaining this setup, the datasets $\mathcal{D}$ will be generated with a sample size  $n = 100$ and fit four models. 

$$
\begin{aligned}
\texttt{predict(fit0, x)} &= \hat{f}_0(x) = \hat{\beta}_0\\
\texttt{predict(fit1, x)} &= \hat{f}_1(x) = \hat{\beta}_0 + \hat{\beta}_1 x \\
\texttt{predict(fit2, x)} &= \hat{f}_2(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 \\
\texttt{predict(fit9, x)} &= \hat{f}_9(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \ldots + \hat{\beta}_9 x^9
\end{aligned}
$$
For making use of the data and the four models, a simulated dataset is generated, and fit the four models. 

```{r}
set.seed(1)
sim_data = get_sim_data(f)
```

```{r}
fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)
```



```{r, fig.height = 6, fig.width = 9, echo = FALSE}
set.seed(42)
plot(y ~ x, data = sim_data, col = "grey", pch = 20,
     main = "Four Polynomial Models fit to a Simulated Dataset")
grid()
grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, f(grid), col = "black", lwd = 3)
lines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = "dodgerblue",  lwd = 2, lty = 2)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = "firebrick",   lwd = 2, lty = 3)
lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = "springgreen", lwd = 2, lty = 4)
lines(grid, predict(fit_9, newdata = data.frame(x = grid)), col = "darkorange",  lwd = 2, lty = 5)

legend("topleft", 
       c("y ~ 1", "y ~ poly(x, 1)", "y ~ poly(x, 2)",  "y ~ poly(x, 9)", "truth"), 
       col = c("dodgerblue", "firebrick", "springgreen", "darkorange", "black"), lty = c(2, 3, 4, 5, 1), lwd = 2)
```
Interpretation: When plotting the four trained models, it can be seen that the zero predictor models does very bad. The first degree mdeol is reasonabale, but it can be seen that second degree model fits much better. The ninth model seem rather wild. 

When staying to the three plots which are created when using three further simulated datasets. The zero predictor and nith degree ploynomial were fit to each. 

```{r, fig.height = 4, fig.width = 12, echo = FALSE}
par(mfrow = c(1, 3))

# if you're reading this code
# it's BAD! don't use it. (or clean it up)
# also, note to self: clean up this code!!!

set.seed(430)
sim_data_1 = get_sim_data(f)
sim_data_2 = get_sim_data(f)
sim_data_3 = get_sim_data(f)
fit_0_1 = lm(y ~ 1, data = sim_data_1)
fit_0_2 = lm(y ~ 1, data = sim_data_2)
fit_0_3 = lm(y ~ 1, data = sim_data_3)
fit_9_1 = lm(y ~ poly(x, degree = 9), data = sim_data_1)
fit_9_2 = lm(y ~ poly(x, degree = 9), data = sim_data_2)
fit_9_3 = lm(y ~ poly(x, degree = 9), data = sim_data_3)

```

```{r}
plot(y ~ x, data = sim_data_1, col = "grey", pch = 20, main = "Simulated Dataset 1")
grid()
grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, predict(fit_0_1, newdata = data.frame(x = grid)), col = "dodgerblue", lwd = 2, lty = 2)
lines(grid, predict(fit_9_1, newdata = data.frame(x = grid)), col = "darkorange", lwd = 2, lty = 5)
legend("topleft", c("y ~ 1", "y ~ poly(x, 9)"), col = c("dodgerblue", "darkorange"), lty = c(2, 5), lwd = 2)

```
```{r}
plot(y ~ x, data = sim_data_2, col = "grey", pch = 20, main = "Simulated Dataset 2")
grid()
grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, predict(fit_0_2, newdata = data.frame(x = grid)), col = "dodgerblue", lwd = 2, lty = 2)
lines(grid, predict(fit_9_2, newdata = data.frame(x = grid)), col = "darkorange", lwd = 2, lty = 5)
legend("topleft", c("y ~ 1", "y ~ poly(x, 9)"), col = c("dodgerblue", "darkorange"), lty = c(2, 5), lwd = 2)
```
```{r}
plot(y ~ x, data = sim_data_3, col = "grey", pch = 20, main = "Simulated Dataset 3")
grid()
grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, predict(fit_0_3, newdata = data.frame(x = grid)), col = "dodgerblue", lwd = 2, lty = 2)
lines(grid, predict(fit_9_3, newdata = data.frame(x = grid)), col = "darkorange", lwd = 2, lty = 5)
legend("topleft", c("y ~ 1", "y ~ poly(x, 9)"), col = c("dodgerblue", "darkorange"), lty = c(2, 5), lwd = 2)
```

Interpretation: The plots make straighten out the difference between the bias and variance of these two models. The zero predictor model is clearly wrong, that is, biased, but nearly the same for each of the datasets, since it has very low variance.

While the ninth degree model does not appear to be correct for any of these three simulations, it can be seen that on average it is, and thus is performing unbiased estimation. These plots do however clearly illustrate that the ninth degree polynomial is extremely variable. Each dataset results in a very different fitted model. Correct on average is not the only goal that after, since in practice, only a single dataset is used. This is why also the models like to exhibit low variance.

In this case, it can be seen that when $k$ = 100, it is a biased model with very low variance. When $k$ = 5, it is again a highly variable model. 

These two sets of plots reinforce the intuition about the bias-variance tradeoff. Complex models (ninth degree polynomial and $k$ = 5) are highly variable, and often unbiased. Simple models (zero predictor linear model and $k = 100$) are very biased, but have extremely low variance.



<!--chapter:end:08_Chapter_07_Bias_Variance_Tradeoff.Rmd-->

---
title: "09_Ersatz"
author: "CTrierweiler, PGaulke"
date: "24 Juli 2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(amsmath)
```


# Classification 

Classification is also a form of supervised learning. Here, the response variable is categorical, as opposed to numeric for regression. The goal is to find a rule, algorithm, or a function which takes as input a feature vector, and outputs a category which is the true category as often as possible. (David Dalpiaz)

That is, the classifier $\hat{C}(x)$ returns the predicted category $\hat{y}(X)$.

$$\hat{y}(x) = \hat{C}(x)$$

- Qualitative variables take values in an unordered set C, such as email {spam, ham}. 
- Given a feature vector X and a qualitative response Y taking values in the set C, the classification task is to build a function C(X) that takes as input the feature vector X and predicts value; i.e. C(X)E C. 
- Often we are more interested in estimating the probabilities that X belongs to each category in C. 

For example, it is more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification fraudulent or not. 


In order to build the first classifier, the Default dataset from the ISLR package is used. 

```{r}
library(ISLR)
library(tibble)
as_tibble(Default)
```
The goal is to decently classify individuals as defaulters based on student status, credit card balance, and income. 
Note: The response default is the factor, as is the predictor student. 

```{r}
is.factor(Default$default)
is.factor(Default$student)
```
As done previous chaper regression, the data is splitted into test and train. In this example, 50 % each are used. 

```{r}
set.seed(42)
default_idx   = sample(nrow(Default), 5000)
default_trn = Default[default_idx, ]
default_tst = Default[-default_idx, ]
```


## Classification Visualization 

Simple classification rules can be used for simple visualizations. In order to create effective visualizations, the function featurePlot () from the package caret () is used. 

```{r, message = FALSE, warning = FALSE}
library(caret)
```
Based on a numerica predictor, a density plot can often suggest a simple split. Essentially this plot graphs a density estimate

$$\hat{f}_{X_i}(x_i \mid Y = k)$$

for each numeric predictor $x_i$ and each category $k$ of the response $y$.

```{r, fig.height = 5, fig.width = 10}
featurePlot(x = default_trn[, c("balance", "income")], 
            y = default_trn$default,
            plot = "density", 
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(2, 1), 
            auto.key = list(columns = 2))
```

Some notes about the arguments to this function according to David Dalpiaz:

- `x` is a data frame containing only numeric predictors. It would be nonsensical to estimate a density for a categorical predictor.
- `y` is the response variable. It needs to be a factor variable. If coded as `0` and `1`, you will need to coerce to factor for plotting.
- `plot` specifies the type of plot, here `density`.
- `scales` defines the scale of the axes for each plot. By default, the axis of each plot would be the same, which often is not useful, so the arguments here, a different axis for each plot, will almost always be used.
- `adjust` specifies the amount of smoothing used for the density estimate.
- `pch` specifies the plot character used for the bottom of the plot.
- `layout` places the individual plots into rows and columns. For some odd reason, it is given as (col, row).
- `auto.key` defines the key at the top of the plot. The number of columns should be the number of categories.

It can be seems that the income variable by itself is not peculiarly effective. However, there seems to be a big difference in default status at a `balance` of about 1400. This information will be used shortly.

```{r, fig.height = 5, fig.width = 10, message = FALSE, warning = FALSE}
featurePlot(x = default_trn[, c("balance", "income")], 
            y = default_trn$student,
            plot = "density", 
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(2, 1), 
            auto.key = list(columns = 2))
```

A similar plot is created, except with `student` as the response. It can be seen that students often carry a slightly larger balance, and have far lower income. This will be useful to know when making more complicated classifiers.

```{r, fig.height = 6, fig.width = 6, message = FALSE, warning = FALSE}
featurePlot(x = default_trn[, c("student", "balance", "income")], 
            y = default_trn$default, 
            plot = "pairs",
            auto.key = list(columns = 2))
```

`plot = "pairs"` can be used to consider multiple variables at the same time. This plot reinforces using `balance` to create a classifier, and again shows that `income` seems not that useful.

```{r, fig.height = 6, fig.width = 6, message = FALSE, warning = FALSE}
library(ellipse)
featurePlot(x = default_trn[, c("balance", "income")], 
            y = default_trn$default, 
            plot = "ellipse",
            auto.key = list(columns = 2))
```

Similar to `pairs` is a plot of type `ellipse`, which requires the `ellipse` package. Here, only numeric predictors are used, as essentially,  multivariate normality is assumed. The ellipses mark points of equal density. 

Example: Credit Card Default 

```{r}
show.index <- sample(1:nrow(Default), 1000)

plot(Default$balance[show.index], 
Default$income[show.index], col =
Default$default[show.index])

boxplot(Default$balance ~ Default$default)
```

## Can we use Linear Regression? 

Supposing for the Default classification task that it is coded 


$${Y} = 
\begin{cases} 
      0 & if \ no \\
      1 & if \ yes
\end{cases}$$


Can a simple linear regresssion of Y on X can be performed and classify as Yes if $\hat{Y} > 0.5$? 

- In this case of a binary outcome, linear regression does a good job as a classifier, and is equivalent to linear discriminat analysis which is discussed in a later. 
- Since in the population 
$$\mathbb{E}[Y \mid X = x] = P(Y = 1 \mid X = x).$$
it might be thinking that regression is perfect for this task. 
- However, linaer regression might produce probabilities less than zero or bigger than one. Logistic regression is more appropriate. 

## Linear versus Logistic Regression 

```{r}
default_trn_lm = default_trn
default_tst_lm = default_tst
```
```{r}
default_trn_lm$default = as.numeric(default_trn_lm$default) - 1
default_tst_lm$default = as.numeric(default_tst_lm$default) - 1
```

```{r}
model_lm = lm(default ~ balance, data = default_trn_lm)
```

```{r, fig.height=5, fig.width=7}
plot(default ~ balance, data = default_trn_lm, 
     col = "darkorange", pch = "|", ylim = c(-0.2, 1),
     main = "Using Linear Regression for Classification")
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
abline(model_lm, lwd = 3, col = "dodgerblue")
```
Linear regression does not estimate $P(Y = 1 \mid X = x)$.
The graph of linear regression shows that the predicted probabilities are below 0.5., indicating that every observation would be classified as `"No" This could be possible, but it is not what is expected. 

```{r}
all(predict(model_lm) < 0.5)
```
A further issue is that the predicted probabilty is less than 0. 
```{r}
any(predict(model_lm) < 0)
```
## Logistic regression 

$$p(x) = P(Y = 1 \mid {X = x})$$

```{r}
model_glm = glm(default ~ balance, data = default_trn, family = "binomial")
```
```{r}
coef(model_glm)
```

```{r}
head(predict(model_glm))
```
```{r}
head(predict(model_glm, type = "link"))
```
```{r}
head(predict(model_glm, type = "response"))
```

```{r}
calc_class_err = function(actual, predicted) {
  mean(actual != predicted)
}
```

```{r}
#calc_class_err(actual = default_trn$default, predicted = model_glm_pred)
```
Logistic regression is used to better estimate the propability.

The model is 

$$\log\left(\frac{p(x)}{1 - p(x)}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p.$$
```{r, fig.height=5, fig.width=7}
plot(default ~ balance, data = default_trn_lm, 
     col = "darkorange", pch = "|", ylim = c(-0.2, 1),
     main = "Using Logistic Regression for Classification")
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
curve(predict(model_glm, data.frame(balance = x), type = "response"), 
      add = TRUE, lwd = 3, col = "dodgerblue")
abline(v = -coef(model_glm)[1] / coef(model_glm)[2], lwd = 2)
```
In logistic regression it suited well to the task. 

This plot contains a wealth of information.

- The orange `|` characters are the data, $(x_i, y_i)$.
- The blue "curve" is the predicted probabilities given by the fitted logistic regression. That is,
$$\hat{p}(x) = \hat{P}(Y = 1 \mid { X = x})$$
- The solid vertical black line represents the decision boundary the `balance` that obtains a predicted probability of 0.5. In this case `balance` = `r -coef(model_glm)[1] / coef(model_glm)[2]`.


## Receiver Operating Characteristics Curve

In data science for business, performance management is a fundamental task. In a classification problem, the AUC (Area Under The Curve) and ROC (Receiver Operating Characteristics) can be counted. The AUC-ROC curve can be used when it is helpful to check or visualize the performance of classification problems. This method is one of the most useful evaluation metrics for checking any classification model's performance. 


In order to introduce this method, the simple model with only blance as a predictor is used. 

```{r}
model_glm = glm(default ~ balance, data = default_trn, family = "binomial")
```


The first step is to write the function in such a way that it is possible to make predictions beased on different cutoffs. 

```{r}
get_logistic_pred = function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = "response")
  ifelse(probs > cut, pos, neg)
}
```


$$
\hat{C}(x) = 
\begin{cases} 
      1 & \hat{p}(x) > c \\
      0 & \hat{p}(x) \leq c 
\end{cases}
$$

The second step will be to receive predictions using a low (0.1), medium (0.5) and high(0.9) cutoff. 

```{r}
test_pred_10 = get_logistic_pred(model_glm, data = default_tst, res = "default", 
                                 pos = "Yes", neg = "No", cut = 0.1)
test_pred_50 = get_logistic_pred(model_glm, data = default_tst, res = "default", 
                                 pos = "Yes", neg = "No", cut = 0.5)
test_pred_90 = get_logistic_pred(model_glm, data = default_tst, res = "default", 
                                 pos = "Yes", neg = "No", cut = 0.9)
```

The thrid step will be that the accurarcy, sensitivity and specificity for these classifiers are evaluated. 



```{r}
test_tab_10 = table(predicted = test_pred_10, actual = default_tst$default)
test_tab_50 = table(predicted = test_pred_50, actual = default_tst$default)
test_tab_90 = table(predicted = test_pred_90, actual = default_tst$default)

test_con_mat_10 = confusionMatrix(test_tab_10, positive = "Yes")
test_con_mat_50 = confusionMatrix(test_tab_50, positive = "Yes")
test_con_mat_90 = confusionMatrix(test_tab_90, positive = "Yes")
```

```{r}
metrics = rbind(
  
  c(test_con_mat_10$overall["Accuracy"], 
    test_con_mat_10$byClass["Sensitivity"], 
    test_con_mat_10$byClass["Specificity"]),
  
  c(test_con_mat_50$overall["Accuracy"], 
    test_con_mat_50$byClass["Sensitivity"], 
    test_con_mat_50$byClass["Specificity"]),
  
  c(test_con_mat_90$overall["Accuracy"], 
    test_con_mat_90$byClass["Sensitivity"], 
    test_con_mat_90$byClass["Specificity"])

)

rownames(metrics) = c("c = 0.10", "c = 0.50", "c = 0.90")
metrics
```
Interpretation: It can be seen that the sensitivity gets lower as the cutoff gets higher. Reversely the specificity gets higher as the cutoff gets higher. This is a very helpful fact to know when it would be interesting to get information in a particular error, instead of giving them equal weight. 

An important notice is that usually the best accurary will be near c = 0.5

However, creating the receiver operating characteristics cure which will sweep through all possible cutoffs and plots the sensitivity and specificity will be more effective than manually checking the cutoffs. 

```{r}
library(pROC)
test_prob = predict(model_glm, newdata = default_tst, type = "response")
test_roc = roc(default_tst$default ~ test_prob, plot = TRUE, print.auc = TRUE)
```

```{r}
as.numeric(test_roc$auc)
```
Interpretation: A good model will have a high AUC, that is as often as possible a high sensitivity and specificity.

<!--chapter:end:09_Chapter_08_Classifaction.Rmd-->

---
title: "k-Nearest Neighbors"
output: pdf_document
---
# k-Nearest Neighbors


In this chapter, the first non-parametric classification method, $k$-nearest neighbors, is introduced. These methods consider locality: 

$$
\hat{f}(x) = \text{average}(\{ y_i : x_i = x \})
$$


The other methods for classificaiton seen so far, have been parametric. These approaches assume the type of: 

$$
f(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p
$$

## K-Nearest-Neighbors in Regression

Examples for K-Nearest Neighbors in R 

```{r}
require(FNN)
require(MASS)
data(Boston)
```

```{r}
set.seed(42)
boston_idx = sample(1:nrow(Boston), size = 250)
trn_boston = Boston[boston_idx, ]
tst_boston  = Boston[-boston_idx, ]
```

```{r}
X_trn_boston = trn_boston["lstat"]
X_tst_boston = tst_boston["lstat"]
y_trn_boston = trn_boston["medv"]
y_tst_boston = tst_boston["medv"]
```

Interpretation: Creating an additional "test" set `lstat_grid`, that is a grid of `lstat` values at which will predict `medv` in order to create graphics.

```{r}
X_trn_boston_min = min(X_trn_boston)
X_trn_boston_max = max(X_trn_boston)
lstat_grid = data.frame(lstat = seq(X_trn_boston_min, X_trn_boston_max, 
                                    by = 0.01))
```

In order to perform KNN for regression, `knn.reg()` from the `FNN` package is needed. 

INPUT

- `train`: the predictors of the training data
- `test`: the predictor values, $x$, at which predictions should be made
- `y`: the response for the training data
- `k`: the number of neighbors to consider

OUTPUT

- the output of `knn.reg()` is exactly $\hat{f}_k(x)$

```{r}
pred_001 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 1)
pred_005 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 5)
pred_010 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 10)
pred_050 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 50)
pred_100 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 100)
pred_250 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 250)
```

Predictions are made for a large number of possible values of `lstat`, for different values of `k`. 

An important notice is that 250 is the total number of observations in this training dataset.

```{r}
par(mfrow = c(3, 2))

plot(medv ~ lstat, data = trn_boston, cex = .8, col = "dodgerblue", main = "k = 1")
lines(lstat_grid$lstat, pred_001$pred, col = "darkorange", lwd = 0.25)

plot(medv ~ lstat, data = trn_boston, cex = .8, col = "dodgerblue", main = "k = 5")
lines(lstat_grid$lstat, pred_005$pred, col = "darkorange", lwd = 0.75)

plot(medv ~ lstat, data = trn_boston, cex = .8, col = "dodgerblue", main = "k = 10")
lines(lstat_grid$lstat, pred_010$pred, col = "darkorange", lwd = 1)

plot(medv ~ lstat, data = trn_boston, cex = .8, col = "dodgerblue", main = "k = 25")
lines(lstat_grid$lstat, pred_050$pred, col = "darkorange", lwd = 1.5)

plot(medv ~ lstat, data = trn_boston, cex = .8, col = "dodgerblue", main = "k = 50")
lines(lstat_grid$lstat, pred_100$pred, col = "darkorange", lwd = 2)

plot(medv ~ lstat, data = trn_boston, cex = .8, col = "dodgerblue", main = "k = 250")
lines(lstat_grid$lstat, pred_250$pred, col = "darkorange", lwd = 2)
```
Interpretation: the orange curves are $\hat{f}_k(x)$ where $x$ are the values which have been defined in `lstat_grid`. Hence, there is a large number of predictions with interpolated lines, but this does not tell anything. 

It can be see that k = 1 is clearly overfitting, as k = 1 is a very complex, highly variable model. Conversely, k = 250 is clearly underfitting the data, as k = 250 is a very simple, low variance model. In fact, here it is predicting a simple average of all the data at each point.


An important thing to know is, how to choose k 


It can be stated that: 
- low `k` = very complex model. very wiggly. specifically jagged
- high `k` = very inflexible model. very smooth

- want: something in the middle which predicts well on unseen data
- that is, want $\hat{f}_k$ to minimize

$$
\text{EPE}\left(Y, \hat{f}_k(X)\right) = 
\mathbb{E}_{X, Y, \mathcal{D}} \left[  (Y - \hat{f}_k(X))^2 \right]
$$

Therefore, needs to test if MSE is an estimate of this. Hence finding best test RMSE will be the strategy. (Best test RMSE is same as best MSE, but with more understandable units.)

```{r}
rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
```

```{r}
# define helper function for getting knn.reg predictions
# note: this function is highly specific to this situation and dataset
make_knn_pred = function(k = 1, training, predicting) {
  pred = FNN::knn.reg(train = training["lstat"], 
                      test = predicting["lstat"], 
                      y = training$medv, k = k)$pred
  act  = predicting$medv
  rmse(predicted = pred, actual = act)
}
```

```{r}
# define values of k to evaluate
k = c(1, 5, 10, 25, 50, 250)
```

```{r}
# get requested train RMSEs
knn_trn_rmse = sapply(k, make_knn_pred, 
                      training = trn_boston, 
                      predicting = trn_boston)
# get requested test RMSEs
knn_tst_rmse = sapply(k, make_knn_pred, 
                      training = trn_boston, 
                      predicting = tst_boston)

# determine "best" k
best_k = k[which.min(knn_tst_rmse)]

# find overfitting, underfitting, and "best"" k
fit_status = ifelse(k < best_k, "Over", ifelse(k == best_k, "Best", "Under"))
```

```{r}
# summarize results
knn_results = data.frame(
  k,
  round(knn_trn_rmse, 2),
  round(knn_tst_rmse, 2),
  fit_status
)
colnames(knn_results) = c("k", "Train RMSE", "Test RMSE", "Fit?")

# display results
knitr::kable(knn_results, escape = FALSE, booktabs = TRUE)
```

The next step will be to find out about the ties? The question is why isn't k = 1 give 0 training error? There are some non-unique $x_i$ values in the training data. Therefore, a further question evokes: How to predict when this is the case?


### Linear versus Non-Linear

Summarize: 
in the linear relationship example
    - lm() works well
    - knn "automatically" approximates
    
in the very non-linear example
    - lm() fails badly
        - could work if ...
    - knn "automatically" approximates
    
```{r}
line_reg_fun = function(x) {
  x
}

quad_reg_fun = function(x) {
  x ^ 2
}

sine_reg_fun = function(x) {
  sin(x)
}

get_sim_data = function(f, sample_size = 100, sd = 1) {
  x = runif(n = sample_size, min = -5, max = 5)
  y = rnorm(n = sample_size, mean = f(x), sd = sd)
  data.frame(x, y)
}

set.seed(42)
line_data = get_sim_data(f = line_reg_fun)
quad_data = get_sim_data(f = quad_reg_fun, sd = 2)
sine_data = get_sim_data(f = sine_reg_fun, sd = 0.5)

x_grid = data.frame(x = seq(-5, 5, by = 0.01))

par(mfrow = c(1, 3))
plot(y ~ x, data = line_data, pch = 1, col = "darkgrey")
grid()
knn_pred = FNN::knn.reg(train = line_data$x, test = x_grid, y = line_data$y, k = 10)$pred
fit = lm(y ~ x, data = line_data)
lines(x_grid$x, line_reg_fun(x_grid$x), lwd = 2)
lines(x_grid$x, knn_pred, col = "darkorange", lwd = 2)
abline(fit, col = "dodgerblue", lwd = 2, lty = 3)

plot(y ~ x, data = quad_data, pch = 1, col = "darkgrey")
grid()
knn_pred = FNN::knn.reg(train = quad_data$x, test = x_grid, y = quad_data$y, k = 10)$pred
fit = lm(y ~ x, data = quad_data)
lines(x_grid$x, quad_reg_fun(x_grid$x), lwd = 2)
lines(x_grid$x, knn_pred, col = "darkorange", lwd = 2)
abline(fit, col = "dodgerblue", lwd = 2, lty = 3)

plot(y ~ x, data = sine_data, pch = 1, col = "darkgrey")
grid()
knn_pred = FNN::knn.reg(train = sine_data$x, test = x_grid, y = sine_data$y, k = 10)$pred
fit = lm(y ~ x, data = sine_data)
lines(x_grid$x, sine_reg_fun(x_grid$x), lwd = 2)
lines(x_grid$x, knn_pred, col = "darkorange", lwd = 2)
abline(fit, col = "dodgerblue", lwd = 2, lty = 3)



# k was reasonably well chosen
# this is a reasonable amount of data
# this is a rather low dimensional problem

# could fix with: y ~ poly(x, degree = 2)
# could fix with: y ~ sin(x)
# both would have better edge behavior
```

    

### Scaling Data

The next step would scale the data. This is helpful becaus sometimes be "scale" differentiates between center and scale. `R` function `scale()` does both by default. Outputs variables with mean = 0, var = 1.

```{r}
sim_knn_data = function(n_obs = 50) {
  x1 = seq(0, 10, length.out = n_obs)
  x2 = runif(n = n_obs, min = 0, max = 2)
  x3 = runif(n = n_obs, min = 0, max = 1)
  x4 = runif(n = n_obs, min = 0, max = 5)
  x5 = runif(n = n_obs, min = 0, max = 5)
  y = x1 ^ 2 + rnorm(n = n_obs)
  data.frame(y, x1, x2, x3,x4, x5)
}
```

```{r}
set.seed(42)
knn_data = sim_knn_data()
```

```{r}
par(mfrow = c(1, 2))
orange_blue = c(rep("grey", 45), rep("darkorange", 5))
plot(x1 ~ x2, data = knn_data, xlim = c(-2, 2), ylim = c(-2, 10), pch = 20, col = orange_blue)
points(1.7, 10)
plot(scale(x1) ~ scale(x2), data = knn_data, xlim = c(-2, 2), ylim = c(-2, 10), pch = 20, col = orange_blue)
points((1.7 - 1.197685) / 0.6072974, (10 - 5) / 2.974975)
```


## K-Nearest Neighbors in Classification

First, taking a few things into mind again:

For example, logistic regression had the form

$$
\log\left(\frac{p(x)}{1 - p(x)}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p.
$$

In this case, the $\beta_j$ are the parameters of the model, which have been learned (estimated) by training (fitting) the model. Those estimates were then used to obtain estimates of the probability $p(x) = P(Y = 1 \mid X = x)$,

$$
\hat{p}(x) = \frac{e^{\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdots  + \hat{\beta}_p x_p}}{1 + e^{\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdots  + \hat{\beta}_p x_p}}
$$

As already seen in the chapter of regression, $k$-nearest neighbors has no such model parameters. 
Instead, it has a so called tuning parameter, $k$. This is a parameter which determines how the model is trained, instead of a parameter that is learned through training. 
An important notice is that tuning parameters are not used exclusively with non-parametric methods. There are also cases where there are tuning parameters for parametric methods.

Often when discussing $k$-nearest neighbors for classification, it is framed as a black-box method that directly returns classifications. Instead, here it is framed as a non-parametric model for the probabilites $p_g(x) = P(Y = g \mid X = x)$. That is a $k$-nearest neighbors model using $k$ neighbors estimates this probability as

$$
\hat{p}_{kg}(x) = \hat{P}_k(Y = g \mid X = x) = \frac{1}{k} \sum_{i \in \mathcal{N}_k(x, \mathcal{D})} I(y_i = g)
$$

Essentially, the probability of each class $g$ is the proportion of the $k$ neighbors of $x$ with that class, $g$.

In next step is to create a classifier becuase it is important to simply classify the class with the highest estimated probability.

$$
\hat{C}_k(x) =  \underset{g}{\mathrm{argmax}} \ \ \hat{p}_{kg}(x)
$$

This is similar to classifying to the class with the most observations in the $k$ nearest neighbors. If more than one class is tied for the highest estimated probablity, simply assigning a class at random to one of the classes tied for highest.

In the binary case this becomes

$$
\hat{C}_k(x) = 
\begin{cases} 
      1 & \hat{p}_{k0}(x) > 0.5 \\
      0 & \hat{p}_{k1}(x) < 0.5
\end{cases}
$$

Again, if the probability for class `0` and `1` are equal, simply assign at random.

```{r, echo = FALSE}
set.seed(42)
knn_ex = tibble::tibble(
  x1 = 1:10,
  x2 = sample(1:10, size = 10, replace = TRUE),
  class = sample(c("darkorange", "dodgerblue"), size = 10, replace = TRUE))
plot(x2 ~ x1, col = class, data = knn_ex,
     ylim = c(0, 10), xlim = c(0, 10), pch = 20, cex = 1.5)
points(8, 6, col = "darkgrey", pch = "x")
plotrix::draw.circle(8, 6, 2.6, nv = 1000, lty = 1, lwd = 2, border = "darkgrey")
legend("bottomleft", c("O", "B"), pch = c(20, 20), col = c("darkorange", "dodgerblue"))
```

In the case above, when predicting at $x = (x_1, x_2) = (8, 6)$,

$$
\hat{p}_{5B}(x_1 = 8, x_2 = 6) = \hat{P}_5(Y = \text{Blue} \mid X_1 = 8, X_2 = 6) = \frac{3}{5}
$$

$$
\hat{p}_{5O}(x_1 = 8, x_2 = 6) = \hat{P}_5(Y = \text{Orange} \mid X_1 = 8, X_2 = 6) = \frac{2}{5}
$$

Thus

$$
\hat{C}_5(x_1 = 8, x_2 = 6) = \text{Blue}
$$


## Binary Data Example

```{r}
library(ISLR)
library(class)
```

Here it is necessary to load some libraries. 

The first step will be discussing $k$-nearest neighbors for classification by returning to the `Default` data from the `ISLR` package. To perform $k$-nearest neighbors for classification, the `knn()` function from the `class` package is used.

In contrast, for example to the logistic regression, the function `knn()` requires that all predictors be numeric, so we coerce `student` to be a `0` and `1` dummy variable instead of a factor. 

Important thing to consider is also, the response should be left as a factor. Numeric predictors are required because of the distance calculations taking place.

```{r}
set.seed(42)
Default$student = as.numeric(Default$student) - 1
default_idx = sample(nrow(Default), 5000)
default_trn = Default[default_idx, ]
default_tst = Default[-default_idx, ]
```

As already seen in the `knn.reg` form the `FNN` package for regression, `knn()` from `class` does not utilize the formula syntax, rather, requires the predictors be their own data frame or matrix, and the class labels be a separate factor variable. 

An important notice is that the `y` data should be a factor vector, not a data frame including a factor vector. 

Furthermore, it is useful that the `FNN` package also contains a `knn()` function for classification.  
Choose `knn()` from `class` as it seems to be much more popular. However, being aware of which packages needs to be loaded and thus which functions will be used. They are very similar, but have some differences.

```{r}
# training data
X_default_trn = default_trn[, -1]
y_default_trn = default_trn$default

# testing data
X_default_tst = default_tst[, -1]
y_default_tst = default_tst$default
```

Consistently, there is very small "training" with $k$-nearest neighbors. Essentially the only training is to simply remember the inputs. Because of this, it can be said that $k$-nearest neighbors is fast at training time. However, at test time, $k$-nearest neighbors is very slow. For each test observation, the method must find the $k$-nearest neighbors, which is not computationally cheap. An important point to notice is that by deafult, `knn()` uses Euclidean distance to determine neighbors.

```{r}
head(knn(train = X_default_trn, 
         test  = X_default_tst, 
         cl    = y_default_trn, 
         k     = 3))
```

Because of the lack of any need for training, the `knn()` function immediately returns classifications. With logistic regression, we needed to use `glm()` to fit the model, then `predict()` to obtain probabilities that could use to make a classifier. Here, the `knn()` function directly returns classifications. That is `knn()` is essentially $\hat{C}_k(x)$.

Here, `knn()` takes four arguments:

- `train`, the predictors for the train set.
- `test`, the predictors for the test set. `knn()` will output results (classifications) for these cases.
- `cl`, the true class labels for the train set.
- `k`, the number of neighbors to consider.

```{r}
calc_class_err = function(actual, predicted) {
  mean(actual != predicted)
}
```

In this case, the `calc_class_err()` function is used to asses how well `knn()` works with this data. 

The test data is used to evaluate.

```{r}
calc_class_err(actual    = y_default_tst,
               predicted = knn(train = X_default_trn,
                               test  = X_default_tst,
                               cl    = y_default_trn,
                               k     = 5))
```

Often with `knn()` the scale of the predictors variables need to be considered. If one variable is contains much higher numbers because of the units or range of the variable, it will dominate other variables in the distance measurements. But this does not essentially mean that it should be such an important variable. It is common practice to scale the predictors to have a mean of zero and unit variance. Be sure to apply the scaling to both the train and test data.

```{r}
calc_class_err(actual    = y_default_tst,
               predicted = knn(train = scale(X_default_trn), 
                               test  = scale(X_default_tst), 
                               cl    = y_default_trn, 
                               k     = 5))
```
Interpretation: the scaling slightly improves the classification accuracy. This may not always be the case, and often, it is normal to attempt classification with and without scaling.

How can $k$ be choosen? One way is to try different values and see which works best.

```{r}
set.seed(42)
k_to_try = 1:100
err_k = rep(x = 0, times = length(k_to_try))

for (i in seq_along(k_to_try)) {
  pred = knn(train = scale(X_default_trn), 
             test  = scale(X_default_tst), 
             cl    = y_default_trn, 
             k     = k_to_try[i])
  err_k[i] = calc_class_err(y_default_tst, pred)
}
```

The `seq_along()` function can be very helpful for looping over a vector that stores non-consecutive numbers. It often removes the need for an additional counter variable. Actually it is not need it in the above `knn()` example, but it is still a good habit. For example in some cases trying out every value of $k$ is not wanted, but only odd integers, which would prevent ties. Or maybe, just checking multiples of 5 to further cut down on computation time would be a good way.

A further notice to make is that a set seed before running this loops is needed. Because when considering even values of $k$, thus, there are ties which are randomly broken.

Therefore, plotting the $k$-nearest neighbor results.

```{r, fig.height = 6, fig.width = 8}
# plot error vs choice of k
plot(err_k, type = "b", col = "dodgerblue", cex = 1, pch = 20, 
     xlab = "k, number of neighbors", ylab = "classification error",
     main = "(Test) Error Rate vs Neighbors")
# add line for min error seen
abline(h = min(err_k), col = "darkorange", lty = 3)
# add line for minority prevalence in test set
abline(h = mean(y_default_tst == "Yes"), col = "grey", lty = 2)
```
Interpretation: the dotted line represents the smallest observed test classification error rate.

```{r}
min(err_k)
```

```{r}
which(err_k == min(err_k))
```
Interpretation: it can be seen that five different values of $k$ are tied for the lowest error rate. 

Given a choice of these five values of $k$, the largest will be selected, as it is the least variable, and has the least chance of overfitting.

```{r}
max(which(err_k == min(err_k)))
```

Recalling that defaulters are the minority class. That is, the majority of observations are non-defaulters.

```{r}
table(y_default_tst)
```

Notice: As $k$ gets larger, eventually the error approaches the test prevalence of the minority class.

```{r}
mean(y_default_tst == "Yes")
```


## Categorical Data

As already seen in LDA and QDA, KNN, they can be used for both binary and multi-class problems. As an example of a multi-class problems, returning to the `iris` data.

```{r}
set.seed(430)
iris_obs = nrow(iris)
iris_idx = sample(iris_obs, size = trunc(0.50 * iris_obs))
iris_trn = iris[iris_idx, ]
iris_tst = iris[-iris_idx, ]
```

All the predictors here are numeric, therefore it will be proceed to splitting the data into predictors and classes.

```{r}
# training data
X_iris_trn = iris_trn[, -5]
y_iris_trn = iris_trn$Species

# testing data
X_iris_tst = iris_tst[, -5]
y_iris_tst = iris_tst$Species
```

As already seen in previous methods, predicted probabilities given test predictors can be obtained. 
In order to do so, the argument, `prob = TRUE` is added.

```{r}
iris_pred = knn(train = scale(X_iris_trn), 
                test  = scale(X_iris_tst),
                cl    = y_iris_trn,
                k     = 10,
                prob  = TRUE)
```

```{r}
head(iris_pred, n = 50)
```

Unfortunately, this only returns the predicted probability of the most common class. In the binary case, this would be sufficient to recover all probabilities, however, for multi-class problems, it can not recover each of the probabilities of interest. This will simply be a minor annoyance for now, which will be fixed when introducing the `caret` package for model training.

```{r}
head(attributes(iris_pred)$prob, n = 50)
```

<!--chapter:end:10_Chapter_09_k_NearestNeighbors.Rmd-->

---
title: "10_Ersatz"
author: "CTrierweiler,PGaulke"
date: "24 Juli 2019"
output: html_document
---
# Cross-validation and the Bootstrap

Cross-validation and the bootstrap are two methods of resampling. These two methods refit a model of interest to samples created from the training set, for the reason to obtain additional information about the fitted model. The methods provide estimates of test-set prediction error, and the standard deviation and bias of the parameter estimates. 

## Training Error versus Test error

Here it is useful to recall the distinction between the test error and the training error. 
- Test error: average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method. 
- Training error: can be easily calculated by applying the statistical learning method to the observations used in its training. 
- Error rate: the training error rate can dramatically underestimate the test error rate. 


## Validation-Set Approach 

In the validation-set approach, the available set of samples is divided into two parts: A training set and a validation or hold-out set. 
The model is fit on the training set, and the fitted model is used to predict the reponse for the observations in the validation set. 
The resulting validation-set error provides an estimate of the test error. This is typically assessed using MSE in the case of a quantitative reponse and misclassification rate in the case of a qualitative (discrete) reponse. 


Example 1. (with explanations)

In the automobile data example, linear vs. higher-order polynomial terms in a linear regression are compared. The 392 observations are splited into two sets, a training set containing 196 of the data points, and a validation set containing the remaining 196 observations. 

```{r}
# a function for calculating the RMSE from two vectors

c.rmse <- function(observed, predicted){
  (observed - predicted)^2%>%
  mean %>%
  sqrt %>%
  round(3)
}

c.rmse2 <- function(observed, predicted) {
round(sqrt(mean((observed -predicted)^2)),3)
}

```

```{r}
require(ISLR)
require(magrittr)
#to load the required packages

set.seed(43245)
#in order to create random numbers, but to save this "seed" and not create new random numbers chunks are runned again (as done if put rnorm(41) instead of set.seed

#in order to have our training data seperated, we need to half it

n <- nrow(Auto) 
# just to have an abbreviation

train <- sample(1:n, ceiling(n/2))
#1: to number of rows, ceiling is used to prevent that in case nrow(auto) is odd, you have a number such as 74,3 (also could use round)

degrees<- 1:10
#the different degrees wanted to put in

v.rmse <- numeric ()
#to create a new vector where all values are putted in from the rmse

for (i in degrees){
#basically just creating an abbreviation for putting in several polynomals into the fit1
  
    
fit1 <- glm(mpg ~ poly(horsepower,i), data = Auto, subset = train)
  v.rmse[i] <-
# fit in into a linear model, in order to create a line that fits the model    
v.rmse[i] <- c.rmse(Auto$mpg[-train], predict(fit1, newdata=Auto[-train,]))  
    
# how it was before, against what it is now with v.rmse:c.rmse(Auto$mpg[-train], predict(fit1, newdata=Auto[-train,]))
#here function is created in order to calculate later the rmse

}
# the plot is created to see all the test error values for the different polys (the number after horsepower)
  

plot(degrees, v.rmse, type ="b", col = "red")   


#type b just shows the type of the line ( can also be l for line or p for points instead of b for both)


```
As a result degree 2 is probably taken, because it is quite good from its v.rmse and it is not complex (the lower the degree, the better is it to understand)

In the next step, is is done not just for one split, but multiple splits:

```{r}

require(ISLR)
require(magrittr)
#to load the required packages

set.seed(120)


degrees <- 1:10

n.splits <- 10

m.rmse <- matrix(NA, length(degrees), n.splits)
#here NA is the data(numbers), length = number of rows, n.splits = number columns

library(ISLR)

for(s in 1:n.splits){
  train <- sample(1:n, ceiling(n/2))
for(i in degrees) {
  fit1<- glm(mpg ~ poly (horsepower, i), data = Auto, subset = train)
m.rmse[i,s] <- c.rmse(Auto$mpg[-train], predict(fit1, newdata = Auto[-train,]))

}
}
  
plot(degrees, m.rmse[,1], type ="l", col = "red", ylim=c(min(m.rmse), max(m.rmse)))
for (s in 1:n.splits){
  lines(degrees, m.rmse[,s], col =s)
}

```


Example 2. 

- Consider fitting polynomial models of degree k = 1:10 data from this data generating process
- Consider k, the polynomial degree, as a turning parameter how well validation set approach works. 

```{r}
num_sims = 100
num_degrees = 10
val_rmse = matrix(0, ncol = num_degrees, nrow = num_sims)
```


```{r}
gen_sim_data = function(sample_size) {
  x = runif(n = sample_size, min = -1, max = 1)
  y = rnorm(n = sample_size, mean = x ^ 3, sd = 0.25)
  data.frame(x, y)
}

set.seed(42)
sim_data = gen_sim_data(sample_size=200)
sim_idx = sample(1:nrow(sim_data),160)
sim_trn = sim_data[sim_idx,]
sim_val = sim_data[-sim_idx,]

```

```{r}
calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
```
```{r}
fit = lm(y ~ poly(x, 10), data = sim_trn)

calc_rmse(actual = sim_trn$y, predicted = predict(fit, sim_trn))
calc_rmse(actual = sim_val$y, predicted = predict(fit, sim_val))
```

```{r}

library(ISLR)


fit = lm(y ~ poly(x, 10), data = sim_trn)

calc_rmse(actual = sim_trn$y, predicted = predict(fit, sim_trn))
calc_rmse(actual = sim_val$y, predicted = predict(fit, sim_val))
```

The simulations are: 

```{r} 
set.seed(42)
for (i in 1:num_sims) {
  # simulate data
  sim_data = gen_sim_data(sample_size = 200)
  # set aside validation set
  sim_idx = sample(1:nrow(sim_data), 160)
  sim_trn = sim_data[sim_idx, ]
  sim_val = sim_data[-sim_idx, ]
  # fit models and store RMSEs
  for (j in 1:num_degrees) {
    #fit model
    fit = glm(y ~ poly(x, degree = j), data = sim_trn)
    # calculate error
    val_rmse[i, j] = calc_rmse(actual = sim_val$y, predicted = predict(fit, sim_val))
  }
}
```
```{r echo = FALSE, fig.height = 5, fig.width = 10}
par(mfrow = c(1, 2))
matplot(t(val_rmse)[, 1:10], pch = 20, type = "b", ylim = c(0.17, 0.35), xlab = "Polynomial Degree", ylab = "RMSE", main = "RMSE vs Degree")
barcol = c("grey", "grey", "dodgerblue", "grey", "grey", "grey", "grey", "grey", "grey", "grey")
barplot(table(factor(apply(val_rmse, 1, which.min), levels = 1:10)),
        ylab = "Times Chosen", xlab = "Polynomial Degree", col = barcol, main = "Model Chosen vs Degree")
```

## Drawbacks of validation set approach 

The validation estimate of the test error can be highly variable, depending on precisely which observations are included in the training set and which observations can be included in the validation set. 
In the validation approach, only a subset of the observations - those that are included in the training set rather than in the validation set - are used to fit the model. 
This suggestes that the validation set error may tend to overestimate the test error for the model fit on the entire data set. 

## K-fold Cross validation 

This is a widely used approach for estimating the test error. The estimtates can be used to select the optimal model and to give an idea of the test error and the final chosen model. The idea is to randomly divide the data into K equal-sized parts. The k part is left out, fit the model to the other predictions for the left-out kth part. This appears through in turn for ach part k = 1, 2,...K, and then the results are combined. 


1          2     3     4     5 
Validation Train Train Train Train

## The Bootstrap 

The bootstrap is another resampling method. It is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method. E.g. it is usesful for providing an estimate of the standard error of a coefficient, or a confidence interval for that coefficient. The bootstrap could be used to replace the cross-validation method, however it aligns significantly more computation. 


Interesting side fact to know about the origin to the name "bootstrap"

It is derived from the sentence "to pull oneself up by one's bootstrap" which was mainly used to be based on one of the eighteenth century "the surprising adventures of Baron Munchausen" by Rudolph Erich Raspe: 

The Baron had fallen in the bottom of the deep lake. Just when it looked like all was lost, he thought to pick himself up by his own bootstrap. 

Hence, it is another meaning of "bootstrap" is computer science. Here to "boot" a computer from a set of core instructions, though the deviation is similar. 


Example 

```{r}
# Bootstrap 95% CI for R-Squared
library(boot)
# function to obtain R-Squared from the data 
rsq <- function(formula, data, indices) {
  d <- data[indices,] # allows boot to select sample 
  fit <- lm(formula, data=d)
  return(summary(fit)$r.square)
} 
# bootstrapping with 1000 replications 
results <- boot(data=mtcars, statistic=rsq, 
   R=1000, formula=mpg~wt+disp)

# view results
results 
plot(results)

# get 95% confidence interval 
boot.ci(results, type="bca")
```
Interpretation: the function rsq returned a number and boot.ci returned a single confidence interval. The statistics function which is provided can also return a vector. 


In the following example 95% Confidence Interval are getting for the three model regression coefficients (intercept, car weight, displacement). In this case, though adding an index parameter to plot( ) and boot.ci( ) to indicate which column in bootobject$t is to analyzed.

```{r}
# Bootstrap 95% CI for regression coefficients 
library(boot)
# function to obtain regression weights 
bs <- function(formula, data, indices) {
  d <- data[indices,] # allows boot to select sample 
  fit <- lm(formula, data=d)
  return(coef(fit)) 
} 
# bootstrapping with 1000 replications 
results <- boot(data=mtcars, statistic=bs, 
   R=1000, formula=mpg~wt+disp)

# view results
results
plot(results, index=1) # intercept 
plot(results, index=2) # wt 
plot(results, index=3) # disp 

# get 95% confidence intervals 
boot.ci(results, type="bca", index=1) # intercept 
boot.ci(results, type="bca", index=2) # wt 
boot.ci(results, type="bca", index=3) # disp

```
Example from statmethods.net

<!--chapter:end:11_Chapter_10_Cross-Validation_and_the_Bootstrap.Rmd-->

---
title: "11_Ersatz"
author: "CTrierweiler,PGaulke"
date: "24 Juli 2019"
output: html_document
---
# Tree-based methods 

In this chapter, tree-based methods for regression and classification are discussed. These include stratifying or segmenting the predictor space into a number of single regions. Since the set of splitting ruls used to segment the predictor space can be summarized in a tree, these type pf approaches are known as decision tree methods. 

## Pro and Cons of Trees

One the one hand, tree-based methods are simple and useful for interpretation. On the other hand, they are typically not competitive with the best supervised learning approaches in terms of prediction accuracy. Further methods are bagging, random forest, and boosting, which grow multiple trees which are then combined to yield a single consensus prediction. Combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss interpretation. 

## The Basics of Decision Trees 

Decision trees can be used to regression and classification problems. In this chaper, the regression problems are considered first and second the classification problems 

## Example

Baseball salaray data: how to stratify it?


```{r}
require(ggplot2)

data("Hitters")

Hitters  %>%
  ggplot(aes(x=Years, y=Hits, col=Salary)) +
  geom_point()
```
The salary level is demonstrated in the shaded from low (dark blue) to high (light blue)

## Decision tree for these data 

```{r}
library(rpart)

b.tree <- rpart(Salary ~ Years + Hits, data = Hitters)

min.of.cp <- b.tree$cptable[which.min(b.tree$cptable[,"xerror"]),"CP"]

pruned.b.tree <- prune(b.tree, cp = min.of.cp)
plot(pruned.b.tree)
text(pruned.b.tree, pretty = 0)

```
Details of the previous figure (Decision tree)
For the hitters data, a regression tree for predicting the log salary of a baseball player, based on the number of years that he has played in the major leagues and the number of hits that he made in the previous year. At a given internal node, the label (of the form Xj < tk) indicating that the left-hand branch emanating from that split, and the right-hand branch corresponds to Xj >- tk=. For example, the left-hand branc corresponse to years < 4.5, and the right-hand branch corresponds to years >= 4.5 
The tree has two internal nodes and three terminal nodes, or leaved. The number in each leaf is the mean of the response for the observations that fall there. 

## Terminology for Trees 

- In keeping with the tree analogy, the region R1, R2, R3 are known as terminal nodes. 
- Decision treers are typically drawn upside down, in the sense that the leaves are at the bottom of the tree.
- The points along the tree where the predictor space is split are referred to as internal nodes. 
- In the hitters tree, the two internal nodes are indicated by the text Years < 4.5 and Hits < 117.5. 

## Interpretation of Results 

- Years is the less important factor in determining Salary, and players with less experience earn lower salaries than more experienced players. 
- Given that a player is less experienced, the number of Hits that he made in the previous year seems to play little role in his Salary. 
- But among players who have been in the major leagues for five or more years, the number of Hits made in the previous year does affect Salary, and players who made more Hits last year tend to have higher salaries. 
- Surely an over-simplification, but compared to a regression model, it is easy to display, interpret and explain. 

## Pruning a tree 

A small tree with fewer sploits (that is, fewer regions R1,...Rj) might lead to lower variance and better interpretations at the cost of a little bias. A possible alternative is to grow a tree only so long as the decreas in the RSS due to each split exceeds some (high) threshold. This will in smaller trees, but is too short-sighted: a seemingly worthless split early on in the tree might be followed by a very good split - that is, a split that leads to a large reduction in RSS later on. 
A better startegy is to grow a very large tree T0, and then prune is back in order to obtain a subtree. Cost complexity pruning - also known as weakest link pruneing - is used to do this. 


## Choosing the best subtree

A trade-off betwen the subtree's complexity and its fit to the training data is controlled by the tuning parameter alpha. The optimal alpha is selecting by using the cross-validation. After that, there is a return to the full data set and obtaining the subtree corresponding to alpha.  

## Summary: tree algorithm 

1. Using recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations. 
2. Applying cost complexity pruning to the large tree in order to obtain a sequence of besr subtrees, as a function of alpha. 
3. Using K-fold cross-validation to choose alpha. For each k = 1, ..., K: 
3.1 Repeating step 1 and 2 on the K-1/Kth fraction of the training data, excluding the kth fold.
3.2 Evaluating the mean squared prediction error on the data in the left-out kth fold, as a function of alpha.
Averaging the results, and picking alpha tp minimize the average error. 
4. Returning the subtree from Step 2 that correspond to the chosen value of alpha. 


## Classification Trees

The classification trees are similar to the regression trees. The difference is that the classification trees are used to predict that every observation belongs to the most commonly occuring class of training obervations in the region to which it belongs. 

## Details of classification Trees

As already used in the regression setting, recursive binary splitting are used to grow a classification tree. In the classification setting, RSS cannnot be used as a criterion for making the binary splits. A natural alternative to the RSS is the classification error rate. This is simply the fraction of the training observation in that region that do not belong to the most common class. 

E = 1 - max(^pmk)/k 

Note: ^pmk represents the proportion of training observations in the mth region that are from the kth class. However, classification errror is not sufficiently sensitive for tree-growing, and in practive two other measures are preferable (Gini Index and Deviance)

## Advantages and Disadvantages of Trees

There are four advanatages and one disadvantage of trees. 

The first advantage is that trees are perfect to explain people. 
The second advantage is that decision trees can be seen as more closely mirror human decision-making than do the regression and classification approaches. 
The third advantage is that trees can be displayed graphically and can be easily interpretated, even by a non-expert. 
The forth advanatge is that tree can easily handle qualitative predictors without the need to create dummy variabls. 
One disadvantage is that trees have not the same level of predictive accurarcy in general, as some of the other regression and classification approaches. 



## Bagging 

Bagging is one way to fix the over-fitting of trees. It is a general-purpose procedure for the reduction of variance of statistical learning method. Bagging is a useful and frequently method used in the context to decision trees. Bagging is a special form of random forest where `mtry` which is equal to p, the number of predictors. 

Example 

The goal is now to fit a bagged model, by using the package `randomForest`. 


```{r, message = FALSE, warning = FALSE}
require(randomForest)
require(MASS)

boston_idx = sample(1:nrow(Boston),nrow(Boston)/2)
boston_trn = Boston [boston_idx,]
boston_tst = Boston [-boston_idx,]

boston_bag = randomForest(medv ~ ., data = boston_trn, mtry = 13, 
                          importance = TRUE, ntrees = 500)
boston_bag
```

```{r}
boston_bag_tst_pred = predict(boston_bag, newdata = boston_tst)
plot(boston_bag_tst_pred,boston_tst$medv,
     xlab = "Predicted", ylab = "Actual",
     main = "Predicted vs Actual: Bagged Model, Test Data",
     col = "dodgerblue", pch = 20)
grid()
abline(0, 1, col = "darkorange", lwd = 2)
```

```{r}
(bag_tst_rmse = calc_rmse(boston_bag_tst_pred, boston_tst$medv))
```
Interpratation: Two interesting results can be seen. 

- The first interesting result is that the predicted vs actual plot has no longer a small number of predicted valued. 
- The second interesting result is that the test error has dropped immemsely. 
Note: the Mean of squared residuals, which is the outbut by the `randomForest`is the Oit of Bag estimate of the error. 

```{r}
plot(boston_bag, col = "dodgerblue", lwd = 2, main = "Bagged Trees: Error vs Number of Trees")
grid()
```

## Random Forest 

Random forests provide an improvement over bagged trees by way of small tweak that decorrelates the trees. Hence, this reduces the variance when averaging the trees. Further, as already seen in bagging, here a number of decision trees are build on bootstrapping training samples. However, when decision trees are build, every time a split in a tree is considered, a random selection of m predictors is chosen as split candidates from the full set of p predictors. The split is allowed to use only one of those m predictors. 

Note: Now a random forest is tried. For regression, the suggestion is to use `mtry` equal to $p/3$. 
```{r}
boston_forest = randomForest(medv ~ ., data = boston_trn, mtry = 4, 
                             importance = TRUE, ntrees = 500)
boston_forest
```

```{r}
importance(boston_forest, type = 1)
varImpPlot(boston_forest, type = 1)
```

```{r}
boston_forest_tst_pred = predict(boston_forest, newdata = boston_tst)
plot(boston_forest_tst_pred, boston_tst$medv,
     xlab = "Predicted", ylab = "Actual",
     main = "Predicted vs Actual: Random Forest, Test Data",
     col = "dodgerblue", pch = 20)
grid()
abline(0, 1, col = "darkorange", lwd = 2)
```

```{r}
(forest_tst_rmse = calc_rmse(boston_forest_tst_pred, boston_tst$medv))
boston_forest_trn_pred = predict(boston_forest, newdata = boston_trn)
forest_trn_rmse = calc_rmse(boston_forest_trn_pred, boston_trn$medv)
forest_oob_rmse = calc_rmse(boston_forest$predicted, boston_trn$medv)
```

Interpretation: Here are three RMSEs noted. The training RMSE, which is optimistic and the OOB RMSE which is a reasonable estimate of the test erro and the test RMSE. Further, the variables importance was calculated. 


```{r, echo = FALSE}
(forst_errors = data.frame(
  Data = c("Training", "OOB", "Test"),
  Error = c(forest_trn_rmse, forest_oob_rmse, forest_tst_rmse)
  )
)
```


## Boosting 

Similar to bagging, boosting is a general approach which can be applied to many methods in statistical learning for regression or classification. When recalling that bagging involves creating multiple copies of the orginal training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to each copy, and then combining all of the trees in order to create a single predictive model. Every tree is built on a bootstrap data set, independent of the other trees. 
Here, booting runs in a similar way, except that the trees are grown sequentially, meaning that each tree is grown using information from previously grown trees. 

Example 

In this example, it is tried to boost a model, which by default will produce a nice variable importance plot as well as plots of marginal effects of the predictors. The package `gbm` is used. 

```{r}
library(gbm)
```

```{r, fig.height = 6, fig.width = 8, message = FALSE, warning = FALSE}
booston_boost = gbm(medv ~ ., data = boston_trn, distribution = "gaussian", 
                    n.trees = 5000, interaction.depth = 4, shrinkage = 0.01)
booston_boost
```
```{r, fig.height = 8, fig.width = 8, message = FALSE, warning = FALSE}
tibble::as_tibble(summary(booston_boost))
```

```{r, fig.height = 5, fig.width = 12, message = FALSE, warning = FALSE}
par(mfrow = c(1, 3))
plot(booston_boost, i = "rm", col = "dodgerblue", lwd = 2)
plot(booston_boost, i = "lstat", col = "dodgerblue", lwd = 2)
plot(booston_boost, i = "dis", col = "dodgerblue", lwd = 2)
```

```{r}
boston_boost_tst_pred = predict(booston_boost, newdata = boston_tst, n.trees = 5000)
(boost_tst_rmse = calc_rmse(boston_boost_tst_pred, boston_tst$medv))
```

```{r}
plot(boston_boost_tst_pred, boston_tst$medv,
     xlab = "Predicted", ylab = "Actual", 
     main = "Predicted vs Actual: Boosted Model, Test Data",
     col = "dodgerblue", pch = 20)
grid()
abline(0, 1, col = "darkorange", lwd = 2)
```
 
 
## Summary 

Decision trees can be used for regression and classification when they are simple and interpretable. However, decision tres are often not competitive with other methods in terms of prediction accuracy. Further, bagging, random forest and boosting are good methods for imporving the prediction accuracy of trees. They work by growing many trees on the training data and then combining the predictions of the resulting ensemble of trees. 
Random forests and boosting are among the state-of-the-art methods for supervised learning. Howeverm their results can be difficult to predict. 

<!--chapter:end:12_Chapter_11_Trees.Rmd-->

---
title: "13_Solution"
date: "26 Juli 2019"
output: html_document
---

# (PART) Final Project - Task {-}

# Preparing for a Prediction on test.data

To get prepared (this is repetitively down for every applied method in order to avoid messing up the data):

```{r}
require(tidyverse)
require(ISLR)
require(magrittr)
require(caret)
require(tree)
require(randomForest)


load("C:/Users/admin/Dropbox/Master/2. Semester/Data Science/MyBook/project_data.Rdata")
summary(train.data)
summary(test.data)

# train.data %>% is.na %>% rowSums %>% table

# 11 variables for frequency of seven plants
# task: The test.data has the same structure but does not contain the frequencies for each of the 7 plants. Your
# goal is precisely to estimate them for the 140 observations
```

```{r, eval=FALSE}
# variables:character to numeric / this might not be needed, but could be
# for train.data
train.data$season = as.numeric(c("spring" = "1", "summer" = "2", "autumn" = "3", "winter" = "4")[train.data$season])

train.data$size = as.numeric(c("small" = "1", "medium" = "2", "large" = "3")[train.data$size])

train.data$speed = as.numeric(c("low" = "1", "medium" = "2", "high" = "3")[train.data$speed])
# the higher, the later. the higher, the larger. the higher, faster.

# for test.data

test.data$season = as.numeric(c("spring" = "1", "summer" = "2", "autumn" = "3", "winter" = "4")[test.data$season])

test.data$size = as.numeric(c("small" = "1", "medium" = "2", "large" = "3")[test.data$size])

test.data$speed = as.numeric(c("low" = "1", "medium" = "2", "high" = "3")[test.data$speed])


```

```{r}
# need to take care of na's

# options: delete na or replace with the mean

# delete na (worse choice)
#is.na(train.data)
#na.omit(train.data)

# replace missing value with the mean (and avoid a loop!)

train.data$mxPH[is.na(train.data$mxPH)] <- round(mean(train.data$mxPH, na.rm = TRUE))
train.data$mnO2[is.na(train.data$mnO2)] <- round(mean(train.data$mnO2, na.rm = TRUE))
train.data$Cl[is.na(train.data$Cl)] <- round(mean(train.data$Cl, na.rm = TRUE))
train.data$NO3[is.na(train.data$NO3)] <- round(mean(train.data$NO3, na.rm = TRUE))
train.data$NH4[is.na(train.data$NH4)] <- round(mean(train.data$NH4, na.rm = TRUE))
train.data$oPO4[is.na(train.data$oPO4)] <- round(mean(train.data$oPO4, na.rm = TRUE))
train.data$PO4[is.na(train.data$PO4)] <- round(mean(train.data$PO4, na.rm = TRUE))
train.data$Chla[is.na(train.data$Chla)] <- round(mean(train.data$Chla, na.rm = TRUE))

# the same with the test.data
test.data$mxPH[is.na(test.data$mxPH)] <- round(mean(test.data$mxPH, na.rm = TRUE))
test.data$mnO2[is.na(test.data$mnO2)] <- round(mean(test.data$mnO2, na.rm = TRUE))
test.data$Cl[is.na(test.data$Cl)] <- round(mean(test.data$Cl, na.rm = TRUE))
test.data$NO3[is.na(test.data$NO3)] <- round(mean(test.data$NO3, na.rm = TRUE))
test.data$NH4[is.na(test.data$NH4)] <- round(mean(test.data$NH4, na.rm = TRUE))
test.data$oPO4[is.na(test.data$oPO4)] <- round(mean(test.data$oPO4, na.rm = TRUE))
test.data$PO4[is.na(test.data$PO4)] <- round(mean(test.data$PO4, na.rm = TRUE))
test.data$Chla[is.na(test.data$Chla)] <- round(mean(test.data$Chla, na.rm = TRUE))

```

This is however also repetitively down which is actually not necessary, or even misleading. However, this is only repetivey as long as the document is WIP. When finished, the other chunks for the creation of testing data will `eval=FALSE`

```{r}
#create test data
set.seed(200)
num_obs = nrow(train.data)

train.index = sample(num_obs, size = trunc(0.50 * num_obs))
newtrain.data = train.data[train_index, ]
traintest.data = train.data[-train_index, ]


# replace again missing valuesith the mean (and avoid a loop!)

newtrain.data$mxPH[is.na(newtrain.data$mxPH)] <- round(mean(newtrain.data$mxPH, na.rm = TRUE))
newtrain.data$mnO2[is.na(newtrain.data$mnO2)] <- round(mean(newtrain.data$mnO2, na.rm = TRUE))
newtrain.data$Cl[is.na(newtrain.data$Cl)] <- round(mean(newtrain.data$Cl, na.rm = TRUE))
newtrain.data$NO3[is.na(newtrain.data$NO3)] <- round(mean(newtrain.data$NO3, na.rm = TRUE))
newtrain.data$NH4[is.na(newtrain.data$NH4)] <- round(mean(newtrain.data$NH4, na.rm = TRUE))
newtrain.data$oPO4[is.na(newtrain.data$oPO4)] <- round(mean(newtrain.data$oPO4, na.rm = TRUE))
newtrain.data$PO4[is.na(newtrain.data$PO4)] <- round(mean(newtrain.data$PO4, na.rm = TRUE))
newtrain.data$Chla[is.na(newtrain.data$Chla)] <- round(mean(newtrain.data$Chla, na.rm = TRUE))

# to to the same with the new test.data
traintest.data$mxPH[is.na(traintest.data$mxPH)] <- round(mean(traintest.data$mxPH, na.rm = TRUE))
traintest.data$mnO2[is.na(traintest.data$mnO2)] <- round(mean(traintest.data$mnO2, na.rm = TRUE))
traintest.data$Cl[is.na(traintest.data$Cl)] <- round(mean(traintest.data$Cl, na.rm = TRUE))
traintest.data$NO3[is.na(traintest.data$NO3)] <- round(mean(traintest.data$NO3, na.rm = TRUE))
traintest.data$NH4[is.na(traintest.data$NH4)] <- round(mean(traintest.data$NH4, na.rm = TRUE))
traintest.data$oPO4[is.na(traintest.data$oPO4)] <- round(mean(traintest.data$oPO4, na.rm = TRUE))
traintest.data$PO4[is.na(traintest.data$PO4)] <- round(mean(traintest.data$PO4, na.rm = TRUE))
traintest.data$Chla[is.na(traintest.data$Chla)] <- round(mean(traintest.data$Chla, na.rm = TRUE))
summary(newtrain.data)
summary(traintest.data)
```





<!--chapter:end:13_Final_Project.Rmd-->

---
title: "12a_Solution_a1"
author: "CTrierweiler,PGaulke"
date: "28 Juli 2019"
output: html_document
---

# Develop the Model for a1  

Methods that will be tested:

- Multiple Linear Regression
- Linear regression with polynomials
- Log Linear Regression
- Trees
- RandomForest

Logistic regression is not applicable on this data set as the response is not binary.

## Multiple Linear Regression

Determining the signifance level wanted: <0,05
```{r}
# Test it

a1.testmlrmod <- lm(a1 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=newtrain.data)
summary(a1.testmlrmod)

# NO3 with highest significance

a1.testmlrmod$coefficients["NO3"]

a1.test.pred.mlr <- (predict(a1.testmlrmod,traintest.data))
 
a1.RMSE.testmlr <- sqrt(mean((a1.test.pred.mlr-traintest.data$a1)^2))
a1.RMSE.testmlr

a1.MAE.testmlr <- mean(abs(a1.test.pred.mlr-traintest.data$a1))
a1.MAE.testmlr

# Reducing the variables
a1.testmlrmod2 <- lm(a1 ~ size + NO3 + PO4, data=newtrain.data)

summary(a1.testmlrmod2)
a1.testmlrmod2$coefficients["PO4"]

a1.test.pred.mlr2 <- (predict(a1.testmlrmod2,traintest.data))
 
a1.RMSE.testmlr2 <- sqrt(mean((a1.test.pred.mlr2-traintest.data$a1)^2))
a1.RMSE.testmlr2

a1.MAE.testmlr2 <- mean(abs(a1.test.pred.mlr2-traintest.data$a1))
a1.MAE.testmlr2

# Reducing the variables to only one variable (SLR)

a1.testmlrmod3 <- lm(a1 ~ PO4, data=newtrain.data)

summary(a1.testmlrmod3)

a1.test.pred.mlr3 <- (predict(a1.testmlrmod3,traintest.data))
 
a1.RMSE.testmlr3 <- sqrt(mean((a1.test.pred.mlr3-traintest.data$a1)^2))
a1.RMSE.testmlr3

a1.MAE.testmlr3 <- mean(abs(a1.test.pred.mlr3-traintest.data$a1))
a1.MAE.testmlr3

# MLR better, trying to reduce to two
a1.testmlrmod4 <- lm(a1 ~ size + PO4, data=newtrain.data)

a1.test.pred.mlr4 <- (predict(a1.testmlrmod4,traintest.data))
 
a1.RMSE.testmlr4 <- sqrt(mean((a1.test.pred.mlr4-traintest.data$a1)^2))
a1.RMSE.testmlr4

a1.MAE.testmlr4 <- mean(abs(a1.test.pred.mlr4-traintest.data$a1))
a1.MAE.testmlr4

# a1-testmlrmod4 as best choice

```

Trying k-fold cross validation

```{r, error=TRUE}
require(caret)
set.seed(200) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
a1.crossmodelmlr <- train(a1 ~ size + PO4, data = train.data, method = "lm",
               trControl = train.control)
# Summarize the results
print(a1.crossmodelmlr)
```


```{r, eval=FALSE}
# playing around for understanding the process
a1pred <- predict(mlrmod2, traintest.data)

actuals_pred <- data.frame(cbind(actuals=newtrain.data$a1, predicteds=a1pred))

correlation_accuracy<- cor(actuals_pred)

head(actuals_pred)
```


## Polynomials

Adding polynomials

```{r}

# test it from mlr4

a1.testpoly <- glm(a1 ~ size + PO4, data=newtrain.data)
summary(a1.testpoly)
# use of poly

a1.testpolymod1 <- glm(a1 ~ poly(as.numeric(size),2) + poly(PO4,2), data=newtrain.data)
summary(a1.testpolymod1)

a1.test.pred.poly <- predict(a1.testpolymod1,traintest.data) 
 
a1.RMSE.polymod <- sqrt(mean((a1.test.pred.poly-traintest.data$a1)^2))
a1.RMSE.polymod

a1.MAE.polymod <- mean(abs(a1.test.pred.poly-traintest.data$a1))
a1.MAE.polymod

# checking for higher polyys

a1.testpolymod2 <- glm(a1 ~ poly(as.numeric(size),2) + poly(PO4,3), data=newtrain.data)
summary(a1.testpolymod2)

a1.test.pred.poly2 <- predict(a1.testpolymod2,traintest.data) 
 
a1.RMSE.polymod2 <- sqrt(mean((a1.test.pred.poly2-traintest.data$a1)^2))
a1.RMSE.polymod2

a1.MAE.polymod2 <- mean(abs(a1.test.pred.poly2-traintest.data$a1))
a1.MAE.polymod2

# Worse, a1.testpolymod1 best choice, less polys?

a1.testpolymod3 <- glm(a1 ~ poly(PO4,3), data=newtrain.data)
summary(a1.testpolymod3)

a1.test.pred.poly3 <- predict(a1.testpolymod3,traintest.data) 
 
a1.RMSE.polymod3 <- sqrt(mean((a1.test.pred.poly3-traintest.data$a1)^2))
a1.RMSE.polymod3

a1.MAE.polymod3 <- mean(abs(a1.test.pred.poly3-traintest.data$a1))
a1.MAE.polymod3

# worse

```


```{r}
## plot the rmse
models.rmse <- tibble(
            model = paste0("model.pd",c(1,2,3)),
						RMSE= c(
							c.rmse(traintest.data$a1,predict(a1.testpolymod1,traintest.data)),
              c.rmse(traintest.data$a1,predict(a1.testpolymod2,traintest.data)),
							c.rmse(traintest.data$a1,predict(a1.testpolymod3,traintest.data))
							)
					)
models.rmse
a1.ncoef <- function(model){
	model %>%
    coefficients %>%
    length %>%
    {. - 1}
}


models.rmse$a1.ncoef <- c(a1.ncoef(a1.testpolymod1),
                        a1.ncoef(a1.testpolymod2),
                        a1.ncoef(a1.testpolymod3))

models.rmse %>%
  ggplot(aes(x=a1.ncoef, y= RMSE)) +
  geom_line(color = "dodgerblue") + 
  geom_point(color = "dodgerblue") +
  scale_x_continuous(breaks = c(0,1,2,3,4,5,6,7,8,9,10) )


# plot the fit

a1.fitp1 <- lm(a1 ~ poly(as.numeric(size),2) + poly(PO4,2), data=train.data )
summary(a1.fitp1)

a1.fitp2 <- lm(a1 ~ poly(as.numeric(size),2) + poly(PO4,3), data=train.data )
summary(a1.fitp2)

a1.fitp3<- lm(a1 ~ poly(PO4,3), data=train.data )
summary(a1.fitp3)


train.data <- train.data %>%
	mutate(fit1 = predict(a1.fitp1),
	fit2 = predict(a1.fitp2),
	fit3 = predict(a1.fitp3))

#  visualization: doesnt make so much sense, but at least having it visualized / too many variables

cols <- c( "Deg.2", "Deg.1")
train.data %>% 
	ggplot(aes(x=a1, y=PO4)) +
	geom_point() +
	geom_line(aes(y=fit1, color="deg 4"), size =1) +
	geom_line(aes(y=fit2, color="deg 5"), size =1) +
  geom_line(aes(y=fit3, color="deg 3"), size =1) +
	theme(legend.title = element_blank(), 
		legend.position = "bottom", 
		legend.direction = "horizontal")

```


## Log Linear Model

```{r}

a1.testlogmod <- lm(log(a1+1) ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=newtrain.data)
summary(a1.testlogmod)

exp(a1.testlogmod$coefficients["NO3"])

a1.test.pred.log <- (predict(a1.testlogmod,traintest.data))
 
a1.RMSE.testlog <- sqrt(mean((a1.test.pred.log-traintest.data$a1)^2))
a1.RMSE.testlog

a1.MAE.testlog <- mean(abs(a1.test.pred.log-traintest.data$a1))
a1.MAE.testlog

# deleting the totally unsignificant

a1.testlogmod1 <- lm(log(a1+1) ~ season + size + speed + NO3 + PO4 , data=newtrain.data)
summary(a1.testlogmod1)
a1.testlogmod1$coefficients

exp(a1.testlogmod1$coefficients)

a1.test.pred.log2 <- (predict(a1.testlogmod1,traintest.data))
 
a1.RMSE.testlog2 <- sqrt(mean((a1.test.pred.log2-traintest.data$a1)^2))
a1.RMSE.testlog2

a1.MAE.testlog2 <- mean(abs(a1.test.pred.log2-traintest.data$a1))
a1.MAE.testlog2

# not increasing, a1.testlogmod better

```



## Trees

```{r}
require(rpart)
require(rattle)
# prepare the testing

a1.testtreemod <- rpart(a1 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data,method="anova")

a1.test.pred.treemod <- predict(a1.testtreemod,traintest.data) 
 
a1.RMSE.treemod <- sqrt(mean((a1.test.pred.treemod-traintest.data$a1)^2))
a1.RMSE.treemod

a1.MAE.treemod <- mean(abs(a1.test.pred.treemod-traintest.data$a1))
a1.MAE.treemod

# pruning it

printcp(a1.testtreemod)

min.xerror <- a1.testtreemod$cptable[which.min(a1.testtreemod$cptable[,"xerror"]),"CP"]

a1.treemod <- prune(a1.testtreemod, cp = min.xerror)
summary(a1.treemod)

a1.test.pred.treemod2 <- predict(a1.treemod,traintest.data) 
 
a1.RMSE.treemod2 <- sqrt(mean((a1.test.pred.treemod2-traintest.data$a1)^2))
a1.RMSE.treemod2

a1.MAE.treemod2 <- mean(abs(a1.test.pred.treemod2-traintest.data$a1))
a1.MAE.treemod2

plot(a1.treemod)
text(a1.treemod, pretty = 0)

```

## RandomForest

```{r}
# test it
require(randomForest)
require(rattle)
set.seed(200)
a1.forestmod <- randomForest(a1 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data, na.action = na.omit, importance = TRUE, ntree=1000)

which.min(a1.forestmod$mse)


print(a1.forestmod)
plot(a1.forestmod)

# understanding the importance of each variable
# a1.forestmodimp <- as.data.frame(sort(importance(a1.forestmod)[,1],decreasing = TRUE),optional = T)
# names(a1.forestmodimp) <- "% Inc MSE"
# a1.forestmodimp
a1.forestmod$importance

a1.testforest <- predict(a1.forestmod,traintest.data)
a1.RMSE.forestmod <- sqrt(mean((a1.testforest-traintest.data$a1)^2))
a1.RMSE.forestmod
 
a1.MAE.forestmod <- mean(abs(a1.testforest-traintest.data$a1))
a1.MAE.forestmod
```

```{r}
a1.accuracy <- data.frame(Method = c("MLR","Poly LR","Log LR","Pruned Tree","RandomForest"),
                         a1.RMSE   = c(a1.RMSE.testmlr4,a1.RMSE.polymod,a1.RMSE.testlog,a1.RMSE.treemod2
,a1.RMSE.forestmod),
                         a1.MAE    = c(a1.MAE.testmlr4,a1.MAE.polymod,a1.MAE.testlog,a1.MAE.treemod2,a1.MAE.forestmod)) 

a1.accuracy

```

The table shows that Random Forest is the best method for a1.

Now we make the prediction:

```{r}
a1.to.pred <-randomForest(a1 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = train.data, na.action = na.omit, importance = TRUE, ntree=1000)

a1.pred <- predict(a1.to.pred, test.data)
a1.pred
# all negative values to zero
a1.pred[a1.pred<0] <- 0
summary(a1.pred)
# just to have a comparison
summary(train.data$a1)
```

<!--chapter:end:13a_Solution_a1.Rmd-->

---
title: "12b_Solution_a2"
author: "CTrierweiler,PGaulke"
date: "29 Juli 2019"
output: html_document
---

# Develop the Model for a2 

Methods that will be tested:

- Multiple Linear Regression
- Linear regression with polynomials
- Log Linear Regression
- Trees
- RandomForest

Logistic regression is not applicable on this data set as the response is not binary.

## Multiple Linear Regression

Determining the signifance level wanted: <0,05
```{r}
# Test it

a2.testmlrmod <- lm(a2 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=newtrain.data)
summary(a2.testmlrmod)

# Chla with highest significance

a2.testmlrmod$coefficients["Chla"]

a2.test.pred.mlr <- (predict(a2.testmlrmod,traintest.data))
 
a2.RMSE.testmlr <- sqrt(mean((a2.test.pred.mlr-traintest.data$a2)^2))
a2.RMSE.testmlr

a2.MAE.testmlr <- mean(abs(a2.test.pred.mlr-traintest.data$a2))
a2.MAE.testmlr

# Reducing the variables
a2.testmlrmod2 <- lm(a2 ~ speed + mxPH + Chla, data=newtrain.data)

summary(a2.testmlrmod2)
a2.testmlrmod2$coefficients["Chla"]

a2.test.pred.mlr2 <- (predict(a2.testmlrmod2,traintest.data))
 
a2.RMSE.testmlr2 <- sqrt(mean((a2.test.pred.mlr2-traintest.data$a2)^2))
a2.RMSE.testmlr2

a2.MAE.testmlr2 <- mean(abs(a2.test.pred.mlr2-traintest.data$a2))
a2.MAE.testmlr2

# Reducing the variables to only one variable (SLR)

a2.testmlrmod3 <- lm(a2 ~ Chla, data=newtrain.data)

summary(a2.testmlrmod3)

a2.test.pred.mlr3 <- (predict(a2.testmlrmod3,traintest.data))
 
a2.RMSE.testmlr3 <- sqrt(mean((a2.test.pred.mlr3-traintest.data$a2)^2))
a2.RMSE.testmlr3

a2.MAE.testmlr3 <- mean(abs(a2.test.pred.mlr3-traintest.data$a2))
a2.MAE.testmlr3

# MLR2 better, optimize
a2.testmlrmod4 <- lm(a2 ~ mxPH + Chla, data=newtrain.data)

a2.test.pred.mlr4 <- (predict(a2.testmlrmod4,traintest.data))
 
a2.RMSE.testmlr4 <- sqrt(mean((a2.test.pred.mlr4-traintest.data$a2)^2))
a2.RMSE.testmlr4

a2.MAE.testmlr4 <- mean(abs(a2.test.pred.mlr4-traintest.data$a2))
a2.MAE.testmlr4

# a2-testmlrmod4 as best choice

```

Trying k-fold cross validation

```{r, error=TRUE}
require(caret)
set.seed(200) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
a2.crossmodelmlr <- train(a2 ~ speed + Chla, data = train.data, method = "lm",
               trControl = train.control)
# Summarize the results
print(a2.crossmodelmlr)
```


## Polynomials

Adding polynomials

```{r}

# test it from mlr4

a2.testpoly <- glm(a2 ~ speed + Chla, data=newtrain.data)
summary(a2.testpoly)
# use of poly

a2.testpolymod1 <- glm(a2 ~ poly(as.numeric(speed),2) + poly(Chla,2), data=newtrain.data)
summary(a2.testpolymod1)

a2.test.pred.poly <- predict(a2.testpolymod1,traintest.data) 
 
a2.RMSE.polymod <- sqrt(mean((a2.test.pred.poly-traintest.data$a2)^2))
a2.RMSE.polymod

a2.MAE.polymod <- mean(abs(a2.test.pred.poly-traintest.data$a2))
a2.MAE.polymod

# checking for lower polys

a2.testpolymod2 <- glm(a2 ~ poly(as.numeric(speed),1) + poly(Chla,2), data=newtrain.data)
summary(a2.testpolymod2)

a2.test.pred.poly2 <- predict(a2.testpolymod2,traintest.data) 
 
a2.RMSE.polymod2 <- sqrt(mean((a2.test.pred.poly2-traintest.data$a2)^2))
a2.RMSE.polymod2

a2.MAE.polymod2 <- mean(abs(a2.test.pred.poly2-traintest.data$a2))
a2.MAE.polymod2

# Best poly is a2.testpolymod2, alternatively just Chla

a2.testpolymod3 <- glm(a2 ~ poly(Chla,2), data=newtrain.data)
summary(a2.testpolymod3)

a2.test.pred.poly3 <- predict(a2.testpolymod3,traintest.data) 
 
a2.RMSE.polymod3 <- sqrt(mean((a2.test.pred.poly3-traintest.data$a2)^2))
a2.RMSE.polymod3

a2.MAE.polymod3 <- mean(abs(a2.test.pred.poly3-traintest.data$a2))
a2.MAE.polymod3

# slightly worse

```


```{r}

## plot the rmse
a2.models.rmse <- tibble(
            a2.model = paste0("model.pd",c(1,2,3)),
						RMSE= c(
							c.rmse(traintest.data$a2,predict(a2.testpolymod1,traintest.data)),
              c.rmse(traintest.data$a2,predict(a2.testpolymod2,traintest.data)),
							c.rmse(traintest.data$a2,predict(a2.testpolymod3,traintest.data))
							)
					)
a2.models.rmse
a2.ncoef <- function(a2.model){
	a2.model %>%
    coefficients %>%
    length %>%
    {. - 1}
}


a2.models.rmse$a2.ncoef <- c(a2.ncoef(a2.testpolymod1),
                        a2.ncoef(a2.testpolymod2),
                        a2.ncoef(a2.testpolymod3))

a2.models.rmse %>%
  ggplot(aes(x=a2.ncoef, y= RMSE)) +
  geom_line(color = "dodgerblue") + 
  geom_point(color = "dodgerblue") +
  scale_x_continuous(breaks = c(0,1,2,3,4,5,6,7,8,9,10) )

# plot the fit

a2.fitp1 <- lm(a2 ~ poly(as.numeric(speed),1) + poly(Chla,2), data=train.data )
summary(a2.fitp1)

a2.fitp2 <- lm(a2 ~ poly(as.numeric(speed),2) + poly(Chla,2), data=train.data )
summary(a2.fitp2)

a2.fitp3<- lm(a2 ~ poly(Chla,2), data=train.data)
summary(a2.fitp3)

train.data <- train.data %>%
	mutate(fit1 = predict(a2.fitp1),
	fit2 = predict(a2.fitp2),
	fit3 = predict(a2.fitp3))

#  visualization: doesnt make so much sense, but at least having it visualized / too many variables

cols <- c( "Deg.2", "Deg.1")
train.data %>% 
	ggplot(aes(x=a2, y=Chla)) +
	geom_point() +
	geom_line(aes(y=fit1, color="deg 3"), size =1) +
	geom_line(aes(y=fit2, color="deg 4"), size =1) +
	geom_line(aes(y=fit3, color="deg 2"), size =1) +
  theme(legend.title = element_blank(), 
		legend.position = "bottom", 
		legend.direction = "horizontal")

```


## Log Linear Model

```{r}

a2.testlogmod <- lm(log(a2+1) ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=newtrain.data)
summary(a2.testlogmod)

exp(a2.testlogmod$coefficients)

a2.test.pred.log <- (predict(a2.testlogmod,traintest.data))
 
a2.RMSE.testlog <- sqrt(mean((a2.test.pred.log-traintest.data$a2)^2))
a2.RMSE.testlog

a2.MAE.testlog <- mean(abs(a2.test.pred.log-traintest.data$a2))
a2.MAE.testlog

# deleting the totally unsignificant

a2.testlogmod1 <- lm(log(a2+1) ~ mxPH + Chla , data=newtrain.data)
summary(a2.testlogmod1)
a2.testlogmod1$coefficients

exp(a2.testlogmod1$coefficients)

a2.test.pred.log2 <- (predict(a2.testlogmod1,traintest.data))
 
a2.RMSE.testlog2 <- sqrt(mean((a2.test.pred.log2-traintest.data$a2)^2))
a2.RMSE.testlog2

a2.MAE.testlog2 <- mean(abs(a2.test.pred.log2-traintest.data$a2))
a2.MAE.testlog2

# not increasing, a2.testlogmod better

```



## Trees

```{r}
require(rpart)
require(rattle)
# prepare the testing

a2.testtreemod <- rpart(a2 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data,method="anova")

a2.test.pred.treemod <- predict(a2.testtreemod,traintest.data) 
 
a2.RMSE.treemod <- sqrt(mean((a2.test.pred.treemod-traintest.data$a2)^2))
a2.RMSE.treemod

a2.MAE.treemod <- mean(abs(a2.test.pred.treemod-traintest.data$a2))
a2.MAE.treemod

# pruning it

printcp(a2.testtreemod)

min.xerror <- a2.testtreemod$cptable[which.min(a2.testtreemod$cptable[,"xerror"]),"CP"]

a2.treemod <- prune(a2.testtreemod, cp = min.xerror)
summary(a2.treemod)

a2.test.pred.treemod2 <- predict(a2.treemod,traintest.data) 
 
a2.RMSE.treemod2 <- sqrt(mean((a2.test.pred.treemod2-traintest.data$a2)^2))
a2.RMSE.treemod2

a2.MAE.treemod2 <- mean(abs(a2.test.pred.treemod2-traintest.data$a2))
a2.MAE.treemod2

plot(a2.treemod)
text(a2.treemod, pretty = 0)

```

## RandomForest

```{r}
# test it
require(randomForest)
require(rattle)
set.seed(200)
a2.forestmod <- randomForest(a2 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data, na.action = na.omit, importance = TRUE, ntree=1000)

which.min(a2.forestmod$mse)


print(a2.forestmod)
plot(a2.forestmod)

# understanding the importance of each variable
# a2.forestmodimp <- as.data.frame(sort(importance(a2.forestmod)[,1],decreasing = TRUE),optional = T)
# names(a2.forestmodimp) <- "% Inc MSE"
# a2.forestmodimp
a2.forestmod$importance

a2.testforest <- predict(a2.forestmod,traintest.data)
a2.RMSE.forestmod <- sqrt(mean((a2.testforest-traintest.data$a2)^2))
a2.RMSE.forestmod
 
a2.MAE.forestmod <- mean(abs(a2.testforest-traintest.data$a2))
a2.MAE.forestmod
```

```{r}
a2.accuracy <- data.frame(Method = c("MLR","Poly LR","Log LR","Pruned Tree","RandomForest"),
                         a2.RMSE   = c(a2.RMSE.testmlr4,a2.RMSE.polymod2,a2.RMSE.testlog,a2.RMSE.treemod2
,a2.RMSE.forestmod),
                         a2.MAE    = c(a2.MAE.testmlr4,a2.MAE.polymod2,a2.MAE.testlog,a2.MAE.treemod2,a2.MAE.forestmod)) 

a2.accuracy

```

The table shows that again RandomForest is the best method for a2.

Now we make the prediction:

```{r}
a2.to.pred <-randomForest(a2 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = train.data, na.action = na.omit, importance = TRUE, ntree=1000)

a2.pred <- predict(a2.to.pred, test.data)
a2.pred
# all negative values to zero
a2.pred[a2.pred<0] <- 0
summary(a2.pred)
# just to have a comparison
summary(train.data$a2)
```

<!--chapter:end:13b_Solution_a2.Rmd-->

---
title: "12c_Solution_a3"
author: "CTrierweiler,PGaulke"
date: "29 Juli 2019"
output: html_document
---

# Develop the Model for a3 

Methods that will be tested:

- Multiple Linear Regression
- Linear regression with polynomials
- Log Linear Regression
- Trees
- RandomForest

Logistic regression is not applicable on this data set as the response is not binary.

## Multiple Linear Regression

```{r}

a3.testmlrmod <- lm(a3 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=newtrain.data)
summary(a3.testmlrmod)

a3.test.pred.mlr <- (predict(a3.testmlrmod,traintest.data))
 
a3.RMSE.testmlr <- sqrt(mean((a3.test.pred.mlr-traintest.data$a3)^2))
a3.RMSE.testmlr

a3.MAE.testmlr <- mean(abs(a3.test.pred.mlr-traintest.data$a3))
a3.MAE.testmlr

# Reducing the variables
a3.testmlrmod2 <- lm(a3 ~ NH4 + mnO2 + season, data=newtrain.data)
summary(a3.testmlrmod2)

a3.test.pred.mlr2 <- (predict(a3.testmlrmod2,traintest.data))
 
a3.RMSE.testmlr2 <- sqrt(mean((a3.test.pred.mlr2-traintest.data$a3)^2))
a3.RMSE.testmlr2

a3.MAE.testmlr2 <- mean(abs(a3.test.pred.mlr2-traintest.data$a3))
a3.MAE.testmlr2

# Reducing the variables to only one variable (SLR)

a3.testmlrmod3 <- lm(a3 ~ season, data=newtrain.data)

summary(a3.testmlrmod3)

a3.test.pred.mlr3 <- (predict(a3.testmlrmod3,traintest.data))
 
a3.RMSE.testmlr3 <- sqrt(mean((a3.test.pred.mlr3-traintest.data$a3)^2))
a3.RMSE.testmlr3

a3.MAE.testmlr3 <- mean(abs(a3.test.pred.mlr3-traintest.data$a3))
a3.MAE.testmlr3

# SLR best, optimize
a3.testmlrmod4 <- lm(a3 ~ season + NH4 * mnO2, data=newtrain.data)
summary(a3.testmlrmod4)
a3.test.pred.mlr4 <- (predict(a3.testmlrmod4,traintest.data))
 
a3.RMSE.testmlr4 <- sqrt(mean((a3.test.pred.mlr4-traintest.data$a3)^2))
a3.RMSE.testmlr4

a3.MAE.testmlr4 <- mean(abs(a3.test.pred.mlr4-traintest.data$a3))
a3.MAE.testmlr4

# a3-testmlrmod4 as best choice

```

Trying k-fold cross validation

```{r}
require(caret)
set.seed(200) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
a3.crossmodelmlr <- train(a3 ~ season + NH4 * mnO2, data = train.data, method = "lm",
               trControl = train.control)
# Summarize the results
print(a3.crossmodelmlr)
```


## Polynomials

Adding polynomials

```{r}

# test it from mlr4

a3.testpoly <- glm(a3 ~ season + NH4 * mnO2, data=newtrain.data)
summary(a3.testpoly)
# use of poly

a3.testpolymod1 <- glm(a3 ~ poly(as.numeric(season),2) + poly(NH4,2) * poly(mnO2,2), data=newtrain.data)
summary(a3.testpolymod1)

a3.test.pred.poly <- predict(a3.testpolymod1,traintest.data) 
 
a3.RMSE.polymod <- sqrt(mean((a3.test.pred.poly-traintest.data$a3)^2))
a3.RMSE.polymod

a3.MAE.polymod <- mean(abs(a3.test.pred.poly-traintest.data$a3))
a3.MAE.polymod

# checking for adjusted polys

a3.testpolymod2 <- glm(a3 ~ poly(as.numeric(season),3) + NH4 * mnO2, data=newtrain.data)
summary(a3.testpolymod2)

a3.test.pred.poly2 <- predict(a3.testpolymod2,traintest.data) 
 
a3.RMSE.polymod2 <- sqrt(mean((a3.test.pred.poly2-traintest.data$a3)^2))
a3.RMSE.polymod2

a3.MAE.polymod2 <- mean(abs(a3.test.pred.poly2-traintest.data$a3))
a3.MAE.polymod2

# Best poly is a3.testpolymod2

a3.testpolymod3 <- glm(a3 ~ poly(as.numeric(season),3), data=newtrain.data)
summary(a3.testpolymod3)

a3.test.pred.poly3 <- predict(a3.testpolymod3,traintest.data) 
 
a3.RMSE.polymod3 <- sqrt(mean((a3.test.pred.poly3-traintest.data$a3)^2))
a3.RMSE.polymod3

a3.MAE.polymod3 <- mean(abs(a3.test.pred.poly3-traintest.data$a3))
a3.MAE.polymod3

# worse
```

Trying k-fold cross validation

```{r}
require(caret)
set.seed(200) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
a3.crossmodelmlr <- train(a3 ~ poly(as.numeric(season),3) + NH4 * mnO2, data = train.data, method = "lm",
               trControl = train.control)
# Summarize the results
print(a3.crossmodelmlr)
```

```{r}
## plot the rmse
a3.models.rmse <- tibble(
            a3.model = paste0("model.pd",c(1,2,3)),
						RMSE= c(
							c.rmse(traintest.data$a3,predict(a3.testpolymod1,traintest.data)),
              c.rmse(traintest.data$a3,predict(a3.testpolymod2,traintest.data)),
							c.rmse(traintest.data$a3,predict(a3.testpolymod3,traintest.data))
							)
					)
a3.models.rmse
a3.ncoef <- function(a3.model){
	a3.model %>%
    coefficients %>%
    length %>%
    {. - 1}
}


a3.models.rmse$a3.ncoef <- c(a3.ncoef(a3.testpolymod1),
                        a3.ncoef(a3.testpolymod2),
                        a3.ncoef(a3.testpolymod3))

a3.models.rmse %>%
  ggplot(aes(x=a3.ncoef, y= RMSE)) +
  geom_line(color = "dodgerblue") + 
  geom_point(color = "dodgerblue") +
  scale_x_continuous(breaks = c(0,1,2,3,4,5,6,7,8,9,10) )

# plot the fit

a3.fitp1 <- lm(a3 ~ poly(as.numeric(season),3) + NH4 * mnO2, data=train.data )
summary(a3.fitp1)

a3.fitp2 <- lm(a3 ~ poly(as.numeric(season),2) + poly(NH4,2) * poly(mnO2,2), data=train.data )
summary(a3.fitp2)

a3.fitp3 <- lm(a3 ~ poly(as.numeric(season),3), data=train.data )
summary(a3.fitp3)

train.data <- train.data %>%
	mutate(fit1 = predict(a3.fitp1),
	fit2 = predict(a3.fitp2),
	fit3 = predict(a3.fitp3))

#  visualization: doesnt make so much sense, but at least having it visualized / too many variables

cols <- c( "Deg.2", "Deg.1")
train.data %>% 
	ggplot(aes(x=a3, y=PO4)) +
	geom_point() +
	geom_line(aes(y=fit1, color="deg 5"), size =1) +
	geom_line(aes(y=fit2, color="deg 6"), size =1) +
  geom_line(aes(y=fit2, color="deg 3"), size =1) +

	theme(legend.title = element_blank(), 
		legend.position = "bottom", 
		legend.direction = "horizontal")

```


## Log Linear Model

```{r}

a3.testlogmod <- lm(log(a3+1) ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=newtrain.data)
summary(a3.testlogmod)

exp(a3.testlogmod$coefficients)

a3.test.pred.log <- (predict(a3.testlogmod,traintest.data))
 
a3.RMSE.testlog <- sqrt(mean((a3.test.pred.log-traintest.data$a3)^2))
a3.RMSE.testlog

a3.MAE.testlog <- mean(abs(a3.test.pred.log-traintest.data$a3))
a3.MAE.testlog

# deleting the totally unsignificant

a3.testlogmod1 <- lm(log(a3+1) ~ season + NH4 * mnO2 , data=newtrain.data)
summary(a3.testlogmod1)
a3.testlogmod1$coefficients

exp(a3.testlogmod1$coefficients)

a3.test.pred.log2 <- (predict(a3.testlogmod1,traintest.data))
 
a3.RMSE.testlog2 <- sqrt(mean((a3.test.pred.log2-traintest.data$a3)^2))
a3.RMSE.testlog2

a3.MAE.testlog2 <- mean(abs(a3.test.pred.log2-traintest.data$a3))
a3.MAE.testlog2

# not increasing, a3.testlogmod better

```

## Trees

```{r}
require(rpart)
require(rattle)
# prepare the testing

a3.testtreemod <- rpart(a3 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data,method="anova")

a3.test.pred.treemod <- predict(a3.testtreemod,traintest.data) 
 
a3.RMSE.treemod <- sqrt(mean((a3.test.pred.treemod-traintest.data$a3)^2))
a3.RMSE.treemod

a3.MAE.treemod <- mean(abs(a3.test.pred.treemod-traintest.data$a3))
a3.MAE.treemod

# pruning it

printcp(a3.testtreemod)

min.xerror <- a3.testtreemod$cptable[which.min(a3.testtreemod$cptable[,"xerror"]),"CP"]

a3.treemod <- prune(a3.testtreemod, cp = min.xerror)
summary(a3.treemod)

a3.test.pred.treemod2 <- predict(a3.treemod,traintest.data) 
 
a3.RMSE.treemod2 <- sqrt(mean((a3.test.pred.treemod2-traintest.data$a3)^2))
a3.RMSE.treemod2

a3.MAE.treemod2 <- mean(abs(a3.test.pred.treemod2-traintest.data$a3))
a3.MAE.treemod2

plot(a3.treemod)
text(a3.treemod, pretty = 0)

```

## RandomForest

```{r}
# test it
require(randomForest)
require(rattle)
set.seed(200)
a3.forestmod <- randomForest(a3 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data, na.action = na.omit, importance = TRUE, ntree=1000)

which.min(a3.forestmod$mse)


print(a3.forestmod)
plot(a3.forestmod)

# understanding the importance of each variable
# a3.forestmodimp <- as.data.frame(sort(importance(a3.forestmod)[,1],decreasing = TRUE),optional = T)
# names(a3.forestmodimp) <- "% Inc MSE"
# a3.forestmodimp
a3.forestmod$importance

a3.testforest <- predict(a3.forestmod,traintest.data)
a3.RMSE.forestmod <- sqrt(mean((a3.testforest-traintest.data$a3)^2))
a3.RMSE.forestmod
 
a3.MAE.forestmod <- mean(abs(a3.testforest-traintest.data$a3))
a3.MAE.forestmod
```

```{r}
a3.accuracy <- data.frame(Method = c("MLR","Poly LR","Log LR","Pruned Tree","RandomForest"),
                         a3.RMSE   = c(a3.RMSE.testmlr4,a3.RMSE.polymod2,a3.RMSE.testlog,a3.RMSE.treemod2
,a3.RMSE.forestmod),
                         a3.MAE    = c(a3.MAE.testmlr4,a3.MAE.polymod2,a3.MAE.testlog,a3.MAE.treemod2,a3.MAE.forestmod)) 

a3.accuracy

```

The table shows that MLR and Poly are the best method for a3.

Now we make the prediction(with a poly):

```{r}
a3.to.pred <-lm(a3 ~ poly(as.numeric(season),3) + NH4 * mnO2, data = train.data)

a3.pred <- predict(a3.to.pred, test.data)
a3.pred
# all negative values to zero
a3.pred[a3.pred<0] <- 0
summary(a3.pred)
# just to have a comparison
summary(train.data$a3)
```

<!--chapter:end:13c_Solution_a3.Rmd-->

---
title: "12d_Solution_a4"
author: "CTrierweiler,PGaulke"
date: "29 Juli 2019"
output: html_document
---

# Develop the Model for a4 

Methods that will be tested:

- Multiple Linear Regression
- Linear regression with polynomials
- Log Linear Regression
- Trees
- RandomForest

Logistic regression is not applicable on this data set as the response is not binary.

## Multiple Linear Regression

```{r}

a4.testmlrmod <- lm(a4 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=newtrain.data)
summary(a4.testmlrmod)

a4.test.pred.mlr <- (predict(a4.testmlrmod,traintest.data))
 
a4.RMSE.testmlr <- sqrt(mean((a4.test.pred.mlr-traintest.data$a4)^2))
a4.RMSE.testmlr

a4.MAE.testmlr <- mean(abs(a4.test.pred.mlr-traintest.data$a4))
a4.MAE.testmlr

# Reducing the variables
a4.testmlrmod2 <- lm(a4 ~ mxPH + NH4 + mnO2 , data=newtrain.data)
summary(a4.testmlrmod2)

a4.test.pred.mlr2 <- (predict(a4.testmlrmod2,traintest.data))
 
a4.RMSE.testmlr2 <- sqrt(mean((a4.test.pred.mlr2-traintest.data$a4)^2))
a4.RMSE.testmlr2

a4.MAE.testmlr2 <- mean(abs(a4.test.pred.mlr2-traintest.data$a4))
a4.MAE.testmlr2

# Figure the best out compared to mlr2 

a4.testmlrmod3 <- lm(a4 ~  mxPH  * mnO2 , data=newtrain.data)

summary(a4.testmlrmod3)

a4.test.pred.mlr3 <- (predict(a4.testmlrmod3,traintest.data))
 
a4.RMSE.testmlr3 <- sqrt(mean((a4.test.pred.mlr3-traintest.data$a4)^2))
a4.RMSE.testmlr3

a4.MAE.testmlr3 <- mean(abs(a4.test.pred.mlr3-traintest.data$a4))
a4.MAE.testmlr3

# a4-testmlrmod3 as best choice

```

Trying k-fold cross validation

```{r}
require(caret)
set.seed(200) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
a4.crossmodelmlr <- train(a4 ~ mxPH + NH4 + mnO2, data = train.data, method = "lm",
               trControl = train.control)
# Summarize the results
print(a4.crossmodelmlr)
```


## Polynomials

Adding polynomials

```{r}

# test it from mlr2

a4.testpoly <- glm(a4 ~ mxPH + NH4 + mnO2, data=newtrain.data)
summary(a4.testpoly)

# use of poly

a4.testpolymod1 <- glm(a4 ~ poly(mxPH,2) * poly(mnO2,2), data=newtrain.data)
summary(a4.testpolymod1)

a4.test.pred.poly <- predict(a4.testpolymod1,traintest.data) 
 
a4.RMSE.polymod <- sqrt(mean((a4.test.pred.poly-traintest.data$a4)^2))
a4.RMSE.polymod

a4.MAE.polymod <- mean(abs(a4.test.pred.poly-traintest.data$a4))
a4.MAE.polymod

# checking for adjusted polys

a4.testpolymod2 <- glm(a4 ~ poly(mxPH,2) * poly(mnO2,1), data=newtrain.data)
summary(a4.testpolymod2)

a4.test.pred.poly2 <- predict(a4.testpolymod2,traintest.data) 
 
a4.RMSE.polymod2 <- sqrt(mean((a4.test.pred.poly2-traintest.data$a4)^2))
a4.RMSE.polymod2

a4.MAE.polymod2 <- mean(abs(a4.test.pred.poly2-traintest.data$a4))
a4.MAE.polymod2

# Best poly is a4.testpolymod2, adjust?

a4.testpolymod3 <- glm(a4 ~ poly(NH4,2), data=newtrain.data)
summary(a4.testpolymod3)

a4.test.pred.poly3 <- predict(a4.testpolymod3,traintest.data) 
 
a4.RMSE.polymod3 <- sqrt(mean((a4.test.pred.poly3-traintest.data$a4)^2))
a4.RMSE.polymod3

a4.MAE.polymod3 <- mean(abs(a4.test.pred.poly3-traintest.data$a4))
a4.MAE.polymod3

# worse

```


```{r}
## plot the rmse
a4.models.rmse <- tibble(
            a4.model = paste0("model.pd",c(1,2,3)),
						RMSE= c(
							c.rmse(traintest.data$a4,predict(a4.testpolymod1,traintest.data)),
              c.rmse(traintest.data$a4,predict(a4.testpolymod2,traintest.data)),
							c.rmse(traintest.data$a4,predict(a4.testpolymod3,traintest.data))
							)
					)
a4.models.rmse
a4.ncoef <- function(a4.model){
	a4.model %>%
    coefficients %>%
    length %>%
    {. - 1}
}


a4.models.rmse$a4.ncoef <- c(a4.ncoef(a4.testpolymod1),
                        a4.ncoef(a4.testpolymod2),
                        a4.ncoef(a4.testpolymod3))

a4.models.rmse %>%
  ggplot(aes(x=a4.ncoef, y= RMSE)) +
  geom_line(color = "dodgerblue") + 
  geom_point(color = "dodgerblue") +
  scale_x_continuous(breaks = c(0,1,2,3,4,5,6,7,8,9,10) )

# plot the fit

a4.fitp1 <- lm(a4 ~ poly(mxPH,2) * poly(mnO2,1), data=train.data )
summary(a4.fitp1)

a4.fitp2 <- lm(a4 ~ poly(mxPH,2) * poly(mnO2,2), data=train.data )
summary(a4.fitp2)

a4.fitp3 <- lm(a4 ~ poly(NH4,2), data=train.data )
summary(a4.fitp3)


train.data <- train.data %>%
	mutate(fit1 = predict(a4.fitp1),
	fit2 = predict(a4.fitp2),
	fit3 = predict(a4.fitp3))

#  visualization: doesnt make so much sense, but at least having it visualized / too many variables

cols <- c( "Deg.2", "Deg.1")
train.data %>% 
	ggplot(aes(x=a4, y=mxPH)) +
	geom_point() +
	geom_line(aes(y=fit1, color="deg 4"), size =1) +
	geom_line(aes(y=fit2, color="deg 3"), size =1) +
	geom_line(aes(y=fit3, color="deg 2"), size =1) +
	theme(legend.title = element_blank(), 
		legend.position = "bottom", 
		legend.direction = "horizontal")

```


## Log Linear Model

```{r}

a4.testlogmod <- lm(log(a4+1) ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=newtrain.data)
summary(a4.testlogmod)

exp(a4.testlogmod$coefficients)

a4.test.pred.log <- (predict(a4.testlogmod,traintest.data))
 
a4.RMSE.testlog <- sqrt(mean((a4.test.pred.log-traintest.data$a4)^2))
a4.RMSE.testlog

a4.MAE.testlog <- mean(abs(a4.test.pred.log-traintest.data$a4))
a4.MAE.testlog

# deleting the totally unsignificant

a4.testlogmod1 <- lm(log(a4+1) ~ mxPH  * mnO2 , data=newtrain.data)
summary(a4.testlogmod1)
a4.testlogmod1$coefficients

exp(a4.testlogmod1$coefficients)

a4.test.pred.log2 <- (predict(a4.testlogmod1,traintest.data))
 
a4.RMSE.testlog2 <- sqrt(mean((a4.test.pred.log2-traintest.data$a4)^2))
a4.RMSE.testlog2

a4.MAE.testlog2 <- mean(abs(a4.test.pred.log2-traintest.data$a4))
a4.MAE.testlog2

# not increasing, a4.testlogmod1 better

```

## Trees

```{r, error=TRUE}
require(rpart)
require(rattle)
# prepare the testing

a4.testtreemod <- rpart(a4 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data,method="anova")

a4.test.pred.treemod <- predict(a4.testtreemod,traintest.data) 
 
a4.RMSE.treemod <- sqrt(mean((a4.test.pred.treemod-traintest.data$a4)^2))
a4.RMSE.treemod

a4.MAE.treemod <- mean(abs(a4.test.pred.treemod-traintest.data$a4))
a4.MAE.treemod

# pruning it

printcp(a4.testtreemod)

min.xerror <- a4.testtreemod$cptable[which.min(a4.testtreemod$cptable[,"xerror"]),"CP"]

a4.treemod <- prune(a4.testtreemod, cp = min.xerror)
summary(a4.treemod)

a4.test.pred.treemod2 <- predict(a4.treemod,traintest.data) 
 
a4.RMSE.treemod2 <- sqrt(mean((a4.test.pred.treemod2-traintest.data$a4)^2))
a4.RMSE.treemod2

a4.MAE.treemod2 <- mean(abs(a4.test.pred.treemod2-traintest.data$a4))
a4.MAE.treemod2

plot(a4.treemod)
text(a4.treemod, pretty = 0)

```

## RandomForest

```{r}
# test it
require(randomForest)
require(rattle)
set.seed(200)
a4.forestmod <- randomForest(a4 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data, na.action = na.omit, importance = TRUE, ntree=1000)

which.min(a4.forestmod$mse)


print(a4.forestmod)
plot(a4.forestmod)

# understanding the importance of each variable
# a4.forestmodimp <- as.data.frame(sort(importance(a4.forestmod)[,1],decreasing = TRUE),optional = T)
# names(a4.forestmodimp) <- "% Inc MSE"
# a4.forestmodimp
a4.forestmod$importance

a4.testforest <- predict(a4.forestmod,traintest.data)
a4.RMSE.forestmod <- sqrt(mean((a4.testforest-traintest.data$a4)^2))
a4.RMSE.forestmod
 
a4.MAE.forestmod <- mean(abs(a4.testforest-traintest.data$a4))
a4.MAE.forestmod
```

```{r}
a4.accuracy <- data.frame(Method = c("MLR","Poly LR","Log LR","Pruned Tree","RandomForest"),
                         a4.RMSE   = c(a4.RMSE.testmlr3,a4.RMSE.polymod2,a4.RMSE.testlog2,a4.RMSE.treemod2
,a4.RMSE.forestmod),
                         a4.MAE    = c(a4.MAE.testmlr3,a4.MAE.polymod2,a4.MAE.testlog2,a4.MAE.treemod2,a4.MAE.forestmod)) 

a4.accuracy

```

The table shows that MLR is the best method for a4.

Now we make the prediction:

```{r}
a4.to.pred <-lm(a4 ~  mxPH  * mnO2, data = train.data)

a4.pred <- predict(a4.to.pred, test.data)
a4.pred
# all negative values to zero
a4.pred[a4.pred<0] <- 0
summary(a4.pred)
# just to have a comparison
summary(train.data$a4)
```

<!--chapter:end:13d_Solution_a4.Rmd-->

---
title: "12e_Solution_a5"
author: "CTrierweiler,PGaulke"
date: "29 Juli 2019"
output: html_document
---

# Develop the Model for a5 

Methods that will be tested:

- Multiple Linear Regression
- Linear regression with polynomials
- Log Linear Regression
- Trees
- RandomForest

Logistic regression is not applicable on this data set as the response is not binary.

## Multiple Linear Regression

```{r}

a5.testmlrmod <- lm(a5 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=newtrain.data)
summary(a5.testmlrmod)

a5.test.pred.mlr <- (predict(a5.testmlrmod,traintest.data))
 
a5.RMSE.testmlr <- sqrt(mean((a5.test.pred.mlr-traintest.data$a5)^2))
a5.RMSE.testmlr

a5.MAE.testmlr <- mean(abs(a5.test.pred.mlr-traintest.data$a5))
a5.MAE.testmlr

# Reducing the variables
a5.testmlrmod2 <- lm(a5 ~ size *season + mnO2, data=newtrain.data)
summary(a5.testmlrmod2)

a5.test.pred.mlr2 <- (predict(a5.testmlrmod2,traintest.data))
 
a5.RMSE.testmlr2 <- sqrt(mean((a5.test.pred.mlr2-traintest.data$a5)^2))
a5.RMSE.testmlr2

a5.MAE.testmlr2 <- mean(abs(a5.test.pred.mlr2-traintest.data$a5))
a5.MAE.testmlr2

# Reducing the variables to only one variable (SLR)

a5.testmlrmod3 <- lm(a5 ~ mnO2, data=newtrain.data)

summary(a5.testmlrmod3)

a5.test.pred.mlr3 <- (predict(a5.testmlrmod3,traintest.data))
 
a5.RMSE.testmlr3 <- sqrt(mean((a5.test.pred.mlr3-traintest.data$a5)^2))
a5.RMSE.testmlr3

a5.MAE.testmlr3 <- mean(abs(a5.test.pred.mlr3-traintest.data$a5))
a5.MAE.testmlr3

# SLR best, optimize
a5.testmlrmod4 <- lm(a5 ~ size * mnO2, data=newtrain.data)
summary(a5.testmlrmod4)
a5.test.pred.mlr4 <- (predict(a5.testmlrmod4,traintest.data))
 
a5.RMSE.testmlr4 <- sqrt(mean((a5.test.pred.mlr4-traintest.data$a5)^2))
a5.RMSE.testmlr4

a5.MAE.testmlr4 <- mean(abs(a5.test.pred.mlr4-traintest.data$a5))
a5.MAE.testmlr4

# a5-testmlrmod4 as best choice

```

Trying k-fold cross validation

```{r}
require(caret)
set.seed(200) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
a5.crossmodelmlr <- train(a5 ~ size * mnO2, data = train.data, method = "lm",
               trControl = train.control)
# Summarize the results
print(a5.crossmodelmlr)
```


## Polynomials

Adding polynomials

```{r}

# test it from mlr4

a5.testpoly <- glm(a5 ~ size * mnO2, data=newtrain.data)
summary(a5.testpoly)
# use of poly

a5.testpolymod1 <- glm(a5 ~ poly(as.numeric(size),2) * poly(mnO2,2), data=newtrain.data)
summary(a5.testpolymod1)

a5.test.pred.poly <- predict(a5.testpolymod1,traintest.data) 
 
a5.RMSE.polymod <- sqrt(mean((a5.test.pred.poly-traintest.data$a5)^2))
a5.RMSE.polymod

a5.MAE.polymod <- mean(abs(a5.test.pred.poly-traintest.data$a5))
a5.MAE.polymod

# checking for adjusted polys

a5.testpolymod2 <- glm(a5 ~ poly(as.numeric(size),2) * poly(mnO2,1), data=newtrain.data)
summary(a5.testpolymod2)

a5.test.pred.poly2 <- predict(a5.testpolymod2,traintest.data) 
 
a5.RMSE.polymod2 <- sqrt(mean((a5.test.pred.poly2-traintest.data$a5)^2))
a5.RMSE.polymod2

a5.MAE.polymod2 <- mean(abs(a5.test.pred.poly2-traintest.data$a5))
a5.MAE.polymod2

# Best poly is a5.testpolymod2, alternative

a5.testpolymod3 <- glm(a5 ~ poly(as.numeric(size),2), data=newtrain.data)
summary(a5.testpolymod3)

a5.test.pred.poly3 <- predict(a5.testpolymod3,traintest.data) 
 
a5.RMSE.polymod3 <- sqrt(mean((a5.test.pred.poly3-traintest.data$a5)^2))
a5.RMSE.polymod3

a5.MAE.polymod3 <- mean(abs(a5.test.pred.poly3-traintest.data$a5))
a5.MAE.polymod3

# worse

```

Trying k-fold cross validation

```{r}
require(caret)
set.seed(200) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
a5.crossmodelmlr <- train(a5 ~ poly(as.numeric(size),2) * poly(mnO2,1), data = train.data, method = "lm",
               trControl = train.control)
# Summarize the results
print(a5.crossmodelmlr)
```

```{r}
## plot the rmse
a5.models.rmse <- tibble(
            a5.model = paste0("model.pd",c(1,2,3)),
						RMSE= c(
							c.rmse(traintest.data$a5,predict(a5.testpolymod1,traintest.data)),
              c.rmse(traintest.data$a5,predict(a5.testpolymod2,traintest.data)),
							c.rmse(traintest.data$a5,predict(a5.testpolymod3,traintest.data))
							)
					)
a5.models.rmse
a5.ncoef <- function(a5.model){
	a5.model %>%
    coefficients %>%
    length %>%
    {. - 1}
}


a5.models.rmse$a5.ncoef <- c(a5.ncoef(a5.testpolymod1),
                        a5.ncoef(a5.testpolymod2),
                        a5.ncoef(a5.testpolymod3))

a5.models.rmse %>%
  ggplot(aes(x=a5.ncoef, y= RMSE)) +
  geom_line(color = "dodgerblue") + 
  geom_point(color = "dodgerblue") +
  scale_x_continuous(breaks = c(0,1,2,3,4,5,6,7,8,9,10) )

# plot the fit

a5.fitp1 <- lm(a5 ~ poly(as.numeric(size),2) * poly(mnO2,2), data=train.data )
summary(a5.fitp1)

a5.fitp2 <- lm(a5 ~ poly(as.numeric(size),2) * poly(mnO2,1), data=train.data )
summary(a5.fitp2)

a5.fitp3 <- lm(a5 ~ poly(as.numeric(size),2), data=train.data )
summary(a5.fitp3)


train.data <- train.data %>%
	mutate(fit1 = predict(a5.fitp1),
	fit2 = predict(a5.fitp2),
	fit3 = predict(a5.fitp3))

#  visualization: doesnt make so much sense, but at least having it visualized / too many variables

cols <- c( "Deg.2", "Deg.1")
train.data %>% 
	ggplot(aes(x=a5, y=size)) +
	geom_point() +
	geom_line(aes(y=fit1, color="deg 8"), size =1) +
	geom_line(aes(y=fit2, color="deg 3"), size =1) +
  geom_line(aes(y=fit3, color="deg 2"), size =1) +
  	theme(legend.title = element_blank(), 
		legend.position = "bottom", 
		legend.direction = "horizontal")

```


## Log Linear Model

```{r}

a5.testlogmod <- lm(log(a5+1) ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=newtrain.data)
summary(a5.testlogmod)

exp(a5.testlogmod$coefficients)


a5.test.pred.log <- (predict(a5.testlogmod,traintest.data))
 
a5.RMSE.testlog <- sqrt(mean((a5.test.pred.log-traintest.data$a5)^2))
a5.RMSE.testlog

a5.MAE.testlog <- mean(abs(a5.test.pred.log-traintest.data$a5))
a5.MAE.testlog

# deleting the totally unsignificant

a5.testlogmod1 <- lm(log(a5+1) ~ size *mnO2 , data=newtrain.data)
summary(a5.testlogmod1)
a5.testlogmod1$coefficients

exp(a5.testlogmod1$coefficients)

a5.test.pred.log2 <- (predict(a5.testlogmod1,traintest.data))
 
a5.RMSE.testlog2 <- sqrt(mean((a5.test.pred.log2-traintest.data$a5)^2))
a5.RMSE.testlog2

a5.MAE.testlog2 <- mean(abs(a5.test.pred.log2-traintest.data$a5))
a5.MAE.testlog2

# not increasing, a5.testlogmod better

```

## Trees

```{r, error=FALSE}
require(rpart)
require(rattle)
# prepare the testing

a5.testtreemod <- rpart(a5 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data,method="anova")

a5.test.pred.treemod <- predict(a5.testtreemod,traintest.data) 
 
a5.RMSE.treemod <- sqrt(mean((a5.test.pred.treemod-traintest.data$a5)^2))
a5.RMSE.treemod

a5.MAE.treemod <- mean(abs(a5.test.pred.treemod-traintest.data$a5))
a5.MAE.treemod

# pruning it

printcp(a5.testtreemod)

min.xerror <- a5.testtreemod$cptable[which.min(a5.testtreemod$cptable[,"xerror"]),"CP"]

a5.treemod <- prune(a5.testtreemod, cp = min.xerror)
summary(a5.treemod)

a5.test.pred.treemod2 <- predict(a5.treemod,traintest.data) 
 
a5.RMSE.treemod2 <- sqrt(mean((a5.test.pred.treemod2-traintest.data$a5)^2))
a5.RMSE.treemod2

a5.MAE.treemod2 <- mean(abs(a5.test.pred.treemod2-traintest.data$a5))
a5.MAE.treemod2

# plot(a5.treemod)
# text(a5.treemod, pretty = 0)

```

## RandomForest

```{r}
# test it
require(randomForest)
require(rattle)
set.seed(200)
a5.forestmod <- randomForest(a5 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data, na.action = na.omit, importance = TRUE, ntree=1000)

which.min(a5.forestmod$mse)


print(a5.forestmod)
plot(a5.forestmod)

# understanding the importance of each variable
# a5.forestmodimp <- as.data.frame(sort(importance(a5.forestmod)[,1],decreasing = TRUE),optional = T)
# names(a5.forestmodimp) <- "% Inc MSE"
# a5.forestmodimp
a5.forestmod$importance

a5.testforest <- predict(a5.forestmod,traintest.data)
a5.RMSE.forestmod <- sqrt(mean((a5.testforest-traintest.data$a5)^2))
a5.RMSE.forestmod
 
a5.MAE.forestmod <- mean(abs(a5.testforest-traintest.data$a5))
a5.MAE.forestmod
```

```{r}
a5.accuracy <- data.frame(Method = c("MLR","Poly LR","Log LR","Pruned Tree","RandomForest"),
                         a5.RMSE   = c(a5.RMSE.testmlr4,a5.RMSE.polymod2,a5.RMSE.testlog,a5.RMSE.treemod2
,a5.RMSE.forestmod),
                         a5.MAE    = c(a5.MAE.testmlr4,a5.MAE.polymod2,a5.MAE.testlog,a5.MAE.treemod2,a5.MAE.forestmod)) 

a5.accuracy

```

The table shows that Random Forest is best method for a5.

Now we make the prediction:

```{r}
a5.to.pred <-randomForest(a5 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data, na.action = na.omit, importance = TRUE, ntree=1000)

a5.pred <- predict(a5.to.pred, test.data)
a5.pred
# all negative values to zero
a5.pred[a5.pred<0] <- 0
summary(a5.pred)
# just to have a comparison
summary(train.data$a5)
```

<!--chapter:end:13e_Solution_a5.Rmd-->

---
title: "12e_Solution_a6"
author: "CTrierweiler,PGaulke"
date: "29 Juli 2019"
output: html_document
---

# Develop the Model for a6 

Methods that will be tested:

- Multiple Linear Regression
- Linear regression with polynomials
- Log Linear Regression
- Trees
- RandomForest

Logistic regression is not applicable on this data set as the response is not binary.

## Multiple Linear Regression

```{r}

a6.testmlrmod <- lm(a6 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=newtrain.data)
summary(a6.testmlrmod)

a6.test.pred.mlr <- (predict(a6.testmlrmod,traintest.data))
 
a6.RMSE.testmlr <- sqrt(mean((a6.test.pred.mlr-traintest.data$a6)^2))
a6.RMSE.testmlr

a6.MAE.testmlr <- mean(abs(a6.test.pred.mlr-traintest.data$a6))
a6.MAE.testmlr

# Reducing the variables
a6.testmlrmod2 <- lm(a6 ~ Cl * NO3, data=newtrain.data)
summary(a6.testmlrmod2)

a6.test.pred.mlr2 <- (predict(a6.testmlrmod2,traintest.data))
 
a6.RMSE.testmlr2 <- sqrt(mean((a6.test.pred.mlr2-traintest.data$a6)^2))
a6.RMSE.testmlr2

a6.MAE.testmlr2 <- mean(abs(a6.test.pred.mlr2-traintest.data$a6))
a6.MAE.testmlr2

# Reducing the variables to only one variable (SLR)

a6.testmlrmod3 <- lm(a6 ~ NO3, data=newtrain.data)

summary(a6.testmlrmod3)

a6.test.pred.mlr3 <- (predict(a6.testmlrmod3,traintest.data))
 
a6.RMSE.testmlr3 <- sqrt(mean((a6.test.pred.mlr3-traintest.data$a6)^2))
a6.RMSE.testmlr3

a6.MAE.testmlr3 <- mean(abs(a6.test.pred.mlr3-traintest.data$a6))
a6.MAE.testmlr3

# SLR best, optimize
a6.testmlrmod4 <- lm(a6 ~  NO3 * mnO2, data=newtrain.data)
summary(a6.testmlrmod4)
a6.test.pred.mlr4 <- (predict(a6.testmlrmod4,traintest.data))
 
a6.RMSE.testmlr4 <- sqrt(mean((a6.test.pred.mlr4-traintest.data$a6)^2))
a6.RMSE.testmlr4

a6.MAE.testmlr4 <- mean(abs(a6.test.pred.mlr4-traintest.data$a6))
a6.MAE.testmlr4

# a6-testmlrmod4 as best choice

```

Trying k-fold cross validation

```{r}
require(caret)
set.seed(200) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
a6.crossmodelmlr <- train(a6 ~ NO3 * mnO2, data = train.data, method = "lm",
               trControl = train.control)
# Summarize the results
print(a6.crossmodelmlr)
```


## Polynomials

Adding polynomials

```{r}

# test it from mlr4

a6.testpoly <- glm(a6 ~ NO3 * mnO2, data=newtrain.data)
summary(a6.testpoly)
# use of poly

a6.testpolymod1 <- glm(a6 ~ poly(NO3,2) * poly(mnO2,2), data=newtrain.data)
summary(a6.testpolymod1)

a6.test.pred.poly <- predict(a6.testpolymod1,traintest.data) 
 
a6.RMSE.polymod <- sqrt(mean((a6.test.pred.poly-traintest.data$a6)^2))
a6.RMSE.polymod

a6.MAE.polymod <- mean(abs(a6.test.pred.poly-traintest.data$a6))
a6.MAE.polymod

# checking for adjusted polys

a6.testpolymod2 <- glm(a6 ~ poly(NO3,1) * poly(mnO2,2), data=newtrain.data)
summary(a6.testpolymod2)

a6.test.pred.poly2 <- predict(a6.testpolymod2,traintest.data) 
 
a6.RMSE.polymod2 <- sqrt(mean((a6.test.pred.poly2-traintest.data$a6)^2))
a6.RMSE.polymod2

a6.MAE.polymod2 <- mean(abs(a6.test.pred.poly2-traintest.data$a6))
a6.MAE.polymod2

# Best poly is a6.testpolymod2, alternative with less variables

a6.testpolymod3 <- glm(a6 ~ poly(mnO2,2), data=newtrain.data)
summary(a6.testpolymod3)

a6.test.pred.poly3 <- predict(a6.testpolymod3,traintest.data) 
 
a6.RMSE.polymod3 <- sqrt(mean((a6.test.pred.poly3-traintest.data$a6)^2))
a6.RMSE.polymod3

a6.MAE.polymod3 <- mean(abs(a6.test.pred.poly3-traintest.data$a6))
a6.MAE.polymod3

# way worse


```

Trying k-fold cross validation

```{r}
require(caret)
set.seed(200) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
a6.crossmodelmlr <- train(a6 ~ poly(NO3,1) * poly(mnO2,2), data = train.data, method = "lm",
               trControl = train.control)
# Summarize the results
print(a6.crossmodelmlr)
```

```{r}
## plot the rmse
a6.models.rmse <- tibble(
            a6.model = paste0("model.pd",c(1,2,3)),
						RMSE= c(
							c.rmse(traintest.data$a6,predict(a6.testpolymod1,traintest.data)),
              c.rmse(traintest.data$a6,predict(a6.testpolymod2,traintest.data)),
							c.rmse(traintest.data$a6,predict(a6.testpolymod3,traintest.data))
							)
					)
a6.models.rmse
a6.ncoef <- function(a6.model){
	a6.model %>%
    coefficients %>%
    length %>%
    {. - 1}
}


a6.models.rmse$a6.ncoef <- c(a6.ncoef(a6.testpolymod1),
                        a6.ncoef(a6.testpolymod2),
                        a6.ncoef(a6.testpolymod3))

a6.models.rmse %>%
  ggplot(aes(x=a6.ncoef, y= RMSE)) +
  geom_line(color = "dodgerblue") + 
  geom_point(color = "dodgerblue") +
  scale_x_continuous(breaks = c(0,1,2,3,4,5,6,7,8,9,10) )

# plot the fit

a6.fitp1 <- lm(a6 ~ poly(NO3,1) * poly(mnO2,2), data=train.data )
summary(a6.fitp1)

a6.fitp2 <- lm(a6 ~ poly(NO3,2) * poly(mnO2,2), data=train.data )
summary(a6.fitp2)

a6.fitp3 <- lm(a6 ~ poly(mnO2,2), data=train.data )
summary(a6.fitp3)


train.data <- train.data %>%
	mutate(fit1 = predict(a6.fitp1),
	fit2 = predict(a6.fitp2),
	fit3 = predict(a6.fitp3))

#  visualization: doesnt make so much sense, but at least having it visualized / too many variables

cols <- c( "Deg.2", "Deg.1")
train.data %>% 
	ggplot(aes(x=a6, y=PO4)) +
	geom_point() +
	geom_line(aes(y=fit1, color="deg 8"), size =1) +
	geom_line(aes(y=fit2, color="deg 5"), size =1) +
	geom_line(aes(y=fit3, color="deg 2"), size =1) +
  theme(legend.title = element_blank(), 
		legend.position = "bottom", 
		legend.direction = "horizontal")

```


## Log Linear Model

```{r}

a6.testlogmod <- lm(log(a6+1) ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=newtrain.data)
summary(a6.testlogmod)

exp(a6.testlogmod$coefficients)

a6.test.pred.log <- (predict(a6.testlogmod,traintest.data))
 
a6.RMSE.testlog <- sqrt(mean((a6.test.pred.log-traintest.data$a6)^2))
a6.RMSE.testlog

a6.MAE.testlog <- mean(abs(a6.test.pred.log-traintest.data$a6))
a6.MAE.testlog

# deleting the totally unsignificant

a6.testlogmod1 <- lm(log(a6+1) ~ NO3*size , data=newtrain.data)
summary(a6.testlogmod1)
a6.testlogmod1$coefficients

exp(a6.testlogmod1$coefficients)

a6.test.pred.log2 <- (predict(a6.testlogmod1,traintest.data))
 
a6.RMSE.testlog2 <- sqrt(mean((a6.test.pred.log2-traintest.data$a6)^2))
a6.RMSE.testlog2

a6.MAE.testlog2 <- mean(abs(a6.test.pred.log2-traintest.data$a6))
a6.MAE.testlog2

# not increasing, a6.testlogmod better

```

## Trees

```{r, error=FALSE}
require(rpart)
require(rattle)
# prepare the testing

a6.testtreemod <- rpart(a6 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data,method="anova")

a6.test.pred.treemod <- predict(a6.testtreemod,traintest.data) 
 
a6.RMSE.treemod <- sqrt(mean((a6.test.pred.treemod-traintest.data$a6)^2))
a6.RMSE.treemod

a6.MAE.treemod <- mean(abs(a6.test.pred.treemod-traintest.data$a6))
a6.MAE.treemod

# pruning it

printcp(a6.testtreemod)

min.xerror <- a6.testtreemod$cptable[which.min(a6.testtreemod$cptable[,"xerror"]),"CP"]

a6.treemod <- prune(a6.testtreemod, cp = min.xerror)
summary(a6.treemod)

a6.test.pred.treemod2 <- predict(a6.treemod,traintest.data) 
 
a6.RMSE.treemod2 <- sqrt(mean((a6.test.pred.treemod2-traintest.data$a6)^2))
a6.RMSE.treemod2

a6.MAE.treemod2 <- mean(abs(a6.test.pred.treemod2-traintest.data$a6))
a6.MAE.treemod2

# plot(a6.treemod)
# text(a6.treemod, pretty = 0)

```

## RandomForest

```{r}
# test it
require(randomForest)
require(rattle)
set.seed(200)
a6.forestmod <- randomForest(a6 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data, na.action = na.omit, importance = TRUE, ntree=1000)

which.min(a6.forestmod$mse)


print(a6.forestmod)
plot(a6.forestmod)

# understanding the importance of each variable
# a6.forestmodimp <- as.data.frame(sort(importance(a6.forestmod)[,1],decreasing = TRUE),optional = T)
# names(a6.forestmodimp) <- "% Inc MSE"
# a6.forestmodimp
a6.forestmod$importance

a6.testforest <- predict(a6.forestmod,traintest.data)
a6.RMSE.forestmod <- sqrt(mean((a6.testforest-traintest.data$a6)^2))
a6.RMSE.forestmod
 
a6.MAE.forestmod <- mean(abs(a6.testforest-traintest.data$a6))
a6.MAE.forestmod
```

```{r}
a6.accuracy <- data.frame(Method = c("MLR","Poly LR","Log LR","Pruned Tree","RandomForest"),
                         a6.RMSE   = c(a6.RMSE.testmlr4,a6.RMSE.polymod2,a6.RMSE.testlog,a6.RMSE.treemod2
,a6.RMSE.forestmod),
                         a6.MAE    = c(a6.MAE.testmlr4,a6.MAE.polymod2,a6.MAE.testlog,a6.MAE.treemod2,a6.MAE.forestmod)) 

a6.accuracy

```

The table shows that MLR is the best method for a6.

Now we make the prediction:

```{r}
a6.to.pred <-lm(a6 ~  NO3 * mnO2, data=train.data)

a6.pred <- predict(a6.to.pred, test.data)
a6.pred
# all negative values to zero
a6.pred[a6.pred<0] <- 0
summary(a6.pred)
# just to have a comparison
summary(train.data$a6)
```

<!--chapter:end:13f_Solution_a6.Rmd-->

---
title: "12g_Solution_a7"
author: "CTrierweiler,PGaulke"
date: "29 Juli 2019"
output: html_document
---
# Develop the Model for a7 

Methods that will be tested:

- Multiple Linear Regression
- Linear regression with polynomials
- Log Linear Regression
- Trees
- RandomForest

Logistic regression is not applicable on this data set as the response is not binary.

## Multiple Linear Regression

```{r}

a7.testmlrmod <- lm(a7 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=newtrain.data)
summary(a7.testmlrmod)

a7.test.pred.mlr <- (predict(a7.testmlrmod,traintest.data))
 
a7.RMSE.testmlr <- sqrt(mean((a7.test.pred.mlr-traintest.data$a7)^2))
a7.RMSE.testmlr

a7.MAE.testmlr <- mean(abs(a7.test.pred.mlr-traintest.data$a7))
a7.MAE.testmlr

# Reducing the variables
a7.testmlrmod2 <- lm(a7 ~ Chla + NO3, data=newtrain.data)
summary(a7.testmlrmod2)

a7.test.pred.mlr2 <- (predict(a7.testmlrmod2,traintest.data))
 
a7.RMSE.testmlr2 <- sqrt(mean((a7.test.pred.mlr2-traintest.data$a7)^2))
a7.RMSE.testmlr2

a7.MAE.testmlr2 <- mean(abs(a7.test.pred.mlr2-traintest.data$a7))
a7.MAE.testmlr2

# Reducing the variables to only one variable (SLR)

a7.testmlrmod3 <- lm(a7 ~ NO3, data=newtrain.data)

summary(a7.testmlrmod3)

a7.test.pred.mlr3 <- (predict(a7.testmlrmod3,traintest.data))
 
a7.RMSE.testmlr3 <- sqrt(mean((a7.test.pred.mlr3-traintest.data$a7)^2))
a7.RMSE.testmlr3

a7.MAE.testmlr3 <- mean(abs(a7.test.pred.mlr3-traintest.data$a7))
a7.MAE.testmlr3

# SLR best, optimize
a7.testmlrmod4 <- lm(a7 ~  NO3 * Chla, data=newtrain.data)
summary(a7.testmlrmod4)
a7.test.pred.mlr4 <- (predict(a7.testmlrmod4,traintest.data))
 
a7.RMSE.testmlr4 <- sqrt(mean((a7.test.pred.mlr4-traintest.data$a7)^2))
a7.RMSE.testmlr4

a7.MAE.testmlr4 <- mean(abs(a7.test.pred.mlr4-traintest.data$a7))
a7.MAE.testmlr4

# a7-testmlrmod4 as best choice

```

Trying k-fold cross validation

```{r}
require(caret)
set.seed(200) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
a7.crossmodelmlr <- train(a7 ~ NO3 * Chla, data = train.data, method = "lm",
               trControl = train.control)
# Summarize the results
print(a7.crossmodelmlr)
```


## Polynomials

Adding polynomials

```{r}

# test it from mlr4

a7.testpoly <- glm(a7 ~ NO3 * Chla, data=newtrain.data)
summary(a7.testpoly)
# use of poly

a7.testpolymod1 <- glm(a7 ~ poly(NO3,2) * poly(Chla,2), data=newtrain.data)
summary(a7.testpolymod1)

a7.test.pred.poly <- predict(a7.testpolymod1,traintest.data) 
 
a7.RMSE.polymod <- sqrt(mean((a7.test.pred.poly-traintest.data$a7)^2))
a7.RMSE.polymod

a7.MAE.polymod <- mean(abs(a7.test.pred.poly-traintest.data$a7))
a7.MAE.polymod

# checking for adjusted polys

a7.testpolymod2 <- glm(a7 ~ poly(NO3,1) * poly(Chla,2), data=newtrain.data)
summary(a7.testpolymod2)

a7.test.pred.poly2 <- predict(a7.testpolymod2,traintest.data) 
 
a7.RMSE.polymod2 <- sqrt(mean((a7.test.pred.poly2-traintest.data$a7)^2))
a7.RMSE.polymod2

a7.MAE.polymod2 <- mean(abs(a7.test.pred.poly2-traintest.data$a7))
a7.MAE.polymod2

# Best poly is a7.testpolymod2

a7.testpolymod3 <- glm(a7 ~ poly(NO3,2), data=newtrain.data)
summary(a7.testpolymod3)

a7.test.pred.poly3 <- predict(a7.testpolymod3,traintest.data) 
 
a7.RMSE.polymod3 <- sqrt(mean((a7.test.pred.poly3-traintest.data$a7)^2))
a7.RMSE.polymod3

a7.MAE.polymod3 <- mean(abs(a7.test.pred.poly3-traintest.data$a7))
a7.MAE.polymod3


# worse

```

Trying k-fold cross validation

```{r}
require(caret)
set.seed(200) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
a7.crossmodelmlr <- train(a7 ~ poly(NO3,1) * poly(Chla,2), data = train.data, method = "lm",
               trControl = train.control)
# Summarize the results
print(a7.crossmodelmlr)
```

```{r}
## plot the rmse
a7.models.rmse <- tibble(
            a7.model = paste0("model.pd",c(1,2,3)),
						RMSE= c(
							c.rmse(traintest.data$a7,predict(a7.testpolymod1,traintest.data)),
              c.rmse(traintest.data$a7,predict(a7.testpolymod2,traintest.data)),
							c.rmse(traintest.data$a7,predict(a7.testpolymod3,traintest.data))
							)
					)
a7.models.rmse
a7.ncoef <- function(a7.model){
	a7.model %>%
    coefficients %>%
    length %>%
    {. - 1}
}


a7.models.rmse$a7.ncoef <- c(a7.ncoef(a7.testpolymod1),
                        a7.ncoef(a7.testpolymod2),
                        a7.ncoef(a7.testpolymod3))

a7.models.rmse %>%
  ggplot(aes(x=a7.ncoef, y= RMSE)) +
  geom_line(color = "dodgerblue") + 
  geom_point(color = "dodgerblue") +
  scale_x_continuous(breaks = c(0,1,2,3,4,5,6,7,8,9,10) )

# plot the fit

a7.fitp1 <- lm(a7 ~ poly(NO3,1) * poly(Chla,2), data=train.data )
summary(a7.fitp1)

a7.fitp2 <- lm(a7 ~ poly(NO3,2) * poly(Chla,2), data=train.data )
summary(a7.fitp2)


a7.fitp3 <- lm(a7 ~ poly(NO3,2), data=train.data )
summary(a7.fitp3)

train.data <- train.data %>%
	mutate(fit1 = predict(a7.fitp1),
	fit2 = predict(a7.fitp2),
	fit3 = predict(a7.fitp3))

#  visualization: doesnt make so much sense, but at least having it visualized / too many variables

cols <- c( "Deg.2", "Deg.1")
train.data %>% 
	ggplot(aes(x=a7, y=PO4)) +
	geom_point() +
	geom_line(aes(y=fit1, color="deg 8"), size =1) +
	geom_line(aes(y=fit2, color="deg 5"), size =1) +
	geom_line(aes(y=fit3, color="deg 2"), size =1) +
  theme(legend.title = element_blank(), 
		legend.position = "bottom", 
		legend.direction = "horizontal")

```


## Log Linear Model

```{r}

a7.testlogmod <- lm(log(a7+1) ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data=newtrain.data)
summary(a7.testlogmod)

exp(a7.testlogmod$coefficients)

a7.test.pred.log <- (predict(a7.testlogmod,traintest.data))
 
a7.RMSE.testlog <- sqrt(mean((a7.test.pred.log-traintest.data$a7)^2))
a7.RMSE.testlog

a7.MAE.testlog <- mean(abs(a7.test.pred.log-traintest.data$a7))
a7.MAE.testlog

# deleting the totally unsignificant

a7.testlogmod1 <- lm(log(a7+1) ~ season*Chla*Cl , data=newtrain.data)
summary(a7.testlogmod1)
a7.testlogmod1$coefficients

exp(a7.testlogmod1$coefficients)

a7.test.pred.log2 <- (predict(a7.testlogmod1,traintest.data))
 
a7.RMSE.testlog2 <- sqrt(mean((a7.test.pred.log2-traintest.data$a7)^2))
a7.RMSE.testlog2

a7.MAE.testlog2 <- mean(abs(a7.test.pred.log2-traintest.data$a7))
a7.MAE.testlog2

# not increasing, a7.testlogmod better

```

## Trees

```{r, error=FALSE}
require(rpart)
require(rattle)
# prepare the testing

a7.testtreemod <- rpart(a7 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data,method="anova")

a7.test.pred.treemod <- predict(a7.testtreemod,traintest.data) 
 
a7.RMSE.treemod <- sqrt(mean((a7.test.pred.treemod-traintest.data$a7)^2))
a7.RMSE.treemod

a7.MAE.treemod <- mean(abs(a7.test.pred.treemod-traintest.data$a7))
a7.MAE.treemod

# pruning it

printcp(a7.testtreemod)

min.xerror <- a7.testtreemod$cptable[which.min(a7.testtreemod$cptable[,"xerror"]),"CP"]

a7.treemod <- prune(a7.testtreemod, cp = min.xerror)
summary(a7.treemod)

a7.test.pred.treemod2 <- predict(a7.treemod,traintest.data) 
 
a7.RMSE.treemod2 <- sqrt(mean((a7.test.pred.treemod2-traintest.data$a7)^2))
a7.RMSE.treemod2

a7.MAE.treemod2 <- mean(abs(a7.test.pred.treemod2-traintest.data$a7))
a7.MAE.treemod2

# plot(a7.treemod)
# text(a7.treemod, pretty = 0)

```

## RandomForest

```{r}
# test it
require(randomForest)
require(rattle)
set.seed(200)
a7.forestmod <- randomForest(a7 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data, na.action = na.omit, importance = TRUE, ntree=1000)

which.min(a7.forestmod$mse)


print(a7.forestmod)
plot(a7.forestmod)

# understanding the importance of each variable
# a7.forestmodimp <- as.data.frame(sort(importance(a7.forestmod)[,1],decreasing = TRUE),optional = T)
# names(a7.forestmodimp) <- "% Inc MSE"
# a7.forestmodimp
a7.forestmod$importance

a7.testforest <- predict(a7.forestmod,traintest.data)
a7.RMSE.forestmod <- sqrt(mean((a7.testforest-traintest.data$a7)^2))
a7.RMSE.forestmod
 
a7.MAE.forestmod <- mean(abs(a7.testforest-traintest.data$a7))
a7.MAE.forestmod
```

```{r}
a7.accuracy <- data.frame(Method = c("MLR","Poly LR","Log LR","Pruned Tree","RandomForest"),
                         a7.RMSE   = c(a7.RMSE.testmlr4,a7.RMSE.polymod2,a7.RMSE.testlog,a7.RMSE.treemod2
,a7.RMSE.forestmod),
                         a7.MAE    = c(a7.MAE.testmlr4,a7.MAE.polymod2,a7.MAE.testlog,a7.MAE.treemod2,a7.MAE.forestmod)) 

a7.accuracy

```

The table shows that RandomForest is the best method for a7.

Now we make the prediction:

```{r}
a7.to.pred <-randomForest(a7 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = train.data, na.action = na.omit, importance = TRUE, ntree=1000)

a7.pred <- predict(a7.to.pred, test.data)
a7.pred
# all negative values to zero
a7.pred[a7.pred<0] <- 0
summary(a7.pred)
# just to have a comparison
summary(train.data$a7)
```

<!--chapter:end:13g_Solution_a7.Rmd-->

---
title: "13_Solution_Task_Conclusion"
author: "CTrierweiler,PGaulke"
date: "29 Juli 2019"
output: html_document
---

# Conclusion

For the best prediction of responses for the given data set, we choose to test different models such as
- Multiple Linear Regression
- Linear regression with polynomials
- Log Linear Regression
- Trees
- RandomForest
We choose multiple linear regression as it is the most common method for data prediction. However, high manual effort was needed in order to choose the most significant varaibles for getting the best prediction model possible. In addition to that, we tested linear regression with polynomials and log linear regression, in order to understand how closer this might bring us to a better prediction. Apart from linear regression models, we also tested two tree-based methods. First, we applied the base decision tree model. However as we faced prolems with that method when pruning the tree, because the tree was then converted to only a root, we decided to add the RandomForest model. Besides that, we left out logistic regression as it is not suitable for predicting non-binary responses.
Before we tested the models, we replaced the N/A's of the given data sets by the mean of the column. This is not the most accurated method, but it might be still more accurate than a random created model. The process was applied on both, the train.data and the test.data in order to have the same premise.
Furthermore, we splitted the `train.data`, to have a regular train.data set and a test.data set with which we used to test the different models. After creating the prediction models with the train.data set (`newtrain.data`), we applied them on the test.data set (`traintest.data`). We checked the RMSE of each method in order to create not only the best possible model for each method, but also in order to compare the different models at the end to understand which one is the best for each response.

Our solution is:

a1 - `randomForest(a1 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = train.data, na.action = na.omit, importance = TRUE, ntree=1000)`

a2 - `randomForest(a2 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = train.data, na.action = na.omit, importance = TRUE, ntree=1000)`

a3 - `lm(a3 ~ poly(as.numeric(season),3) + NH4 * mnO2, data = train.data)`

a4 - `lm(a4 ~  mxPH  * mnO2, data = train.data)`

a5 - `randomForest(a5 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = newtrain.data, na.action = na.omit, importance = TRUE, ntree=1000)`

a6 - `lm(a6 ~  NO3 * mnO2, data=train.data)`

a7 - `randomForest(a7 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = train.data, na.action = na.omit, importance = TRUE, ntree=1000)`

Although, this is the bet that we were able to determine, there are indeed ways to improve these models. Especially, for tree-based models we could apply methods such as *boosting* and *bagging*/*bootstraping*, as these models support to increase accuracy of the prediction models. Furthermore, as already mentioned, the replacement of N/A's in the data.set can be also done by a more accurate model. For example, by simulating the missing data.
However, this is so far out of our skill-range, but it might be an option for future work on this prediction.

## Making a vector out of it

```{r}
our.predictions <- c(a1=a1.pred,a2=a2.pred,a3=a3.pred,a4=a4.pred,a5=a5.pred,a6=a6.pred,a7=a7.pred)
our.predictions
length(our.predictions)
```

<!--chapter:end:14_Solution_Task_Conclusion.Rmd-->

